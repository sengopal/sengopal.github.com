<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Senthilkumar Gopal - Neuron</title><link href="https://sengopal.me/" rel="alternate"></link><link href="https://sengopal.me/feeds/neuron.atom.xml" rel="self"></link><id>https://sengopal.me/</id><updated>2024-09-02T00:00:00-07:00</updated><subtitle>Musings of a machine learning researcher, engineer and leader</subtitle><entry><title>Slurm Cluster usage tips for quick debug and testing</title><link href="https://sengopal.me/posts/slurm-cluster-usage-tips-for-quick-debug-and-testing" rel="alternate"></link><published>2024-09-02T00:00:00-07:00</published><updated>2024-09-02T00:00:00-07:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2024-09-02:/posts/slurm-cluster-usage-tips-for-quick-debug-and-testing</id><summary type="html">This post covers practical Slurm commands and best practices to help you reserve, access, and work cleanly on nodes.</summary><content type="html">&lt;p&gt;The following is a cheatsheet for Slurm commands used for running
tests, debugging, and reserving compute nodes.&lt;/p&gt;
&lt;p&gt;To start working with an idle node, you can allocate one exclusively
using &lt;code&gt;salloc --exclusive -N 1&lt;/code&gt;. This command requests a full
node for your use, allowing direct work without interference. Once your
allocation is granted, you can see which node you received by checking
the job ID in &lt;code&gt;squeue&lt;/code&gt;. Instead of SSHing manually into the
node, it is cleaner and simpler to use
&lt;code&gt;srun -N 1 --pty --exclusive bash&lt;/code&gt; to start an interactive
bash session directly on the node. This avoids common SSH permission
errors.&lt;/p&gt;
&lt;p&gt;If you want to verify which nodes are idle before requesting one, you
can run &lt;code&gt;sinfo -N -r -l&lt;/code&gt;. This will list nodes and their
states, helping you identify available resources. After allocation, if
you need specific information about your node, you can retrieve detailed
status using &lt;code&gt;scontrol show nodes&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There may be cases where you need a specific node due to setup
constraints or to implement a particular network topology. In that case,
instead of asking for a number of nodes, you can use
&lt;code&gt;--nodelist&lt;/code&gt; or &lt;code&gt;-w&lt;/code&gt; followed by the desired node
name when calling &lt;code&gt;salloc&lt;/code&gt; or &lt;code&gt;sbatch&lt;/code&gt;. Keep in
mind that if a node is already exclusively allocated, your request will
hang until the node becomes free.&lt;/p&gt;
&lt;p&gt;Sometimes, you may want to occupy a node for an extended period to
perform manual debugging. A simple method is to submit a job that
“sleeps” for a long time using
&lt;code&gt;sbatch --nodes=1 --wrap "sleep 36000"&lt;/code&gt;. This holds the node
for about ten hours, giving you ample time to test and debug.&lt;/p&gt;
&lt;h3 id="caution-with-salloc"&gt;Caution with &lt;code&gt;salloc&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;While using &lt;code&gt;salloc&lt;/code&gt; for direct work is convenient, it is
generally better to run structured jobs with &lt;code&gt;sbatch&lt;/code&gt;
whenever possible. Scheduled jobs are managed more fairly and usually
have better priority than interactive ones. If you install custom
dependencies or system-level libraries during your session, make sure
they are isolated and do not impact other users. Using shared locations
like &lt;code&gt;/shared_inference&lt;/code&gt; can make it easier to move across
nodes without repeated installations.&lt;/p&gt;
&lt;p&gt;If you have an active reservation on the cluster, you can submit jobs
against it with
&lt;code&gt;sbatch --reservation=dedicated_two your_script.sh&lt;/code&gt;. You can
check available reservations with &lt;code&gt;scontrol show res&lt;/code&gt; and
adjust your job submissions accordingly.&lt;/p&gt;
&lt;p&gt;Finally, if your workflow involves setting up environments or
installing packages before running experiments, one efficient approach
is to create a batch script that performs all setup steps and then
blocks by running &lt;code&gt;/bin/bash&lt;/code&gt;. This allows you to SSH or
&lt;code&gt;srun&lt;/code&gt; into the node and continue your work interactively
without leaving stray processes.&lt;/p&gt;</content><category term="Neuron"></category><category term="ml-code"></category><category term="llm"></category><category term="ml-acceleration"></category><category term="slurm"></category><category term="hpc-concepts"></category></entry><entry><title>Neuron - Handling NaNs</title><link href="https://sengopal.me/posts/neuron-handling-nans" rel="alternate"></link><published>2024-01-20T00:00:00-08:00</published><updated>2024-01-20T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2024-01-20:/posts/neuron-handling-nans</id><summary type="html">A runbook for triaging Neuron accuracy issues and means to verify the model accuracy</summary><content type="html">&lt;p&gt;LLMs work with floating point numbers and susceptible to saturation
or loss of precision issue. This is typically seen with NaN &lt;em&gt;(Not a
Number)&lt;/em&gt; errors.&lt;/p&gt;
&lt;p&gt;Neuron compiler suggests the following options to overcome the NaN
issue.&lt;/p&gt;
&lt;h3 id="compiler-flag---enable-saturate-infinity"&gt;Compiler flag:
&lt;code&gt;--enable-saturate-infinity&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuronx-cc/index.html#neuron-compiler-2-7-0-40"&gt;Ref&lt;/a&gt;
A computation that can generate +/- infinity is at a high risk of
generating Not-a-Number (NaN) values when the infinity value is used in
subsequent computations. This option helps avoid this by converting
+Inf/-Inf values to MAX/MIN_FLOAT before operations that could produce
NaN values for +Inf/-Inf inputs on the target architecture. While this
option helps to avoid NaN values, there is a potential performance
degradation that occurs during model execution when this conversion is
enabled.&lt;/p&gt;
&lt;h3 id="compiler-flag---enable-mixed-precision-accumulation"&gt;Compiler
flag: &lt;code&gt;--enable-mixed-precision-accumulation&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/compiler/neuronx-cc/index.html#neuron-compiler-2-13-66-0"&gt;Ref&lt;/a&gt;
To perform intermediate calculations of reduction operators (such as the
dot or reduce operators) in FP32 regardless of the operation’s defined
datatype.&lt;/p&gt;
&lt;h3 id="nans-due-to-saturation"&gt;NaNs due to saturation&lt;/h3&gt;
&lt;p&gt;To triage an intermediate BF16 tensor, np.isnan would not work, since
NumPy supports only &lt;code&gt;float16&lt;/code&gt;, &lt;code&gt;float32&lt;/code&gt;, and
&lt;code&gt;float64&lt;/code&gt; by default and &lt;code&gt;bfloat16&lt;/code&gt; is not a
standard NumPy dtype. The data typically looks like
&lt;code&gt;bf16_hex = [0x3f80, 0xbf80, 0x4000, 0x7f80, 0xff80, 0x7fc0]&lt;/code&gt;.
We can either try to use torch.isnan or convert to fp32 as below.&lt;/p&gt;
&lt;h4 id="convert-to-fp32-for-printingchecking"&gt;Convert to fp32 for
printing/checking&lt;/h4&gt;
&lt;div class="sourceCode" id="cb1"&gt;&lt;pre class="sourceCode python"&gt;&lt;code class="sourceCode python"&gt;&lt;span id="cb1-1"&gt;&lt;a aria-hidden="true" href="#cb1-1" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="kw"&gt;def&lt;/span&gt; bf16_to_float32(bits):&lt;/span&gt;
&lt;span id="cb1-2"&gt;&lt;a aria-hidden="true" href="#cb1-2" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="im"&gt;import&lt;/span&gt; struct&lt;/span&gt;
&lt;span id="cb1-3"&gt;&lt;a aria-hidden="true" href="#cb1-3" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;# Pad with 16 zero bits to match float32&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-4"&gt;&lt;a aria-hidden="true" href="#cb1-4" tabindex="-1"&gt;&lt;/a&gt;    f32_bits &lt;span class="op"&gt;=&lt;/span&gt; bits &lt;span class="op"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="dv"&gt;16&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-5"&gt;&lt;a aria-hidden="true" href="#cb1-5" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; struct.unpack(&lt;span class="st"&gt;'f'&lt;/span&gt;, struct.pack(&lt;span class="st"&gt;'I'&lt;/span&gt;, f32_bits))[&lt;span class="dv"&gt;0&lt;/span&gt;]&lt;/span&gt;
&lt;span id="cb1-6"&gt;&lt;a aria-hidden="true" href="#cb1-6" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-7"&gt;&lt;a aria-hidden="true" href="#cb1-7" tabindex="-1"&gt;&lt;/a&gt;converted &lt;span class="op"&gt;=&lt;/span&gt; [bf16_to_float32(b) &lt;span class="cf"&gt;for&lt;/span&gt; b &lt;span class="kw"&gt;in&lt;/span&gt; bf16_hex]&lt;/span&gt;
&lt;span id="cb1-8"&gt;&lt;a aria-hidden="true" href="#cb1-8" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="bu"&gt;print&lt;/span&gt;(converted)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;To add Neuron available conversion method&lt;/em&gt;&lt;/p&gt;</content><category term="Neuron"></category><category term="ml-code"></category><category term="llm"></category><category term="neuron"></category></entry><entry><title>EFA and OpenFabrics</title><link href="https://sengopal.me/posts/efa-and-openfabrics" rel="alternate"></link><published>2023-12-29T00:00:00-08:00</published><updated>2023-12-29T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-29:/posts/efa-and-openfabrics</id><summary type="html">This post works through a high level overview of OpenMPI and EFA</summary><content type="html">&lt;p&gt;This post attempts to clarify the use of EFA and OpenMPI in
multi-node inference, focusing on how components like the Matching
Transport Layer (MTL), libfabric, and the OFI framework enable
efficient, low-latency communication. Installing EFA for a node &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html"&gt;Ref&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="terms-involved"&gt;Terms Involved&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Matching Transport Layer (MTL)&lt;/strong&gt; The Matching
Transport Layer (MTL) is a component used in the Open MPI implementation
when utilizing libfabric for managing two-sided tagged messages. MTL is
responsible for matching message tags and ensuring that messages are
delivered to the correct destination. This layer is designed to work
closely with the underlying network fabric, such as EFA, to provide
efficient and reliable message passing between nodes in a
high-performance computing (HPC) environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EFA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;EFA integrates with the libfabric API, which is part of the
OpenFabrics Interfaces (OFI) framework. This integration allows EFA to
bypass the operating system kernel, reducing overhead and enabling
low-latency, high-throughput communication directly with the network
interface hardware. This is critical for scaling HPC and machine
learning applications on AWS. By leveraging these components, AWS’s EFA
can provide enhanced performance for HPC and ML applications, enabling
efficient inter-node communication and supporting large-scale
computational tasks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MPI (Message Passing Interface)&lt;/strong&gt; Communication
protocol for parallel programming in distributed computing environments,
particularly in HPC clusters. It provides a standardized way for
processes to communicate with each other across nodes in a cluster,
supporting point-to-point and collective communication.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OFED (OpenFabrics Enterprise Distribution)&lt;/strong&gt; Set of
open-source software components that enable high-performance networking
on clusters, especially those using InfiniBand and other
high-performance fabrics. It provides the necessary drivers, libraries,
and tools to enable low-latency, high-bandwidth communication between
nodes in a cluster. OFED is commonly used in environments where RDMA
(Remote Direct Memory Access) and InfiniBand technologies are deployed,
facilitating direct memory access and efficient data transfer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LibFabric&lt;/strong&gt; Low-level communication library designed
to abstract hardware-specific communication protocols. It provides a
unified API for building high-performance network communication systems
and is often used for RDMA, shared memory, and other communication
technologies. Libfabric allows applications to use different network
fabrics (such as InfiniBand, iWARP, and RoCE) without being tightly
coupled to a particular hardware implementation, making it highly
flexible and adaptable for various cluster environments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RDMA (Remote Direct Memory Access)&lt;/strong&gt; Allows for
high-speed data transfer between nodes in a cluster without involving
the CPU, offering significant reductions in latency and CPU utilization.
By directly accessing the memory of a remote node, RDMA enables faster
data transfers than traditional networking methods, making it ideal for
applications that require large amounts of data to be exchanged between
nodes with minimal overhead. RDMA is supported by technologies like
InfiniBand and RoCE (RDMA over Converged Ethernet), and is critical in
HPC, machine learning, and cloud computing environments.&lt;/p&gt;
&lt;h3 id="openfabrics-interfaces-ofi"&gt;OpenFabrics Interfaces (OFI)&lt;/h3&gt;
&lt;p&gt;OpenFabrics Interfaces (OFI) is a framework designed to expose
communication services to middleware and applications, particularly in
high-performance computing (HPC) environments. Here are the key aspects
of OFI:&lt;/p&gt;
&lt;h3 id="purpose-and-design"&gt;Purpose and Design&lt;/h3&gt;
&lt;p&gt;OFI is specifically designed to meet the performance and scalability
requirements of HPC applications such as Message Passing Interface (MPI)
libraries, Symmetric Hierarchical Memory Access (SHMEM) libraries,
Partitioned Global Address Space (PGAS) programming models, Database
Management Systems (DBMS), and enterprise applications running in
tightly coupled network environments. Its design aligns fabric services
with application needs, providing a tight semantic fit between
applications and the underlying fabric hardware. This reduces software
overhead and improves efficiency when transmitting or receiving data
over a fabric.&lt;/p&gt;
&lt;h3 id="components"&gt;Components&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Libfabric&lt;/strong&gt;: The primary implementation of OFI is the
libfabric library, which defines and exports the user-space API of OFI.
Libfabric is designed to be independent of the underlying network
protocols and the specific implementation of networking devices, making
it versatile and widely applicable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provider Libraries&lt;/strong&gt;: These libraries interface with
the hardware and provide the necessary services to the applications
through libfabric.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kernel Services and Daemons&lt;/strong&gt;: These components
support the user-space libraries and manage the communication between
the application and the hardware.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Applications&lt;/strong&gt;: These are used to validate and
benchmark the performance of the OFI framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="references"&gt;References&lt;/h3&gt;
&lt;ol type="1"&gt;
&lt;li&gt;&lt;a href="https://digitalcloud.training/aws-networking-eni-vs-efa-vs-ena/"&gt;Digital
Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html"&gt;AWS
Docs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Neuron"></category><category term="ml-code"></category><category term="llm"></category><category term="AWS"></category><category term="neuron"></category></entry><entry><title>AI Compilers - A Study guide</title><link href="https://sengopal.me/posts/ai-compilers-a-study-guide" rel="alternate"></link><published>2023-12-22T00:00:00-08:00</published><updated>2023-12-22T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-22:/posts/ai-compilers-a-study-guide</id><summary type="html">A growing list/study guide of AI compilers, from foundational concepts like graph lowering and systolic arrays to practical tools like TorchDynamo and Glow.</summary><content type="html">&lt;p&gt;The following formulates a study guide for learning about AI
compilers and systematically understanding the inner functions. These
are targetted more towards Model and Framework level developers to build
a deeper understanding of compiler functions.&lt;/p&gt;
&lt;h4 id="ai-compilers-demystified---an-introduction"&gt;AI Compilers
Demystified - An Introduction&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://medium.com/geekculture/ai-compilers-ae28afbc4907"&gt;Medium
Article&lt;/a&gt; is a good high level summary of where AI compiler interacts
with the frameworks and no learning about ML Accelerators is complete
without learning about &lt;a href="https://cplu.medium.com/should-we-all-embrace-systolic-array-df3830f193dc"&gt;Systolic
Arrays&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/extras/images/compiler/compiler-structure.png"/&gt; &lt;em&gt;Ref: https://arxiv.org/abs/2002.03794&lt;/em&gt;&lt;/p&gt;
&lt;h4 id="ml-systems-for-tinyml"&gt;ML Systems for TinyML&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://harvard-edge.github.io/cs249r_book/contents/hw_acceleration/hw_acceleration.html"&gt;HW
Acceleration&lt;/a&gt; provides an excellent end to end context on why ML
compilers are necessary for compute optimization and the evolution of
TPUs to ASICs. &lt;a href="https://github.com/harvard-edge/cs249r_book"&gt;CS249R&lt;/a&gt; is a good
course for edgeML learning, but necessarily for compilers only.&lt;/p&gt;
&lt;h4 id="glow-graph-lowering-compiler-techniques-for-neural-networks"&gt;Glow:
Graph Lowering Compiler Techniques for Neural Networks&lt;/h4&gt;
&lt;p&gt;The &lt;a href="https://arxiv.org/pdf/1805.00907"&gt;Glow&lt;/a&gt; paper
discusses what is compilation and how optimized code is generated for
different types of hardware and what IR (Intermediate representations)
are. Though its outdated and replaced with TorchDynamo and
TorchInductor, this paper sets up a good fundamental framework for
understanding the inner mechanisms.&lt;/p&gt;
&lt;h4 id="textbook---deeplearningsystems.ai"&gt;Textbook -
Deeplearningsystems.ai&lt;/h4&gt;
&lt;p&gt;A no-nonsense rich end-to-end textbook to understand the fundamentals
of DL algorithms (Chapters 1-6) and Hardware and compiler level
optimizations for these algorithms (Chapters 7-9)&lt;/p&gt;
&lt;h4 id="a-simple-compiler-example"&gt;A simple compiler Example&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=mTq5PIBCizU"&gt;Build a
Hardware Compiler for Machine Learning and Image Processing&lt;/a&gt; walks
through in 10 minutes the concepts of building an Hardware accelerator
for Image processing and ML algorithms using frameworks like Halide and
others. The goal is to convert architecture-agnostic algorithm
descriptions into hardware accelerators. It emphasizes using High-Level
Synthesis (HLS) as a backend instead of directly targeting Verilog or
VHDL. HLS allows compilers to emit HLS C++ code with directives,
enhancing productivity despite potentially sacrificing some control over
optimization. It goes into some of the design choices of handling
optimizations affecting program semantics (e.g., quantization, bit width
tuning) at the frontend which ensures optimizations directly impacting
program outputs are addressed early in the compilation process. They
utilize “for loops” as an intermediate representation and as a target
output for HLS. Frameworks like TensorFlow, PyTorch, Halide, and TVM
naturally express algorithms as dense linear algebra operations,
simplifying translation to HLS-compatible constructs. They discuss loop
transformations (e.g., fusion, unrolling) and memory optimizations
(e.g., banking).&lt;/p&gt;
&lt;h3 id="a-day-in-the-life-of-a-compiler-engineer"&gt;A day in the life of a
Compiler Engineer&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=egZB5Uxki0I"&gt;Torchdynamo
deep dive&lt;/a&gt; gives the viewer an idea on what issues does a Framework
and compiler engineer faces during their development cycle. Edward from
the PyTorch team discuss TorchDynamo, graph capture part of the
torch.compile ecosystem which aims to capture Python code efficiently by
capturing operations in a format suitable for compilation. TorchDynamo
intercepts Python bytecode execution to generate and optimize
computation graphs.He deep dives into bailouts in deep learning
compilers and optimizations in symbolic evaluation during graph
capture.&lt;/p&gt;
&lt;h3 id="platforms-for-ai-accelerator-design"&gt;Platforms for AI
accelerator design&lt;/h3&gt;
&lt;p&gt;A concept platform for designing and evaluating ML Accelerators - &lt;a href="https://sites.google.com/berkeley.edu/gemminitutorialiiswc2021/"&gt;Gemmini&lt;/a&gt;
| &lt;a href="https://berkeley.app.box.com/s/oc16092wrjhijigf065sctt2xosujsdi"&gt;Tutorial&lt;/a&gt;.
An interesting read to understand what bottlenecks usually occur in
accelerator design and challenges in designing generalized hardware.&lt;/p&gt;
&lt;h3 id="references-for-further-readingscourses"&gt;References for further
readings/courses&lt;/h3&gt;
&lt;ol type="1"&gt;
&lt;li&gt;Textbook: &lt;a href="https://www.amazon.com/Efficient-Processing-Deep-Neural-Networks/dp/168173835X"&gt;Efficient
Processing of Deep Neural Networks&lt;/a&gt; by Vivienne Sze, Yu-Hsin Chen,
Tien-Ju Yang and Joel S. Emer&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/MPSLab-ASU/ML-Accelerators"&gt;A survey of
papers and books about ML Accelerators&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.bilibili.com/video/BV17J4m1h7Jf/?spm_id_from=333.788"&gt;EE290-2&lt;/a&gt;
- Hardware for ML from Berkeley&lt;/li&gt;
&lt;li&gt;&lt;a href="https://abdelfattah-class.github.io/ece5545/"&gt;ECE 5545:
Machine Learning Hardware and Systems&lt;/a&gt; - Well detailed course for ML
HW and Systems design with &lt;a href="https://www.youtube.com/playlist?list=PL0mFAhrXqy9CuopJhAB8GVu_Oy7J0ery6"&gt;Course
Videos&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.iith.ac.in/ramakrishna/C4ML-Jan19/"&gt;C4ML
Course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://courses.cs.washington.edu/courses/cse548/17sp/"&gt;WashU-cse548&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://people.cs.nycu.edu.tw/~ttyeh/course/2023_Fall/IOC5009/outline.html"&gt;IOC5009
- Accelerator Architectures for Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="other-compiler-implementations"&gt;Other Compiler
implementations&lt;/h3&gt;
&lt;ol type="1"&gt;
&lt;li&gt;&lt;a href="https://autokernel-docs-en.readthedocs.io/en/latest/blog/ai_compiler%20overview.html"&gt;AutoKernel&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/blogs/machine-learning/introducing-nnvm-compiler-a-new-open-end-to-end-compiler-for-ai-frameworks/"&gt;NNVM&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Neuron"></category><category term="neuron"></category><category term="ml-code"></category><category term="llm"></category></entry><entry><title>Aliasing on XLA</title><link href="https://sengopal.me/posts/aliasing-on-xla" rel="alternate"></link><published>2023-12-20T00:00:00-08:00</published><updated>2023-12-20T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-20:/posts/aliasing-on-xla</id><summary type="html">This post explores the concept of aliasing in XLA, its significance, the mechanisms through which it is implemented, and future directions for extending aliasing optimizations.</summary><content type="html">&lt;p&gt;Modern machine learning models demand substantial memory, especially
during training and inference. To address this challenge, compilers like
XLA (Accelerated Linear Algebra) implement memory optimizations such as
aliasing. Aliasing allows different parts of a computation to share the
same underlying memory buffer, thereby reducing memory footprint and
improving execution performance.&lt;/p&gt;
&lt;h2 id="what-is-aliasing"&gt;What is Aliasing&lt;/h2&gt;
&lt;p&gt;Aliasing in XLA refers to mapping multiple logical buffers onto the
same physical memory region. In practice, this means that input and
output tensors in a computation can share the same memory if certain
conditions are met. By setting up such controlled aliases, XLA can avoid
allocating separate memory for intermediate results, resulting in
reduced memory usage and faster data access.&lt;/p&gt;
&lt;p&gt;However, establishing aliases must be managed carefully to maintain
computational correctness. If two aliases modify the same memory
simultaneously without proper sequencing, it can lead to corrupted
results. Consequently, XLA enforces strict constraints based on shape
compatibility, computation dependencies, and memory layouts, as outlined
in the design of the XLA compiler&lt;a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="how-xla-supports-aliasing"&gt;How XLA Supports Aliasing&lt;/h2&gt;
&lt;p&gt;XLA supports aliasing primarily through the
&lt;code&gt;OptimizeInputOutputBufferAlias&lt;/code&gt; class. This optimization
pass attempts to reuse input buffers for output buffers wherever
possible while respecting correctness guarantees. The following
annotated code illustrates the core flow:&lt;/p&gt;
&lt;p&gt;_Ref: &lt;a href="https://github.com/openxla/xla/blob/main/xla/hlo/transforms/simplifiers/optimize_input_output_buffer_alias.cc"&gt;optimize_input_output_buffer_alias.cc&lt;/a&gt;
with annotations&lt;/p&gt;
&lt;div class="sourceCode" id="cb1"&gt;&lt;pre class="sourceCode cpp"&gt;&lt;code class="sourceCode cpp"&gt;&lt;span id="cb1-1"&gt;&lt;a aria-hidden="true" href="#cb1-1" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="kw"&gt;namespace&lt;/span&gt; xla &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-2"&gt;&lt;a aria-hidden="true" href="#cb1-2" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-3"&gt;&lt;a aria-hidden="true" href="#cb1-3" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;// Attempts to establish aliases between input and output buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-4"&gt;&lt;a aria-hidden="true" href="#cb1-4" tabindex="-1"&gt;&lt;/a&gt;absl&lt;span class="op"&gt;::&lt;/span&gt;StatusOr&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="dt"&gt;bool&lt;/span&gt;&lt;span class="op"&gt;&amp;gt;&lt;/span&gt; OptimizeInputOutputBufferAlias&lt;span class="op"&gt;::&lt;/span&gt;Build&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-5"&gt;&lt;a aria-hidden="true" href="#cb1-5" tabindex="-1"&gt;&lt;/a&gt;    absl&lt;span class="op"&gt;::&lt;/span&gt;Span&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;gt;&lt;/span&gt; input_shapes&lt;span class="op"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span id="cb1-6"&gt;&lt;a aria-hidden="true" href="#cb1-6" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; output_shape&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-7"&gt;&lt;a aria-hidden="true" href="#cb1-7" tabindex="-1"&gt;&lt;/a&gt;    HloInputOutputAliasConfig&lt;span class="op"&gt;*&lt;/span&gt; alias_config&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-8"&gt;&lt;a aria-hidden="true" href="#cb1-8" tabindex="-1"&gt;&lt;/a&gt;    HloBufferDonorConfig&lt;span class="op"&gt;*&lt;/span&gt; buffer_donor_config&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-9"&gt;&lt;a aria-hidden="true" href="#cb1-9" tabindex="-1"&gt;&lt;/a&gt;  &lt;/span&gt;
&lt;span id="cb1-10"&gt;&lt;a aria-hidden="true" href="#cb1-10" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="dt"&gt;bool&lt;/span&gt; changed &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="kw"&gt;false&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-11"&gt;&lt;a aria-hidden="true" href="#cb1-11" tabindex="-1"&gt;&lt;/a&gt;  &lt;/span&gt;
&lt;span id="cb1-12"&gt;&lt;a aria-hidden="true" href="#cb1-12" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Disable aliasing if the output has a dynamic shape.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-13"&gt;&lt;a aria-hidden="true" href="#cb1-13" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;output_shape&lt;span class="op"&gt;.&lt;/span&gt;is_dynamic&lt;span class="op"&gt;())&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-14"&gt;&lt;a aria-hidden="true" href="#cb1-14" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="kw"&gt;false&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-15"&gt;&lt;a aria-hidden="true" href="#cb1-15" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-16"&gt;&lt;a aria-hidden="true" href="#cb1-16" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-17"&gt;&lt;a aria-hidden="true" href="#cb1-17" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Structures to collect potential donor input buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-18"&gt;&lt;a aria-hidden="true" href="#cb1-18" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="kw"&gt;struct&lt;/span&gt; DonorEntry &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-19"&gt;&lt;a aria-hidden="true" href="#cb1-19" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; param_number&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-20"&gt;&lt;a aria-hidden="true" href="#cb1-20" tabindex="-1"&gt;&lt;/a&gt;    ShapeIndex index&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-21"&gt;&lt;a aria-hidden="true" href="#cb1-21" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; shape_size&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-22"&gt;&lt;a aria-hidden="true" href="#cb1-22" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;};&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-23"&gt;&lt;a aria-hidden="true" href="#cb1-23" tabindex="-1"&gt;&lt;/a&gt;  absl&lt;span class="op"&gt;::&lt;/span&gt;flat_hash_map&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="dt"&gt;int64_t&lt;/span&gt;&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="bu"&gt;std::&lt;/span&gt;vector&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;DonorEntry&lt;span class="op"&gt;&amp;gt;&amp;gt;&lt;/span&gt; donors&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-24"&gt;&lt;a aria-hidden="true" href="#cb1-24" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-25"&gt;&lt;a aria-hidden="true" href="#cb1-25" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Populate donors: traverse each input's subshapes and collect eligible buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-26"&gt;&lt;a aria-hidden="true" href="#cb1-26" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;for&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="dt"&gt;int64_t&lt;/span&gt; param_number &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt; param_number &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; input_shapes&lt;span class="op"&gt;.&lt;/span&gt;size&lt;span class="op"&gt;();&lt;/span&gt; &lt;span class="op"&gt;++&lt;/span&gt;param_number&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-27"&gt;&lt;a aria-hidden="true" href="#cb1-27" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; input_shape &lt;span class="op"&gt;=&lt;/span&gt; input_shapes&lt;span class="op"&gt;[&lt;/span&gt;param_number&lt;span class="op"&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-28"&gt;&lt;a aria-hidden="true" href="#cb1-28" tabindex="-1"&gt;&lt;/a&gt;    TF_RET_CHECK&lt;span class="op"&gt;(&lt;/span&gt;LayoutUtil&lt;span class="op"&gt;::&lt;/span&gt;HasLayout&lt;span class="op"&gt;(&lt;/span&gt;input_shape&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-29"&gt;&lt;a aria-hidden="true" href="#cb1-29" tabindex="-1"&gt;&lt;/a&gt;    ShapeUtil&lt;span class="op"&gt;::&lt;/span&gt;ForEachSubshape&lt;span class="op"&gt;(&lt;/span&gt;input_shape&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="op"&gt;[&amp;amp;](&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; subshape&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="at"&gt;const&lt;/span&gt; ShapeIndex&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; index&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-30"&gt;&lt;a aria-hidden="true" href="#cb1-30" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(!&lt;/span&gt;LayoutUtil&lt;span class="op"&gt;::&lt;/span&gt;IsDenseArray&lt;span class="op"&gt;(&lt;/span&gt;subshape&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;||&lt;/span&gt; subshape&lt;span class="op"&gt;.&lt;/span&gt;is_dynamic&lt;span class="op"&gt;())&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-31"&gt;&lt;a aria-hidden="true" href="#cb1-31" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-32"&gt;&lt;a aria-hidden="true" href="#cb1-32" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-33"&gt;&lt;a aria-hidden="true" href="#cb1-33" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;alias_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;ParameterHasAlias&lt;span class="op"&gt;(&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; index&lt;span class="op"&gt;))&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-34"&gt;&lt;a aria-hidden="true" href="#cb1-34" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-35"&gt;&lt;a aria-hidden="true" href="#cb1-35" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-36"&gt;&lt;a aria-hidden="true" href="#cb1-36" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="va"&gt;registered_buffer_donor_only_&lt;/span&gt; &lt;span class="op"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-37"&gt;&lt;a aria-hidden="true" href="#cb1-37" tabindex="-1"&gt;&lt;/a&gt;          &lt;span class="op"&gt;!&lt;/span&gt;buffer_donor_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;ParameterIsBufferDonor&lt;span class="op"&gt;(&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; index&lt;span class="op"&gt;))&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-38"&gt;&lt;a aria-hidden="true" href="#cb1-38" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-39"&gt;&lt;a aria-hidden="true" href="#cb1-39" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-40"&gt;&lt;a aria-hidden="true" href="#cb1-40" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="dt"&gt;int64_t&lt;/span&gt; memory_space &lt;span class="op"&gt;=&lt;/span&gt; subshape&lt;span class="op"&gt;.&lt;/span&gt;layout&lt;span class="op"&gt;().&lt;/span&gt;memory_space&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-41"&gt;&lt;a aria-hidden="true" href="#cb1-41" tabindex="-1"&gt;&lt;/a&gt;      donors&lt;span class="op"&gt;[&lt;/span&gt;memory_space&lt;span class="op"&gt;].&lt;/span&gt;emplace_back&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-42"&gt;&lt;a aria-hidden="true" href="#cb1-42" tabindex="-1"&gt;&lt;/a&gt;          DonorEntry&lt;span class="op"&gt;{&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; index&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="va"&gt;shape_size_fn_&lt;/span&gt;&lt;span class="op"&gt;(&lt;/span&gt;subshape&lt;span class="op"&gt;)});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-43"&gt;&lt;a aria-hidden="true" href="#cb1-43" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-44"&gt;&lt;a aria-hidden="true" href="#cb1-44" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-45"&gt;&lt;a aria-hidden="true" href="#cb1-45" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-46"&gt;&lt;a aria-hidden="true" href="#cb1-46" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Structures to collect potential donee output buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-47"&gt;&lt;a aria-hidden="true" href="#cb1-47" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="kw"&gt;struct&lt;/span&gt; DoneeEntry &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-48"&gt;&lt;a aria-hidden="true" href="#cb1-48" tabindex="-1"&gt;&lt;/a&gt;    ShapeIndex index&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-49"&gt;&lt;a aria-hidden="true" href="#cb1-49" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; shape_size&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-50"&gt;&lt;a aria-hidden="true" href="#cb1-50" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;};&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-51"&gt;&lt;a aria-hidden="true" href="#cb1-51" tabindex="-1"&gt;&lt;/a&gt;  absl&lt;span class="op"&gt;::&lt;/span&gt;flat_hash_map&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="dt"&gt;int64_t&lt;/span&gt;&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="bu"&gt;std::&lt;/span&gt;vector&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;DoneeEntry&lt;span class="op"&gt;&amp;gt;&amp;gt;&lt;/span&gt; donees&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-52"&gt;&lt;a aria-hidden="true" href="#cb1-52" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-53"&gt;&lt;a aria-hidden="true" href="#cb1-53" tabindex="-1"&gt;&lt;/a&gt;  TF_RET_CHECK&lt;span class="op"&gt;(&lt;/span&gt;LayoutUtil&lt;span class="op"&gt;::&lt;/span&gt;HasLayout&lt;span class="op"&gt;(&lt;/span&gt;output_shape&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-54"&gt;&lt;a aria-hidden="true" href="#cb1-54" tabindex="-1"&gt;&lt;/a&gt;  ShapeUtil&lt;span class="op"&gt;::&lt;/span&gt;ForEachSubshape&lt;span class="op"&gt;(&lt;/span&gt;output_shape&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="op"&gt;[&amp;amp;](&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; subshape&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="at"&gt;const&lt;/span&gt; ShapeIndex&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; index&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-55"&gt;&lt;a aria-hidden="true" href="#cb1-55" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(!&lt;/span&gt;LayoutUtil&lt;span class="op"&gt;::&lt;/span&gt;IsDenseArray&lt;span class="op"&gt;(&lt;/span&gt;subshape&lt;span class="op"&gt;))&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-56"&gt;&lt;a aria-hidden="true" href="#cb1-56" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-57"&gt;&lt;a aria-hidden="true" href="#cb1-57" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-58"&gt;&lt;a aria-hidden="true" href="#cb1-58" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;alias_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;OutputHasAlias&lt;span class="op"&gt;(&lt;/span&gt;index&lt;span class="op"&gt;))&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-59"&gt;&lt;a aria-hidden="true" href="#cb1-59" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-60"&gt;&lt;a aria-hidden="true" href="#cb1-60" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-61"&gt;&lt;a aria-hidden="true" href="#cb1-61" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; memory_space &lt;span class="op"&gt;=&lt;/span&gt; subshape&lt;span class="op"&gt;.&lt;/span&gt;layout&lt;span class="op"&gt;().&lt;/span&gt;memory_space&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-62"&gt;&lt;a aria-hidden="true" href="#cb1-62" tabindex="-1"&gt;&lt;/a&gt;    donees&lt;span class="op"&gt;[&lt;/span&gt;memory_space&lt;span class="op"&gt;].&lt;/span&gt;emplace_back&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-63"&gt;&lt;a aria-hidden="true" href="#cb1-63" tabindex="-1"&gt;&lt;/a&gt;        DoneeEntry&lt;span class="op"&gt;{&lt;/span&gt;index&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="va"&gt;shape_size_fn_&lt;/span&gt;&lt;span class="op"&gt;(&lt;/span&gt;subshape&lt;span class="op"&gt;)});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-64"&gt;&lt;a aria-hidden="true" href="#cb1-64" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-65"&gt;&lt;a aria-hidden="true" href="#cb1-65" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-66"&gt;&lt;a aria-hidden="true" href="#cb1-66" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// For each memory space, match donors and donees based on shape size.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-67"&gt;&lt;a aria-hidden="true" href="#cb1-67" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;for&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; &lt;span class="op"&gt;[&lt;/span&gt;memory_space&lt;span class="op"&gt;,&lt;/span&gt; donor_vector&lt;span class="op"&gt;]&lt;/span&gt; &lt;span class="op"&gt;:&lt;/span&gt; donors&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-68"&gt;&lt;a aria-hidden="true" href="#cb1-68" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="kw"&gt;auto&lt;/span&gt; donee_it &lt;span class="op"&gt;=&lt;/span&gt; donees&lt;span class="op"&gt;.&lt;/span&gt;find&lt;span class="op"&gt;(&lt;/span&gt;memory_space&lt;span class="op"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-69"&gt;&lt;a aria-hidden="true" href="#cb1-69" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;donee_it &lt;span class="op"&gt;==&lt;/span&gt; donees&lt;span class="op"&gt;.&lt;/span&gt;end&lt;span class="op"&gt;())&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-70"&gt;&lt;a aria-hidden="true" href="#cb1-70" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;continue&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-71"&gt;&lt;a aria-hidden="true" href="#cb1-71" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-72"&gt;&lt;a aria-hidden="true" href="#cb1-72" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; donee_vector &lt;span class="op"&gt;=&lt;/span&gt; donee_it&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;second&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-73"&gt;&lt;a aria-hidden="true" href="#cb1-73" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-74"&gt;&lt;a aria-hidden="true" href="#cb1-74" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;// Sort both donors and donees by decreasing size to maximize reuse of large buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-75"&gt;&lt;a aria-hidden="true" href="#cb1-75" tabindex="-1"&gt;&lt;/a&gt;    absl&lt;span class="op"&gt;::&lt;/span&gt;c_stable_sort&lt;span class="op"&gt;(&lt;/span&gt;donor_vector&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-76"&gt;&lt;a aria-hidden="true" href="#cb1-76" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="op"&gt;[](&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; DonorEntry&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; a&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="at"&gt;const&lt;/span&gt; DonorEntry&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; b&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt; &lt;span class="cf"&gt;return&lt;/span&gt; a&lt;span class="op"&gt;.&lt;/span&gt;shape_size &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; b&lt;span class="op"&gt;.&lt;/span&gt;shape_size&lt;span class="op"&gt;;&lt;/span&gt; &lt;span class="op"&gt;});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-77"&gt;&lt;a aria-hidden="true" href="#cb1-77" tabindex="-1"&gt;&lt;/a&gt;    absl&lt;span class="op"&gt;::&lt;/span&gt;c_stable_sort&lt;span class="op"&gt;(&lt;/span&gt;donee_vector&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-78"&gt;&lt;a aria-hidden="true" href="#cb1-78" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="op"&gt;[](&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; DoneeEntry&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; a&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="at"&gt;const&lt;/span&gt; DoneeEntry&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; b&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt; &lt;span class="cf"&gt;return&lt;/span&gt; a&lt;span class="op"&gt;.&lt;/span&gt;shape_size &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; b&lt;span class="op"&gt;.&lt;/span&gt;shape_size&lt;span class="op"&gt;;&lt;/span&gt; &lt;span class="op"&gt;});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-79"&gt;&lt;a aria-hidden="true" href="#cb1-79" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-80"&gt;&lt;a aria-hidden="true" href="#cb1-80" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;// Two-pointer matching: match donor and donee of the same size.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-81"&gt;&lt;a aria-hidden="true" href="#cb1-81" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; donor_vector_index &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-82"&gt;&lt;a aria-hidden="true" href="#cb1-82" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; donee_vector_index &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-83"&gt;&lt;a aria-hidden="true" href="#cb1-83" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;while&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;donor_vector_index &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; donor_vector&lt;span class="op"&gt;.&lt;/span&gt;size&lt;span class="op"&gt;()&lt;/span&gt; &lt;span class="op"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-84"&gt;&lt;a aria-hidden="true" href="#cb1-84" tabindex="-1"&gt;&lt;/a&gt;           donee_vector_index &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; donee_vector&lt;span class="op"&gt;.&lt;/span&gt;size&lt;span class="op"&gt;())&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-85"&gt;&lt;a aria-hidden="true" href="#cb1-85" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="at"&gt;const&lt;/span&gt; &lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; donor &lt;span class="op"&gt;=&lt;/span&gt; donor_vector&lt;span class="op"&gt;[&lt;/span&gt;donor_vector_index&lt;span class="op"&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-86"&gt;&lt;a aria-hidden="true" href="#cb1-86" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="at"&gt;const&lt;/span&gt; &lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; donee &lt;span class="op"&gt;=&lt;/span&gt; donee_vector&lt;span class="op"&gt;[&lt;/span&gt;donee_vector_index&lt;span class="op"&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-87"&gt;&lt;a aria-hidden="true" href="#cb1-87" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;donor&lt;span class="op"&gt;.&lt;/span&gt;shape_size &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; donee&lt;span class="op"&gt;.&lt;/span&gt;shape_size&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-88"&gt;&lt;a aria-hidden="true" href="#cb1-88" tabindex="-1"&gt;&lt;/a&gt;        donor_vector_index&lt;span class="op"&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-89"&gt;&lt;a aria-hidden="true" href="#cb1-89" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt; &lt;span class="cf"&gt;else&lt;/span&gt; &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;donor&lt;span class="op"&gt;.&lt;/span&gt;shape_size &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; donee&lt;span class="op"&gt;.&lt;/span&gt;shape_size&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-90"&gt;&lt;a aria-hidden="true" href="#cb1-90" tabindex="-1"&gt;&lt;/a&gt;        donee_vector_index&lt;span class="op"&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-91"&gt;&lt;a aria-hidden="true" href="#cb1-91" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt; &lt;span class="cf"&gt;else&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-92"&gt;&lt;a aria-hidden="true" href="#cb1-92" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;// Match found: establish alias and remove donor from pool.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-93"&gt;&lt;a aria-hidden="true" href="#cb1-93" tabindex="-1"&gt;&lt;/a&gt;        TF_RETURN_IF_ERROR&lt;span class="op"&gt;(&lt;/span&gt;alias_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;SetUpAlias&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-94"&gt;&lt;a aria-hidden="true" href="#cb1-94" tabindex="-1"&gt;&lt;/a&gt;            donee&lt;span class="op"&gt;.&lt;/span&gt;index&lt;span class="op"&gt;,&lt;/span&gt; donor&lt;span class="op"&gt;.&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; donor&lt;span class="op"&gt;.&lt;/span&gt;index&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-95"&gt;&lt;a aria-hidden="true" href="#cb1-95" tabindex="-1"&gt;&lt;/a&gt;        TF_RETURN_IF_ERROR&lt;span class="op"&gt;(&lt;/span&gt;buffer_donor_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;RemoveBufferDonor&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-96"&gt;&lt;a aria-hidden="true" href="#cb1-96" tabindex="-1"&gt;&lt;/a&gt;            donor&lt;span class="op"&gt;.&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; donor&lt;span class="op"&gt;.&lt;/span&gt;index&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-97"&gt;&lt;a aria-hidden="true" href="#cb1-97" tabindex="-1"&gt;&lt;/a&gt;        donor_vector_index&lt;span class="op"&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-98"&gt;&lt;a aria-hidden="true" href="#cb1-98" tabindex="-1"&gt;&lt;/a&gt;        donee_vector_index&lt;span class="op"&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-99"&gt;&lt;a aria-hidden="true" href="#cb1-99" tabindex="-1"&gt;&lt;/a&gt;        changed &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="kw"&gt;true&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-100"&gt;&lt;a aria-hidden="true" href="#cb1-100" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-101"&gt;&lt;a aria-hidden="true" href="#cb1-101" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-102"&gt;&lt;a aria-hidden="true" href="#cb1-102" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-103"&gt;&lt;a aria-hidden="true" href="#cb1-103" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-104"&gt;&lt;a aria-hidden="true" href="#cb1-104" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;return&lt;/span&gt; changed&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-105"&gt;&lt;a aria-hidden="true" href="#cb1-105" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-106"&gt;&lt;a aria-hidden="true" href="#cb1-106" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-107"&gt;&lt;a aria-hidden="true" href="#cb1-107" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;// Entry point for running the alias optimization on an HLO module.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-108"&gt;&lt;a aria-hidden="true" href="#cb1-108" tabindex="-1"&gt;&lt;/a&gt;absl&lt;span class="op"&gt;::&lt;/span&gt;StatusOr&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="dt"&gt;bool&lt;/span&gt;&lt;span class="op"&gt;&amp;gt;&lt;/span&gt; OptimizeInputOutputBufferAlias&lt;span class="op"&gt;::&lt;/span&gt;Run&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-109"&gt;&lt;a aria-hidden="true" href="#cb1-109" tabindex="-1"&gt;&lt;/a&gt;    HloModule&lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-110"&gt;&lt;a aria-hidden="true" href="#cb1-110" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="at"&gt;const&lt;/span&gt; absl&lt;span class="op"&gt;::&lt;/span&gt;flat_hash_set&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;absl&lt;span class="op"&gt;::&lt;/span&gt;string_view&lt;span class="op"&gt;&amp;gt;&amp;amp;&lt;/span&gt; execution_threads&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-111"&gt;&lt;a aria-hidden="true" href="#cb1-111" tabindex="-1"&gt;&lt;/a&gt;  &lt;/span&gt;
&lt;span id="cb1-112"&gt;&lt;a aria-hidden="true" href="#cb1-112" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Extract input and output shapes from the module entry computation.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-113"&gt;&lt;a aria-hidden="true" href="#cb1-113" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="at"&gt;const&lt;/span&gt; &lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; entry_computation_layout &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;entry_computation_layout&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-114"&gt;&lt;a aria-hidden="true" href="#cb1-114" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="bu"&gt;std::&lt;/span&gt;vector&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;Shape&lt;span class="op"&gt;&amp;gt;&lt;/span&gt; input_shapes&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-115"&gt;&lt;a aria-hidden="true" href="#cb1-115" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;for&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="dt"&gt;int64_t&lt;/span&gt; i &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt; i &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; &lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;entry_computation&lt;span class="op"&gt;()-&amp;gt;&lt;/span&gt;num_parameters&lt;span class="op"&gt;();&lt;/span&gt; &lt;span class="op"&gt;++&lt;/span&gt;i&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-116"&gt;&lt;a aria-hidden="true" href="#cb1-116" tabindex="-1"&gt;&lt;/a&gt;    input_shapes&lt;span class="op"&gt;.&lt;/span&gt;push_back&lt;span class="op"&gt;(&lt;/span&gt;entry_computation_layout&lt;span class="op"&gt;.&lt;/span&gt;parameter_shape&lt;span class="op"&gt;(&lt;/span&gt;i&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-117"&gt;&lt;a aria-hidden="true" href="#cb1-117" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-118"&gt;&lt;a aria-hidden="true" href="#cb1-118" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; output_shape &lt;span class="op"&gt;=&lt;/span&gt; entry_computation_layout&lt;span class="op"&gt;.&lt;/span&gt;result_shape&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-119"&gt;&lt;a aria-hidden="true" href="#cb1-119" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-120"&gt;&lt;a aria-hidden="true" href="#cb1-120" tabindex="-1"&gt;&lt;/a&gt;  HloInputOutputAliasConfig&lt;span class="op"&gt;*&lt;/span&gt; alias_config &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="op"&gt;&amp;amp;&lt;/span&gt;&lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;input_output_alias_config&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-121"&gt;&lt;a aria-hidden="true" href="#cb1-121" tabindex="-1"&gt;&lt;/a&gt;  HloBufferDonorConfig&lt;span class="op"&gt;*&lt;/span&gt; buffer_donor_config &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="op"&gt;&amp;amp;&lt;/span&gt;&lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;buffer_donor_config&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-122"&gt;&lt;a aria-hidden="true" href="#cb1-122" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-123"&gt;&lt;a aria-hidden="true" href="#cb1-123" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Attempt to build aliasing configuration.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-124"&gt;&lt;a aria-hidden="true" href="#cb1-124" tabindex="-1"&gt;&lt;/a&gt;  TF_ASSIGN_OR_RETURN&lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="dt"&gt;bool&lt;/span&gt; changed&lt;span class="op"&gt;,&lt;/span&gt; Build&lt;span class="op"&gt;(&lt;/span&gt;input_shapes&lt;span class="op"&gt;,&lt;/span&gt; output_shape&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-125"&gt;&lt;a aria-hidden="true" href="#cb1-125" tabindex="-1"&gt;&lt;/a&gt;                                          alias_config&lt;span class="op"&gt;,&lt;/span&gt; buffer_donor_config&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-126"&gt;&lt;a aria-hidden="true" href="#cb1-126" tabindex="-1"&gt;&lt;/a&gt;  &lt;/span&gt;
&lt;span id="cb1-127"&gt;&lt;a aria-hidden="true" href="#cb1-127" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Verify that the resulting alias configuration is correct.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-128"&gt;&lt;a aria-hidden="true" href="#cb1-128" tabindex="-1"&gt;&lt;/a&gt;  TF_RETURN_IF_ERROR&lt;span class="op"&gt;(&lt;/span&gt;alias_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;Verify&lt;span class="op"&gt;(*&lt;/span&gt;&lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="va"&gt;shape_size_fn_&lt;/span&gt;&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-129"&gt;&lt;a aria-hidden="true" href="#cb1-129" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-130"&gt;&lt;a aria-hidden="true" href="#cb1-130" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;return&lt;/span&gt; changed&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-131"&gt;&lt;a aria-hidden="true" href="#cb1-131" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-132"&gt;&lt;a aria-hidden="true" href="#cb1-132" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-133"&gt;&lt;a aria-hidden="true" href="#cb1-133" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="op"&gt;}&lt;/span&gt; &lt;span class="co"&gt;// namespace xla&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code performs input preparation, donor and donee collection,
size-based sorting, two-pointer matching of donors and donees, alias
setup, and verification of the final configuration.&lt;/p&gt;
&lt;h2 id="dynamic-shape-handling-on-aliasing"&gt;Dynamic Shape Handling on
Aliasing&lt;/h2&gt;
&lt;p&gt;Dynamic shapes introduce uncertainty in buffer sizes during runtime.
If an output tensor’s shape is dynamic, the memory required cannot be
reliably determined at compile time. As a result, XLA restricts aliasing
for dynamic shapes by disabling optimization whenever the output is
dynamic. Allowing aliasing for dynamic outputs could lead to incorrect
buffer allocations and data corruption if the output grows larger than
the input buffer. Verifying correctness would also require complex
runtime checks that can negate performance benefits. By disallowing
aliasing in such cases, XLA ensures a simple and predictable memory
model, though at the cost of missed optimization opportunities for
dynamic workloads.&lt;/p&gt;
&lt;p&gt;Recent research, such as work on dynamic bufferization in MLIR&lt;a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, proposes more sophisticated methods
to handle dynamic shapes while minimizing overhead.&lt;/p&gt;
&lt;h2 id="safe-aliasing-with-dynamic-shapes"&gt;Safe Aliasing with Dynamic
Shapes&lt;/h2&gt;
&lt;p&gt;One strategy to safely allow aliasing with dynamic shapes is to
introduce runtime shape guards. These guards would validate buffer
compatibility at execution time, only enabling aliasing if the actual
shapes are compatible. Another strategy involves over-allocating buffers
with conservative margins so that minor runtime variations can still be
safely accommodated. A third strategy is deferred memory binding, in
which buffer reuse decisions are delayed until runtime when precise
shape information is available. These approaches trade a small amount of
runtime complexity for potentially significant memory savings.&lt;/p&gt;
&lt;h2 id="aliasing-optimization-to-cross-computation-buffer-reuse"&gt;Aliasing
Optimization to Cross-Computation Buffer Reuse&lt;/h2&gt;
&lt;p&gt;To extend aliasing beyond a single computation, XLA would require
global buffer lifetime analysis across multiple HLO computations. By
analyzing liveness and memory usage across function calls and loops, it
would be possible to recycle buffers globally. Techniques like memory
pooling and cross-computation liveness tracking, adapted from
traditional compiler research &lt;a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, could help enable
aliasing. Such global optimizations would offer further memory
reductions for large-scale training workloads.&lt;/p&gt;
&lt;h2 id="trade-offs-between-compile-time-and-runtime-aliasing"&gt;Trade-offs
Between Compile-Time and Runtime Aliasing&lt;/h2&gt;
&lt;p&gt;Compile-time aliasing offers a simpler and predictable execution
model with no runtime overhead, but it is inherently conservative. It
cannot exploit dynamic input properties or adapt to changing workloads.
Runtime aliasing decisions, in contrast, allow more aggressive
optimization by adapting to actual runtime shapes, but introduce
additional complexity and potential variability in performance. Hybrid
models that integrate compile-time planning with lightweight runtime
validation, such as those explored in MLIR &lt;a class="footnote-ref" href="#fn4" id="fnref4" role="doc-noteref"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;,
are promising future directions.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;section class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;“XLA: Optimizing Compiler for Machine Learning.” OpenXLA
Project, https://openxla.org/xla/tf2xla. Accessed 20 Dec. 2023.&lt;a class="footnote-back" href="#fnref1" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Shape Inference - MLIR.
https://mlir.llvm.org/docs/ShapeInference/. Accessed 20 Dec. 2023.&lt;a class="footnote-back" href="#fnref2" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Muchnick, Steven. Advanced compiler design
implementation. Morgan kaufmann, 1997.&lt;a class="footnote-back" href="#fnref3" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;Levental, Maksim. “Memory planning for deep neural
networks.” arXiv preprint arXiv:2203.00448 (2022).&lt;a class="footnote-back" href="#fnref4" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="Neuron"></category><category term="ml-code"></category><category term="llm"></category><category term="ml-acceleration"></category><category term="hpc-concept"></category></entry><entry><title>All Reduce Decomposition</title><link href="https://sengopal.me/posts/all-reduce-decomposition" rel="alternate"></link><published>2023-12-20T00:00:00-08:00</published><updated>2023-12-20T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-20:/posts/all-reduce-decomposition</id><summary type="html">This post works through the metrics that are critical for ML inference</summary><content type="html">&lt;p&gt;As part of LLM serving, AllReduce is a common, but expensive
operation which also blocks compute utilization. A common improvement
would be to replace AllGather with a computationally equivalent
ReduceScatter + AllGather&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReduceScatter: &lt;/strong&gt; In this step, each process performs
a reduction operation (e.g., sum, product, max, min) on its input data.
The resulting reduced values are then scattered across all processes,
such that &lt;strong&gt;each process receives a distinct portion of the
overall reduction result.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AllGather:&lt;/strong&gt; After the ReduceScatter step, each
process holds a different portion of the overall reduction result. The
AllGather operation then &lt;strong&gt;collects all these partial results from
all processes&lt;/strong&gt; and distributes the complete result to every
process.&lt;/p&gt;
&lt;p&gt;By breaking down AllReduce into these two steps, it can potentially
improve performance and communication efficiency, particularly in
certain parallel computing architectures or communication patterns. Some
parallel computing libraries or frameworks may implement AllReduce as a
single operation or provide optimized implementations that combine the
two steps for better performance.&lt;/p&gt;
&lt;h2 id="diagram-explanation"&gt;Diagram Explanation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;     Initial State        Reduce-Scatter         AllGather          Final State
   ┌───┬───┬───┬───┐    ┌───┬───┬───┬───┐    ┌───┬───┬───┬───┐    ┌───┬───┬───┬───┐
D0 │ A │ B │ C │ D │ →  │ W │   │   │   │ →  │ W │ X │ Y │ Z │ →  │ W │ X │ Y │ Z │
   ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤
D1 │ E │ F │ G │ H │ →  │   │ X │   │   │ →  │ W │ X │ Y │ Z │ →  │ W │ X │ Y │ Z │
   ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤
D2 │ I │ J │ K │ L │ →  │   │   │ Y │   │ →  │ W │ X │ Y │ Z │ →  │ W │ X │ Y │ Z │
   ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤
D3 │ M │ N │ O │ P │ →  │   │   │   │ Z │ →  │ W │ X │ Y │ Z │ →  │ W │ X │ Y │ Z │
   └───┴───┴───┴───┘    └───┴───┴───┴───┘    └───┴───┴───┴───┘    └───┴───┴───┴───┘&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt;&lt;/p&gt;
&lt;ol type="1"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Initial State&lt;/strong&gt;: Each device (D0, D1, D2, D3) has
its own data (A-P).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reduce-Scatter&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each device performs a partial reduction on a specific portion of
the data.&lt;/li&gt;
&lt;li&gt;W = reduction of (A, E, I, M)&lt;/li&gt;
&lt;li&gt;X = reduction of (B, F, J, N)&lt;/li&gt;
&lt;li&gt;Y = reduction of (C, G, K, O)&lt;/li&gt;
&lt;li&gt;Z = reduction of (D, H, L, P)&lt;/li&gt;
&lt;li&gt;The results are scattered across devices.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;AllGather&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each device gathers the partial results from all other devices.&lt;/li&gt;
&lt;li&gt;All devices now have the complete reduced result (W, X, Y, Z).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Final State&lt;/strong&gt;: All devices have the same, complete
reduced result.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This decomposition can be more efficient than a direct AllReduce in
certain network topologies and for large data sizes. It allows for
better utilization of network bandwidth and can reduce overall
communication time&lt;a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="citations"&gt;Citations&lt;/h2&gt;
&lt;section class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;https://marek.ai/allreduce-the-basis-of-multi-device-communication-for-neural-network-training.html&lt;a class="footnote-back" href="#fnref1" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html&lt;a class="footnote-back" href="#fnref2" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html&lt;a class="footnote-back" href="#fnref3" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="Neuron"></category><category term="ml-code"></category><category term="llm"></category><category term="ml-acceleration"></category><category term="hpc-concept"></category></entry><entry><title>Concept - Coalescing</title><link href="https://sengopal.me/posts/concept-coalescing" rel="alternate"></link><published>2023-12-20T00:00:00-08:00</published><updated>2023-12-20T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-20:/posts/concept-coalescing</id><summary type="html">This post works through the metrics that are critical for ML inference</summary><content type="html">&lt;p&gt;Memory Coalescing is a fundamental performance consideration in HPC
programming&lt;a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and provides another dimension for
memory efficiency and how &lt;strong&gt;aligned versus misaligned memory
accesses&lt;/strong&gt; can significantly impact throughput.&lt;/p&gt;
&lt;p&gt;The global memory of a CUDA device is implemented with DRAMs. Each
time a DRAM location is accessed, a range of consecutive locations that
includes the requested location is actually accessed. Many sensors are
provided in each DRAM chip and they work in parallel. Each senses the
content of a bit within these consecutive locations. Once detected by
the sensors, the data from all these consecutive locations can be
transferred at very high-speed to the processor. These consecutive
locations accessed and delivered are referred to as DRAM bursts.&lt;/p&gt;
&lt;p&gt;Recognizing the burst mechanism, current CUDA devices employ a
technique that allows the programmers to achieve high global memory
access efficiency by organizing memory access of threads into favorable
patterns. This technique takes advantage of the fact that threads in a
warp execute the same instruction at any given point in time (SIMT).
When all threads in a warp execute a load instruction, the hardware
detects whether they access consecutive global memory locations. If they
do, the hardware combines, or coalesces, all these accesses into a
consolidated access to consecutive DRAM locations. Such coalesced access
allows the DRAMs to deliver data as a burst.&lt;/p&gt;
&lt;h2 id="memory-layout-semantics-and-access-patterns"&gt;Memory Layout
Semantics and Access Patterns&lt;/h2&gt;
&lt;p&gt;Programming languages provide abstract representations of matrices
and arrays, but underneath these abstractions, memory is always linear.
In C/C++ and CUDA, multidimensional arrays are typically stored in
&lt;strong&gt;row-major order&lt;/strong&gt;, meaning consecutive elements of a row
occupy adjacent memory locations.&lt;/p&gt;
&lt;p&gt;This becomes critically relevant in CUDA, where &lt;strong&gt;thread
warps&lt;/strong&gt; execute memory accesses in parallel. When threads within
a warp access consecutive memory addresses, the hardware can
&lt;strong&gt;coalesce&lt;/strong&gt; those accesses into a single wide memory
transaction. Misaligned or scattered accesses, by contrast, require
independent transactions for each thread, resulting in substantial
performance degradation.&lt;/p&gt;
&lt;h3 id="coalesced-access-the-b-matrix"&gt;Coalesced Access – The &lt;em&gt;B&lt;/em&gt;
Matrix&lt;/h3&gt;
&lt;p&gt;Consider the classic case of dense matrix multiplication&lt;a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; When accessing the &lt;em&gt;B&lt;/em&gt;
matrix, each CUDA thread reads a distinct column. Since matrices are
stored in row-major order, reading columns means accessing
&lt;strong&gt;consecutive addresses&lt;/strong&gt; along a row — which are adjacent
in memory.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Image ref - CoffeeBeforeArch" src="/extras/images/coalescing/coalescing-1.png"/&gt;
&lt;figcaption aria-hidden="true"&gt;Image ref - CoffeeBeforeArch&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Image ref - CoffeeBeforeArch&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This access pattern enables &lt;strong&gt;coalescing&lt;/strong&gt;: instead of
multiple separate memory reads, the GPU issues a single transaction that
serves the entire warp. The result is reduced memory latency and
improved effective bandwidth.&lt;/p&gt;
&lt;h3 id="misaligned-access-the-a-matrix"&gt;Misaligned Access – The
&lt;em&gt;A&lt;/em&gt; Matrix&lt;/h3&gt;
&lt;p&gt;In contrast, accessing the &lt;em&gt;A&lt;/em&gt; matrix typically involves each
thread reading a different row and iterating across columns. Due to
row-major layout, rows are &lt;strong&gt;not adjacent in memory&lt;/strong&gt; —
they are separated by the width of the matrix.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Image ref - CoffeeBeforeArch" src="/extras/images/coalescing/coalescing-2.png"/&gt;
&lt;figcaption aria-hidden="true"&gt;Image ref - CoffeeBeforeArch&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Image ref - CoffeeBeforeArch&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As a result, even if thread accesses are conceptually aligned in the
matrix, they are &lt;strong&gt;thousands of elements apart&lt;/strong&gt; in memory.
These accesses are non-coalesced, forcing the hardware to issue multiple
independent transactions per warp. This behavior incurs significantly
higher latency and reduces throughput.&lt;/p&gt;
&lt;h2 id="optimization-via-transposition"&gt;Optimization via
Transposition&lt;/h2&gt;
&lt;p&gt;To address the misaligned access pattern in the &lt;em&gt;A&lt;/em&gt; matrix, a
simple yet effective transformation can be applied:
&lt;strong&gt;pre-transposing&lt;/strong&gt; the &lt;em&gt;A&lt;/em&gt; matrix prior to the
kernel launch. Transposition transforms rows into columns. When both A
and B matrices are accessed along columns, &lt;strong&gt;all memory accesses
become coalesced&lt;/strong&gt;, optimizing memory throughput without
introducing shared memory or tiling techniques. Implementation-wise,
this requires updating index calculations inside the kernel. Rather than
iterating over fixed rows, threads iterate across columns (mirroring the
access pattern used for B)&lt;a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="tldr"&gt;TLDR;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Row-major layout&lt;/strong&gt; requires careful design of access
patterns to ensure spatial locality&lt;a class="footnote-ref" href="#fn4" id="fnref4" role="doc-noteref"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Transposing input matrices can transform misaligned accesses into
coalesced accesses.&lt;/li&gt;
&lt;li&gt;Profiling should always validate performance assumptions; even minor
misalignments can incur major penalties.&lt;/li&gt;
&lt;li&gt;Optimization strategies must consider not only compute efficiency
but also &lt;strong&gt;global memory bandwidth and access
coalescing&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="references"&gt;References&lt;/h3&gt;
&lt;section class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;&lt;a href="https://github.com/coffeeBeforeArch"&gt;CoffeeBeforeArch&lt;/a&gt; Github
Repository&lt;a class="footnote-back" href="#fnref1" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=_qSP455IekE&amp;amp;t=154s"&gt;CoffeeBeforeArch&lt;/a&gt;
Youtube tutorial&lt;a class="footnote-back" href="#fnref2" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;https://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/lec35.html&lt;a class="footnote-back" href="#fnref3" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;https://medium.com/distributed-knowledge/cuda-memory-management-use-cases-f9d340f7c704&lt;a class="footnote-back" href="#fnref4" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="Neuron"></category><category term="ml-code"></category><category term="llm"></category><category term="ml-acceleration"></category></entry><entry><title>What is Neuron SDK</title><link href="https://sengopal.me/posts/what-is-neuron-sdk" rel="alternate"></link><published>2023-12-20T00:00:00-08:00</published><updated>2023-12-20T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-20:/posts/what-is-neuron-sdk</id><summary type="html">This post introduces AWS Neuron SDK - an SDK that streamlines deep learning and generative AI workloads on AWS Inferentia and Trainium by integrating with frameworks like PyTorch and JAX.</summary><content type="html">&lt;p&gt;&lt;strong&gt;AWS Neuron&lt;/strong&gt; is a software development kit (SDK)
designed to optimize deep learning and generative AI workloads on AWS
Inferentia and AWS Trainium-powered Amazon EC2 instances. It integrates
seamlessly with popular machine learning frameworks like PyTorch and
JAX, enabling developers to build, train, and deploy high-performance
models efficiently.&lt;/p&gt;
&lt;h3 id="neuron-sdk-components"&gt;Neuron SDK Components&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Neuron Compiler&lt;/strong&gt;&lt;br/&gt;
Translates machine learning models from frameworks such as PyTorch and
JAX into executable code optimized for Inferentia and Trainium
hardware.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Neuron Runtime&lt;/strong&gt;&lt;br/&gt;
Serves as the execution engine, managing the efficient operation of
compiled models on AWS hardware accelerators.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Developer Tools&lt;/strong&gt;&lt;br/&gt;
Provides utilities for monitoring, profiling, and debugging, offering
deep insights into model behavior and system performance.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="focus-areas"&gt;Focus Areas&lt;/h3&gt;
&lt;h4 id="feature-enablement"&gt;Feature Enablement&lt;/h4&gt;
&lt;p&gt;Integrates new inference features, such as floating-point
quantization, to enhance model performance on Neuron hardware. This
involves collaboration across the compiler, runtime, and tensor
management components.&lt;/p&gt;
&lt;h4 id="inference-techniques"&gt;Inference Techniques&lt;/h4&gt;
&lt;p&gt;Implements advanced methods like speculative decoding and look-ahead
decoding to improve inference speed for large language models, ensuring
these techniques are effectively supported by Neuron hardware.&lt;/p&gt;
&lt;h4 id="performance-optimization"&gt;Performance Optimization&lt;/h4&gt;
&lt;p&gt;Various strategies are used to enhance efficiency, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Batching&lt;/strong&gt;&lt;br/&gt;
Processes multiple inputs simultaneously to improve throughput,
particularly useful for cost-sensitive applications.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Pipelining&lt;/strong&gt;&lt;br/&gt;
Divides model execution across multiple NeuronCores to optimize data
flow and reduce latency, ideal for latency-critical
applications.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Overlapping Operations&lt;/strong&gt;&lt;br/&gt;
Executes tasks concurrently, such as overlapping data loading with
computation, to maximize resource utilization and minimize idle
time.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Operator Fusion&lt;/strong&gt;&lt;br/&gt;
Combines multiple operations into a single step to reduce memory
overhead and improve computational efficiency.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Quantization&lt;/strong&gt;&lt;br/&gt;
Reduces the precision of model weights and activations to lower memory
usage and increase inference speed, with minimal impact on
accuracy.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Custom C++ Operators&lt;/strong&gt;&lt;br/&gt;
Develops tailored operators to optimize specific model components for
enhanced performance in unique workloads.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;For more detailed information, refer to the official &lt;a href="https://awsdocs-neuron.readthedocs-hosted.com/"&gt;AWS Neuron
Documentation&lt;/a&gt;.&lt;/p&gt;</content><category term="Neuron"></category><category term="neuron"></category><category term="ml-code"></category><category term="llm"></category></entry><entry><title>Neuron Glossary</title><link href="https://sengopal.me/posts/neuron-glossary" rel="alternate"></link><published>2023-12-13T00:00:00-08:00</published><updated>2023-12-13T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-13:/posts/neuron-glossary</id><summary type="html">This post acts as a running glossary for Neuron and HPC related terms and technologies.</summary><content type="html">&lt;p&gt;Last Updated on &lt;em&gt;Mar 12, 2024&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The following are a running list of new terms that are encountered
while working on the Neuron stack. This acts as a jump off point into
deeper dives into these terms and context behind them&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FPGA (Field-Programmable Gate Array)&lt;/strong&gt; Unlike GPUs
with a fixed design, FPGAs are essentially blank slates. They contain a
fabric of programmable logic blocks that can be configured to perform
specific tasks. This flexibility allows FPGAs to be customized for a
wide range of applications, including cryptography, financial modeling,
and high-frequency trading.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ASIC&lt;/strong&gt; An ASIC is a chip designed for a specific
purpose. It offers high performance and efficiency for that particular
task because the hardware is optimized for it. This aligns exactly with
how Intel describes the Gaudi as a deep learning accelerator.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Arithmetic Intensity&lt;/strong&gt; Metric that quantifies the
ratio of computational operations (measured in floating-point
operations, or FLOPs) to data movement (measured in bytes) during a
computation. It helps determine whether a particular operation is
compute-bound or memory-bound. For example, applying the ReLU activation
function to a tensor involves reading 2 bytes, performing 1 comparison
operation, and writing 2 bytes per element, resulting in an arithmetic
intensity of 1 FLOP per 4 bytes accessed. This low ratio indicates that
such operations are typically memory-bound, meaning the time spent on
memory accesses exceeds the time spent on computations. &lt;a href="https://astralord.github.io/posts/transformer-inference-optimization-toolset/"&gt;Ref&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Gradient checkpointing /activation checkpointing&lt;/strong&gt; A
technique to reduce memory usage by clearing activations of certain
layers and recomputing them during a backward pass. Effectively, this
trades extra computation time for reduced memory usage. If a module is
checkpointed, at the end of a forward pass, the inputs to and outputs
from the module stay in memory.&lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html"&gt;ref&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Strength Reduction&lt;/strong&gt;&lt;br/&gt;
This optimization replaces computationally expensive operations with
equivalent but less costly ones. For example, replacing multiplication
by a constant with a shift operation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Constant Folding&lt;/strong&gt;&lt;br/&gt;
Evaluates constant expressions at compile time, replacing them with
their computed values to reduce runtime computation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Common Subexpression Elimination&lt;/strong&gt;&lt;br/&gt;
Identifies and eliminates duplicate calculations by reusing previously
computed values, enhancing efficiency.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dead-Code Elimination&lt;/strong&gt;&lt;br/&gt;
Removes code that does not affect the program’s outcome, such as
computations whose results are never used, thereby streamlining the
codebase.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scalar Replacement&lt;/strong&gt;&lt;br/&gt;
Replaces array references with scalar variables when possible, reducing
memory access overhead and improving performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If-Conversion&lt;/strong&gt;&lt;br/&gt;
Transforms conditional branches into conditional instructions,
minimizing branch penalties and enhancing instruction-level
parallelism.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Function Inlining&lt;/strong&gt;&lt;br/&gt;
Substitutes the body of a called function directly into the calling
code, eliminating call overhead and enabling further optimizations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Call Specialization&lt;/strong&gt;&lt;br/&gt;
Tailors function calls based on known arguments, creating specialized
versions of functions to improve performance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Peephole Optimizations&lt;/strong&gt;&lt;br/&gt;
Examines a small window of consecutive instructions to identify and
replace inefficient sequences with more efficient ones, enhancing code
quality.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Canonicalizing Loops&lt;/strong&gt;&lt;br/&gt;
Transforms loops to start at index 0 and stride by 1, which simplifies
the loop structure and may enable more optimizations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generating Constants&lt;/strong&gt;&lt;br/&gt;
Replaces index calculations with pre-computed constants, reducing the
need for computation during program execution.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Eliminating Load/Store Pairs&lt;/strong&gt;&lt;br/&gt;
Removes redundant load and store operations by combining them or
eliminating unnecessary memory accesses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;XLA (Accelerated Linear Algebra)&lt;/strong&gt;&lt;br/&gt;
XLA is a domain-specific compiler for linear algebra that optimizes
computations for machine learning models. It is particularly focused on
optimizing TensorFlow computations and improving performance on hardware
accelerators like GPUs and TPUs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TVM&lt;/strong&gt;&lt;br/&gt;
TVM is an open-source deep learning compiler stack designed to optimize
the performance of deep learning models across different hardware
platforms. It provides a flexible and efficient way to deploy models on
a wide range of devices, including CPUs, GPUs, and specialized
accelerators.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MLIR (Multi-Level Intermediate Representation)&lt;/strong&gt;&lt;br/&gt;
MLIR is an intermediate representation used to define and optimize
programs at multiple levels of abstraction. It is designed to facilitate
cross-compiler optimizations and improve the portability and efficiency
of code across different hardware architectures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLVM / GCC&lt;/strong&gt; LLVM is a modular and flexible compiler
infrastructure consisting of reusable components that provide
fine-grained control over code generation and optimization. Its
intermediate representation (IR) is portable and efficient, allowing it
to target various hardware architectures. In contrast, GCC is a
monolithic compiler system that integrates the compiler, assembler,
linker, and debugger, following a more traditional approach to code
generation and optimizations with a long history of use in production
environments. While LLVM’s modularity allows for greater extensibility
and customization, GCC tends to be less modular, requiring deeper
integration for adding features. LLVM excels in modern hardware
optimizations, especially for GPUs and specialized accelerators, whereas
GCC is more focused on traditional CPU-based platforms and embedded
systems.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PCIe&lt;/strong&gt; high-speed interface standard used to connect
peripheral devices, such as graphics cards, network cards, and storage
devices, to the CPU and memory. It provides fast, low-latency data
transfer with a scalable architecture, offering multiple lanes for
simultaneous data communication. PCIe operates with a point-to-point
architecture, where each device communicates directly with the CPU
through a dedicated link, ensuring high performance and low overhead.
Its speed and flexibility make it ideal for modern hardware
accelerators, like GPUs and NVMe storage devices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Memory-Mapped I/O (MMIO)&lt;/strong&gt; technique where I/O devices
are mapped to specific memory addresses, allowing the CPU to communicate
with them as if they were part of the system’s memory. This method
provides a simple and efficient way to read from and write to I/O
devices using standard memory access instructions, without the need for
special I/O instructions. MMIO allows devices such as graphics cards,
network interfaces, and other peripherals to interact directly with the
processor and memory, enabling faster and more efficient data transfers,
especially when working with high-performance devices like GPUs or
custom accelerators.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hypervisors&lt;/strong&gt; Hypervisors are software, firmware, or
hardware components that enable virtualization by managing the creation
and execution of virtual machines. Type 1 (bare-metal) run directly on
the hardware, providing high performance and better isolation for VMs.
Examples include VMware ESXi, Microsoft Hyper-V, and Xen. Type 2
(hosted) run on top of a host operating system, with the hypervisor
providing virtualized hardware resources to guest operating systems.
Examples include VMware Workstation and Oracle VirtualBox.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SR-IOV (Single Root I/O Virtualization)&lt;/strong&gt; allows a
single physical network interface card (NIC) or other I/O devices to
appear as multiple separate virtual devices to virtual machines. It
improves performance by allowing VMs to access I/O resources directly
without the need for extensive virtualization overhead. SR-IOV enables
better scalability and efficiency by allowing multiple VMs to share a
single physical device while maintaining near-native performance. In
SR-IOV-enabled systems, the hypervisor configures the physical device
(e.g., NIC, GPU) to expose virtual interfaces, which are then directly
assigned to virtual machines.&lt;/p&gt;</content><category term="Neuron"></category><category term="ml-code"></category><category term="llm"></category></entry></feed>