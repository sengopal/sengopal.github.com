<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Senthilkumar Gopal - Large Language Models</title><link href="https://sengopal.github.io/" rel="alternate"></link><link href="https://sengopal.github.io/feeds/large-language-models.atom.xml" rel="self"></link><id>https://sengopal.github.io/</id><updated>2024-05-18T00:00:00-07:00</updated><subtitle>Musings of a machine learning researcher, engineer and leader</subtitle><entry><title>LLM Inference Systems</title><link href="https://sengopal.github.io/posts/llm-inference-systems.html" rel="alternate"></link><published>2024-05-18T00:00:00-07:00</published><updated>2024-05-18T00:00:00-07:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.github.io,2024-05-18:/posts/llm-inference-systems.html</id><summary type="html">A quick clarification between the terms - Triton, TensorRT, and TensorRT-LLM</summary><content type="html">&lt;p&gt;In my exploration of various inference systems, there is a general
confusion about Triton, TensorRT, and TensorRT-LLM. These are all
related to optimizing and deploying machine learning models, but with
different purposes and use cases, specifically what inputs and outputs
that they support.&lt;/p&gt;
&lt;h3 id="triton"&gt;Triton&lt;/h3&gt;
&lt;p&gt;Triton is an open-source inference &lt;strong&gt;server&lt;/strong&gt; (NVIDIA)
designed to optimize and deploy machine learning models for inference
workloads and supports multiple deep learning frameworks, including
TensorFlow, PyTorch, and TensorRT. Triton supportsfeatures like
batching, concurrent model execution, and dynamic batching to improve
performance and efficiency.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inputs&lt;/strong&gt; Triton can accept inputs in various formats,
such as images, text, or numerical data, depending on the machine
learning model being deployed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outputs&lt;/strong&gt; The outputs depend on the specific machine
learning model and its task, such as classification results, object
detection bounding boxes, or text generation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="tensorrt"&gt;TensorRT&lt;/h3&gt;
&lt;p&gt;TensorRT is a high-performance deep learning inference optimizer and
runtime &lt;strong&gt;library&lt;/strong&gt; developed by NVIDIA designed to
optimize and accelerate the inference performance of deep learning
models on NVIDIA GPUs. TensorRT can optimize models from various deep
learning frameworks, including TensorFlow, PyTorch, and ONNX.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inputs&lt;/strong&gt; TensorRT accepts deep learning models in
various formats, such as TensorFlow frozen graphs, PyTorch traced
models, or ONNX models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outputs&lt;/strong&gt; TensorRT optimizes the models for faster
inference on NVIDIA GPUs, but it does not change the modelâ€™s output
format. The outputs remain the same as the original model, such as
classification probabilities, object detection bounding boxes, or
segmentation masks.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="tensorrt-llm"&gt;TensorRT-LLM&lt;/h3&gt;
&lt;p&gt;TensorRT-LLM is a branch of TensorRT specifically designed for
optimizing and deploying large language models (LLMs) on NVIDIA GPUs. It
provides optimizations and techniques tailored for the efficient
inference of LLMs, which can be computationally expensive due to their
large size and complex architectures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Inputs&lt;/strong&gt; TensorRT-LLM accepts pre-trained LLM models
in various formats, such as TensorFlow, PyTorch, or ONNX.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outputs&lt;/strong&gt; The outputs of TensorRT-LLM remain the same
as the original LLM model, which typically involves generating text
based on the input prompt or context.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TL;DR - Triton is an inference server that can deploy and optimize
models from different frameworks, including TensorRT-optimized models.
TensorRT is a library for optimizing and accelerating deep learning
models for inference on NVIDIA GPUs. TensorRT-LLM is a specialized
branch of TensorRT focused on optimizing and deploying large language
models efficiently on NVIDIA GPUs.&lt;/p&gt;</content><category term="Large Language Models"></category><category term="ml-code"></category><category term="llm"></category></entry></feed>