<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Senthilkumar Gopal - ML Systems</title><link href="https://sengopal.me/" rel="alternate"/><link href="https://sengopal.me/feeds/ml-systems.atom.xml" rel="self"/><id>https://sengopal.me/</id><updated>2024-09-02T00:00:00-07:00</updated><subtitle>Musings of a machine learning researcher, engineer and leader</subtitle><entry><title>Slurm Cluster usage tips for quick debug and testing</title><link href="https://sengopal.me/posts/slurm-cluster-usage-tips-for-quick-debug-and-testing" rel="alternate"/><published>2024-09-02T00:00:00-07:00</published><updated>2024-09-02T00:00:00-07:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2024-09-02:/posts/slurm-cluster-usage-tips-for-quick-debug-and-testing</id><summary type="html">This post covers practical Slurm commands and best practices to help you reserve, access, and work cleanly on nodes.</summary><content type="html">&lt;p&gt;The following is a cheatsheet for Slurm commands used for running
tests, debugging, and reserving compute nodes.&lt;/p&gt;
&lt;p&gt;To start working with an idle node, you can allocate one exclusively
using &lt;code&gt;salloc --exclusive -N 1&lt;/code&gt;. This command requests a full
node for your use, allowing direct work without interference. Once your
allocation is granted, you can see which node you received by checking
the job ID in &lt;code&gt;squeue&lt;/code&gt;. Instead of SSHing manually into the
node, it is cleaner and simpler to use
&lt;code&gt;srun -N 1 --pty --exclusive bash&lt;/code&gt; to start an interactive
bash session directly on the node. This avoids common SSH permission
errors.&lt;/p&gt;
&lt;p&gt;If you want to verify which nodes are idle before requesting one, you
can run &lt;code&gt;sinfo -N -r -l&lt;/code&gt;. This will list nodes and their
states, helping you identify available resources. After allocation, if
you need specific information about your node, you can retrieve detailed
status using &lt;code&gt;scontrol show nodes&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;There may be cases where you need a specific node due to setup
constraints or to implement a particular network topology. In that case,
instead of asking for a number of nodes, you can use
&lt;code&gt;--nodelist&lt;/code&gt; or &lt;code&gt;-w&lt;/code&gt; followed by the desired node
name when calling &lt;code&gt;salloc&lt;/code&gt; or &lt;code&gt;sbatch&lt;/code&gt;. Keep in
mind that if a node is already exclusively allocated, your request will
hang until the node becomes free.&lt;/p&gt;
&lt;p&gt;Sometimes, you may want to occupy a node for an extended period to
perform manual debugging. A simple method is to submit a job that
“sleeps” for a long time using
&lt;code&gt;sbatch --nodes=1 --wrap "sleep 36000"&lt;/code&gt;. This holds the node
for about ten hours, giving you ample time to test and debug.&lt;/p&gt;
&lt;h3 id="caution-with-salloc"&gt;Caution with &lt;code&gt;salloc&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;While using &lt;code&gt;salloc&lt;/code&gt; for direct work is convenient, it is
generally better to run structured jobs with &lt;code&gt;sbatch&lt;/code&gt;
whenever possible. Scheduled jobs are managed more fairly and usually
have better priority than interactive ones. If you install custom
dependencies or system-level libraries during your session, make sure
they are isolated and do not impact other users. Using shared locations
like &lt;code&gt;/shared_inference&lt;/code&gt; can make it easier to move across
nodes without repeated installations.&lt;/p&gt;
&lt;p&gt;If you have an active reservation on the cluster, you can submit jobs
against it with
&lt;code&gt;sbatch --reservation=dedicated_two your_script.sh&lt;/code&gt;. You can
check available reservations with &lt;code&gt;scontrol show res&lt;/code&gt; and
adjust your job submissions accordingly.&lt;/p&gt;
&lt;p&gt;Finally, if your workflow involves setting up environments or
installing packages before running experiments, one efficient approach
is to create a batch script that performs all setup steps and then
blocks by running &lt;code&gt;/bin/bash&lt;/code&gt;. This allows you to SSH or
&lt;code&gt;srun&lt;/code&gt; into the node and continue your work interactively
without leaving stray processes.&lt;/p&gt;</content><category term="ML Systems"/><category term="ml-code"/><category term="llm"/><category term="ml-acceleration"/><category term="slurm"/><category term="hpc-concepts"/></entry><entry><title>EFA and OpenFabrics</title><link href="https://sengopal.me/posts/efa-and-openfabrics" rel="alternate"/><published>2023-12-29T00:00:00-08:00</published><updated>2023-12-29T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-29:/posts/efa-and-openfabrics</id><summary type="html">This post works through a high level overview of OpenMPI and EFA</summary><content type="html">&lt;p&gt;This post attempts to clarify the use of EFA and OpenMPI in
multi-node inference, focusing on how components like the Matching
Transport Layer (MTL), libfabric, and the OFI framework enable
efficient, low-latency communication. Installing EFA for a node &lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html"&gt;Ref&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="terms-involved"&gt;Terms Involved&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Matching Transport Layer (MTL)&lt;/strong&gt; The Matching
Transport Layer (MTL) is a component used in the Open MPI implementation
when utilizing libfabric for managing two-sided tagged messages. MTL is
responsible for matching message tags and ensuring that messages are
delivered to the correct destination. This layer is designed to work
closely with the underlying network fabric, such as EFA, to provide
efficient and reliable message passing between nodes in a
high-performance computing (HPC) environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;EFA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;EFA integrates with the libfabric API, which is part of the
OpenFabrics Interfaces (OFI) framework. This integration allows EFA to
bypass the operating system kernel, reducing overhead and enabling
low-latency, high-throughput communication directly with the network
interface hardware. This is critical for scaling HPC and machine
learning applications on AWS. By leveraging these components, AWS’s EFA
can provide enhanced performance for HPC and ML applications, enabling
efficient inter-node communication and supporting large-scale
computational tasks.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MPI (Message Passing Interface)&lt;/strong&gt; Communication
protocol for parallel programming in distributed computing environments,
particularly in HPC clusters. It provides a standardized way for
processes to communicate with each other across nodes in a cluster,
supporting point-to-point and collective communication.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;OFED (OpenFabrics Enterprise Distribution)&lt;/strong&gt; Set of
open-source software components that enable high-performance networking
on clusters, especially those using InfiniBand and other
high-performance fabrics. It provides the necessary drivers, libraries,
and tools to enable low-latency, high-bandwidth communication between
nodes in a cluster. OFED is commonly used in environments where RDMA
(Remote Direct Memory Access) and InfiniBand technologies are deployed,
facilitating direct memory access and efficient data transfer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LibFabric&lt;/strong&gt; Low-level communication library designed
to abstract hardware-specific communication protocols. It provides a
unified API for building high-performance network communication systems
and is often used for RDMA, shared memory, and other communication
technologies. Libfabric allows applications to use different network
fabrics (such as InfiniBand, iWARP, and RoCE) without being tightly
coupled to a particular hardware implementation, making it highly
flexible and adaptable for various cluster environments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RDMA (Remote Direct Memory Access)&lt;/strong&gt; Allows for
high-speed data transfer between nodes in a cluster without involving
the CPU, offering significant reductions in latency and CPU utilization.
By directly accessing the memory of a remote node, RDMA enables faster
data transfers than traditional networking methods, making it ideal for
applications that require large amounts of data to be exchanged between
nodes with minimal overhead. RDMA is supported by technologies like
InfiniBand and RoCE (RDMA over Converged Ethernet), and is critical in
HPC, machine learning, and cloud computing environments.&lt;/p&gt;
&lt;h3 id="openfabrics-interfaces-ofi"&gt;OpenFabrics Interfaces (OFI)&lt;/h3&gt;
&lt;p&gt;OpenFabrics Interfaces (OFI) is a framework designed to expose
communication services to middleware and applications, particularly in
high-performance computing (HPC) environments. Here are the key aspects
of OFI:&lt;/p&gt;
&lt;h3 id="purpose-and-design"&gt;Purpose and Design&lt;/h3&gt;
&lt;p&gt;OFI is specifically designed to meet the performance and scalability
requirements of HPC applications such as Message Passing Interface (MPI)
libraries, Symmetric Hierarchical Memory Access (SHMEM) libraries,
Partitioned Global Address Space (PGAS) programming models, Database
Management Systems (DBMS), and enterprise applications running in
tightly coupled network environments. Its design aligns fabric services
with application needs, providing a tight semantic fit between
applications and the underlying fabric hardware. This reduces software
overhead and improves efficiency when transmitting or receiving data
over a fabric.&lt;/p&gt;
&lt;h3 id="components"&gt;Components&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Libfabric&lt;/strong&gt;: The primary implementation of OFI is the
libfabric library, which defines and exports the user-space API of OFI.
Libfabric is designed to be independent of the underlying network
protocols and the specific implementation of networking devices, making
it versatile and widely applicable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Provider Libraries&lt;/strong&gt;: These libraries interface with
the hardware and provide the necessary services to the applications
through libfabric.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kernel Services and Daemons&lt;/strong&gt;: These components
support the user-space libraries and manage the communication between
the application and the hardware.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Applications&lt;/strong&gt;: These are used to validate and
benchmark the performance of the OFI framework.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="references"&gt;References&lt;/h3&gt;
&lt;ol type="1"&gt;
&lt;li&gt;&lt;a href="https://digitalcloud.training/aws-networking-eni-vs-efa-vs-ena/"&gt;Digital
Cloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html"&gt;AWS
Docs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="ML Systems"/><category term="ml-code"/><category term="llm"/><category term="AWS"/><category term="neuron"/></entry><entry><title>Aliasing on XLA</title><link href="https://sengopal.me/posts/aliasing-on-xla" rel="alternate"/><published>2023-12-20T00:00:00-08:00</published><updated>2023-12-20T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-20:/posts/aliasing-on-xla</id><summary type="html">This post explores the concept of aliasing in XLA, its significance, the mechanisms through which it is implemented, and future directions for extending aliasing optimizations.</summary><content type="html">&lt;p&gt;Modern machine learning models demand substantial memory, especially
during training and inference. To address this challenge, compilers like
XLA (Accelerated Linear Algebra) implement memory optimizations such as
aliasing. Aliasing allows different parts of a computation to share the
same underlying memory buffer, thereby reducing memory footprint and
improving execution performance.&lt;/p&gt;
&lt;h2 id="what-is-aliasing"&gt;What is Aliasing&lt;/h2&gt;
&lt;p&gt;Aliasing in XLA refers to mapping multiple logical buffers onto the
same physical memory region. In practice, this means that input and
output tensors in a computation can share the same memory if certain
conditions are met. By setting up such controlled aliases, XLA can avoid
allocating separate memory for intermediate results, resulting in
reduced memory usage and faster data access.&lt;/p&gt;
&lt;p&gt;However, establishing aliases must be managed carefully to maintain
computational correctness. If two aliases modify the same memory
simultaneously without proper sequencing, it can lead to corrupted
results. Consequently, XLA enforces strict constraints based on shape
compatibility, computation dependencies, and memory layouts, as outlined
in the design of the XLA compiler&lt;a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="how-xla-supports-aliasing"&gt;How XLA Supports Aliasing&lt;/h2&gt;
&lt;p&gt;XLA supports aliasing primarily through the
&lt;code&gt;OptimizeInputOutputBufferAlias&lt;/code&gt; class. This optimization
pass attempts to reuse input buffers for output buffers wherever
possible while respecting correctness guarantees. The following
annotated code illustrates the core flow:&lt;/p&gt;
&lt;p&gt;_Ref: &lt;a href="https://github.com/openxla/xla/blob/main/xla/hlo/transforms/simplifiers/optimize_input_output_buffer_alias.cc"&gt;optimize_input_output_buffer_alias.cc&lt;/a&gt;
with annotations&lt;/p&gt;
&lt;div class="sourceCode" id="cb1"&gt;&lt;pre class="sourceCode cpp"&gt;&lt;code class="sourceCode cpp"&gt;&lt;span id="cb1-1"&gt;&lt;a aria-hidden="true" href="#cb1-1" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="kw"&gt;namespace&lt;/span&gt; xla &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-2"&gt;&lt;a aria-hidden="true" href="#cb1-2" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-3"&gt;&lt;a aria-hidden="true" href="#cb1-3" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;// Attempts to establish aliases between input and output buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-4"&gt;&lt;a aria-hidden="true" href="#cb1-4" tabindex="-1"&gt;&lt;/a&gt;absl&lt;span class="op"&gt;::&lt;/span&gt;StatusOr&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="dt"&gt;bool&lt;/span&gt;&lt;span class="op"&gt;&amp;gt;&lt;/span&gt; OptimizeInputOutputBufferAlias&lt;span class="op"&gt;::&lt;/span&gt;Build&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-5"&gt;&lt;a aria-hidden="true" href="#cb1-5" tabindex="-1"&gt;&lt;/a&gt;    absl&lt;span class="op"&gt;::&lt;/span&gt;Span&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;gt;&lt;/span&gt; input_shapes&lt;span class="op"&gt;,&lt;/span&gt; &lt;/span&gt;
&lt;span id="cb1-6"&gt;&lt;a aria-hidden="true" href="#cb1-6" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; output_shape&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-7"&gt;&lt;a aria-hidden="true" href="#cb1-7" tabindex="-1"&gt;&lt;/a&gt;    HloInputOutputAliasConfig&lt;span class="op"&gt;*&lt;/span&gt; alias_config&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-8"&gt;&lt;a aria-hidden="true" href="#cb1-8" tabindex="-1"&gt;&lt;/a&gt;    HloBufferDonorConfig&lt;span class="op"&gt;*&lt;/span&gt; buffer_donor_config&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-9"&gt;&lt;a aria-hidden="true" href="#cb1-9" tabindex="-1"&gt;&lt;/a&gt;  &lt;/span&gt;
&lt;span id="cb1-10"&gt;&lt;a aria-hidden="true" href="#cb1-10" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="dt"&gt;bool&lt;/span&gt; changed &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="kw"&gt;false&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-11"&gt;&lt;a aria-hidden="true" href="#cb1-11" tabindex="-1"&gt;&lt;/a&gt;  &lt;/span&gt;
&lt;span id="cb1-12"&gt;&lt;a aria-hidden="true" href="#cb1-12" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Disable aliasing if the output has a dynamic shape.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-13"&gt;&lt;a aria-hidden="true" href="#cb1-13" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;output_shape&lt;span class="op"&gt;.&lt;/span&gt;is_dynamic&lt;span class="op"&gt;())&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-14"&gt;&lt;a aria-hidden="true" href="#cb1-14" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;return&lt;/span&gt; &lt;span class="kw"&gt;false&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-15"&gt;&lt;a aria-hidden="true" href="#cb1-15" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-16"&gt;&lt;a aria-hidden="true" href="#cb1-16" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-17"&gt;&lt;a aria-hidden="true" href="#cb1-17" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Structures to collect potential donor input buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-18"&gt;&lt;a aria-hidden="true" href="#cb1-18" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="kw"&gt;struct&lt;/span&gt; DonorEntry &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-19"&gt;&lt;a aria-hidden="true" href="#cb1-19" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; param_number&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-20"&gt;&lt;a aria-hidden="true" href="#cb1-20" tabindex="-1"&gt;&lt;/a&gt;    ShapeIndex index&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-21"&gt;&lt;a aria-hidden="true" href="#cb1-21" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; shape_size&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-22"&gt;&lt;a aria-hidden="true" href="#cb1-22" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;};&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-23"&gt;&lt;a aria-hidden="true" href="#cb1-23" tabindex="-1"&gt;&lt;/a&gt;  absl&lt;span class="op"&gt;::&lt;/span&gt;flat_hash_map&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="dt"&gt;int64_t&lt;/span&gt;&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="bu"&gt;std::&lt;/span&gt;vector&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;DonorEntry&lt;span class="op"&gt;&amp;gt;&amp;gt;&lt;/span&gt; donors&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-24"&gt;&lt;a aria-hidden="true" href="#cb1-24" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-25"&gt;&lt;a aria-hidden="true" href="#cb1-25" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Populate donors: traverse each input's subshapes and collect eligible buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-26"&gt;&lt;a aria-hidden="true" href="#cb1-26" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;for&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="dt"&gt;int64_t&lt;/span&gt; param_number &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt; param_number &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; input_shapes&lt;span class="op"&gt;.&lt;/span&gt;size&lt;span class="op"&gt;();&lt;/span&gt; &lt;span class="op"&gt;++&lt;/span&gt;param_number&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-27"&gt;&lt;a aria-hidden="true" href="#cb1-27" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; input_shape &lt;span class="op"&gt;=&lt;/span&gt; input_shapes&lt;span class="op"&gt;[&lt;/span&gt;param_number&lt;span class="op"&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-28"&gt;&lt;a aria-hidden="true" href="#cb1-28" tabindex="-1"&gt;&lt;/a&gt;    TF_RET_CHECK&lt;span class="op"&gt;(&lt;/span&gt;LayoutUtil&lt;span class="op"&gt;::&lt;/span&gt;HasLayout&lt;span class="op"&gt;(&lt;/span&gt;input_shape&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-29"&gt;&lt;a aria-hidden="true" href="#cb1-29" tabindex="-1"&gt;&lt;/a&gt;    ShapeUtil&lt;span class="op"&gt;::&lt;/span&gt;ForEachSubshape&lt;span class="op"&gt;(&lt;/span&gt;input_shape&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="op"&gt;[&amp;amp;](&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; subshape&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="at"&gt;const&lt;/span&gt; ShapeIndex&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; index&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-30"&gt;&lt;a aria-hidden="true" href="#cb1-30" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(!&lt;/span&gt;LayoutUtil&lt;span class="op"&gt;::&lt;/span&gt;IsDenseArray&lt;span class="op"&gt;(&lt;/span&gt;subshape&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;||&lt;/span&gt; subshape&lt;span class="op"&gt;.&lt;/span&gt;is_dynamic&lt;span class="op"&gt;())&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-31"&gt;&lt;a aria-hidden="true" href="#cb1-31" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-32"&gt;&lt;a aria-hidden="true" href="#cb1-32" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-33"&gt;&lt;a aria-hidden="true" href="#cb1-33" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;alias_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;ParameterHasAlias&lt;span class="op"&gt;(&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; index&lt;span class="op"&gt;))&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-34"&gt;&lt;a aria-hidden="true" href="#cb1-34" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-35"&gt;&lt;a aria-hidden="true" href="#cb1-35" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-36"&gt;&lt;a aria-hidden="true" href="#cb1-36" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="va"&gt;registered_buffer_donor_only_&lt;/span&gt; &lt;span class="op"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-37"&gt;&lt;a aria-hidden="true" href="#cb1-37" tabindex="-1"&gt;&lt;/a&gt;          &lt;span class="op"&gt;!&lt;/span&gt;buffer_donor_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;ParameterIsBufferDonor&lt;span class="op"&gt;(&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; index&lt;span class="op"&gt;))&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-38"&gt;&lt;a aria-hidden="true" href="#cb1-38" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-39"&gt;&lt;a aria-hidden="true" href="#cb1-39" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-40"&gt;&lt;a aria-hidden="true" href="#cb1-40" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="dt"&gt;int64_t&lt;/span&gt; memory_space &lt;span class="op"&gt;=&lt;/span&gt; subshape&lt;span class="op"&gt;.&lt;/span&gt;layout&lt;span class="op"&gt;().&lt;/span&gt;memory_space&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-41"&gt;&lt;a aria-hidden="true" href="#cb1-41" tabindex="-1"&gt;&lt;/a&gt;      donors&lt;span class="op"&gt;[&lt;/span&gt;memory_space&lt;span class="op"&gt;].&lt;/span&gt;emplace_back&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-42"&gt;&lt;a aria-hidden="true" href="#cb1-42" tabindex="-1"&gt;&lt;/a&gt;          DonorEntry&lt;span class="op"&gt;{&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; index&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="va"&gt;shape_size_fn_&lt;/span&gt;&lt;span class="op"&gt;(&lt;/span&gt;subshape&lt;span class="op"&gt;)});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-43"&gt;&lt;a aria-hidden="true" href="#cb1-43" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-44"&gt;&lt;a aria-hidden="true" href="#cb1-44" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-45"&gt;&lt;a aria-hidden="true" href="#cb1-45" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-46"&gt;&lt;a aria-hidden="true" href="#cb1-46" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Structures to collect potential donee output buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-47"&gt;&lt;a aria-hidden="true" href="#cb1-47" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="kw"&gt;struct&lt;/span&gt; DoneeEntry &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-48"&gt;&lt;a aria-hidden="true" href="#cb1-48" tabindex="-1"&gt;&lt;/a&gt;    ShapeIndex index&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-49"&gt;&lt;a aria-hidden="true" href="#cb1-49" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; shape_size&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-50"&gt;&lt;a aria-hidden="true" href="#cb1-50" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;};&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-51"&gt;&lt;a aria-hidden="true" href="#cb1-51" tabindex="-1"&gt;&lt;/a&gt;  absl&lt;span class="op"&gt;::&lt;/span&gt;flat_hash_map&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="dt"&gt;int64_t&lt;/span&gt;&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="bu"&gt;std::&lt;/span&gt;vector&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;DoneeEntry&lt;span class="op"&gt;&amp;gt;&amp;gt;&lt;/span&gt; donees&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-52"&gt;&lt;a aria-hidden="true" href="#cb1-52" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-53"&gt;&lt;a aria-hidden="true" href="#cb1-53" tabindex="-1"&gt;&lt;/a&gt;  TF_RET_CHECK&lt;span class="op"&gt;(&lt;/span&gt;LayoutUtil&lt;span class="op"&gt;::&lt;/span&gt;HasLayout&lt;span class="op"&gt;(&lt;/span&gt;output_shape&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-54"&gt;&lt;a aria-hidden="true" href="#cb1-54" tabindex="-1"&gt;&lt;/a&gt;  ShapeUtil&lt;span class="op"&gt;::&lt;/span&gt;ForEachSubshape&lt;span class="op"&gt;(&lt;/span&gt;output_shape&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="op"&gt;[&amp;amp;](&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; subshape&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="at"&gt;const&lt;/span&gt; ShapeIndex&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; index&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-55"&gt;&lt;a aria-hidden="true" href="#cb1-55" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(!&lt;/span&gt;LayoutUtil&lt;span class="op"&gt;::&lt;/span&gt;IsDenseArray&lt;span class="op"&gt;(&lt;/span&gt;subshape&lt;span class="op"&gt;))&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-56"&gt;&lt;a aria-hidden="true" href="#cb1-56" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-57"&gt;&lt;a aria-hidden="true" href="#cb1-57" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-58"&gt;&lt;a aria-hidden="true" href="#cb1-58" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;alias_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;OutputHasAlias&lt;span class="op"&gt;(&lt;/span&gt;index&lt;span class="op"&gt;))&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-59"&gt;&lt;a aria-hidden="true" href="#cb1-59" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;return&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-60"&gt;&lt;a aria-hidden="true" href="#cb1-60" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-61"&gt;&lt;a aria-hidden="true" href="#cb1-61" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; memory_space &lt;span class="op"&gt;=&lt;/span&gt; subshape&lt;span class="op"&gt;.&lt;/span&gt;layout&lt;span class="op"&gt;().&lt;/span&gt;memory_space&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-62"&gt;&lt;a aria-hidden="true" href="#cb1-62" tabindex="-1"&gt;&lt;/a&gt;    donees&lt;span class="op"&gt;[&lt;/span&gt;memory_space&lt;span class="op"&gt;].&lt;/span&gt;emplace_back&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-63"&gt;&lt;a aria-hidden="true" href="#cb1-63" tabindex="-1"&gt;&lt;/a&gt;        DoneeEntry&lt;span class="op"&gt;{&lt;/span&gt;index&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="va"&gt;shape_size_fn_&lt;/span&gt;&lt;span class="op"&gt;(&lt;/span&gt;subshape&lt;span class="op"&gt;)});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-64"&gt;&lt;a aria-hidden="true" href="#cb1-64" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-65"&gt;&lt;a aria-hidden="true" href="#cb1-65" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-66"&gt;&lt;a aria-hidden="true" href="#cb1-66" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// For each memory space, match donors and donees based on shape size.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-67"&gt;&lt;a aria-hidden="true" href="#cb1-67" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;for&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; &lt;span class="op"&gt;[&lt;/span&gt;memory_space&lt;span class="op"&gt;,&lt;/span&gt; donor_vector&lt;span class="op"&gt;]&lt;/span&gt; &lt;span class="op"&gt;:&lt;/span&gt; donors&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-68"&gt;&lt;a aria-hidden="true" href="#cb1-68" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="kw"&gt;auto&lt;/span&gt; donee_it &lt;span class="op"&gt;=&lt;/span&gt; donees&lt;span class="op"&gt;.&lt;/span&gt;find&lt;span class="op"&gt;(&lt;/span&gt;memory_space&lt;span class="op"&gt;);&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-69"&gt;&lt;a aria-hidden="true" href="#cb1-69" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;donee_it &lt;span class="op"&gt;==&lt;/span&gt; donees&lt;span class="op"&gt;.&lt;/span&gt;end&lt;span class="op"&gt;())&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-70"&gt;&lt;a aria-hidden="true" href="#cb1-70" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;continue&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-71"&gt;&lt;a aria-hidden="true" href="#cb1-71" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-72"&gt;&lt;a aria-hidden="true" href="#cb1-72" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; donee_vector &lt;span class="op"&gt;=&lt;/span&gt; donee_it&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;second&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-73"&gt;&lt;a aria-hidden="true" href="#cb1-73" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-74"&gt;&lt;a aria-hidden="true" href="#cb1-74" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;// Sort both donors and donees by decreasing size to maximize reuse of large buffers.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-75"&gt;&lt;a aria-hidden="true" href="#cb1-75" tabindex="-1"&gt;&lt;/a&gt;    absl&lt;span class="op"&gt;::&lt;/span&gt;c_stable_sort&lt;span class="op"&gt;(&lt;/span&gt;donor_vector&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-76"&gt;&lt;a aria-hidden="true" href="#cb1-76" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="op"&gt;[](&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; DonorEntry&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; a&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="at"&gt;const&lt;/span&gt; DonorEntry&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; b&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt; &lt;span class="cf"&gt;return&lt;/span&gt; a&lt;span class="op"&gt;.&lt;/span&gt;shape_size &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; b&lt;span class="op"&gt;.&lt;/span&gt;shape_size&lt;span class="op"&gt;;&lt;/span&gt; &lt;span class="op"&gt;});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-77"&gt;&lt;a aria-hidden="true" href="#cb1-77" tabindex="-1"&gt;&lt;/a&gt;    absl&lt;span class="op"&gt;::&lt;/span&gt;c_stable_sort&lt;span class="op"&gt;(&lt;/span&gt;donee_vector&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-78"&gt;&lt;a aria-hidden="true" href="#cb1-78" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="op"&gt;[](&lt;/span&gt;&lt;span class="at"&gt;const&lt;/span&gt; DoneeEntry&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; a&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="at"&gt;const&lt;/span&gt; DoneeEntry&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; b&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt; &lt;span class="cf"&gt;return&lt;/span&gt; a&lt;span class="op"&gt;.&lt;/span&gt;shape_size &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; b&lt;span class="op"&gt;.&lt;/span&gt;shape_size&lt;span class="op"&gt;;&lt;/span&gt; &lt;span class="op"&gt;});&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-79"&gt;&lt;a aria-hidden="true" href="#cb1-79" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-80"&gt;&lt;a aria-hidden="true" href="#cb1-80" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="co"&gt;// Two-pointer matching: match donor and donee of the same size.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-81"&gt;&lt;a aria-hidden="true" href="#cb1-81" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; donor_vector_index &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-82"&gt;&lt;a aria-hidden="true" href="#cb1-82" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="dt"&gt;int64_t&lt;/span&gt; donee_vector_index &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-83"&gt;&lt;a aria-hidden="true" href="#cb1-83" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="cf"&gt;while&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;donor_vector_index &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; donor_vector&lt;span class="op"&gt;.&lt;/span&gt;size&lt;span class="op"&gt;()&lt;/span&gt; &lt;span class="op"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-84"&gt;&lt;a aria-hidden="true" href="#cb1-84" tabindex="-1"&gt;&lt;/a&gt;           donee_vector_index &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; donee_vector&lt;span class="op"&gt;.&lt;/span&gt;size&lt;span class="op"&gt;())&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-85"&gt;&lt;a aria-hidden="true" href="#cb1-85" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="at"&gt;const&lt;/span&gt; &lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; donor &lt;span class="op"&gt;=&lt;/span&gt; donor_vector&lt;span class="op"&gt;[&lt;/span&gt;donor_vector_index&lt;span class="op"&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-86"&gt;&lt;a aria-hidden="true" href="#cb1-86" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="at"&gt;const&lt;/span&gt; &lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; donee &lt;span class="op"&gt;=&lt;/span&gt; donee_vector&lt;span class="op"&gt;[&lt;/span&gt;donee_vector_index&lt;span class="op"&gt;];&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-87"&gt;&lt;a aria-hidden="true" href="#cb1-87" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;donor&lt;span class="op"&gt;.&lt;/span&gt;shape_size &lt;span class="op"&gt;&amp;gt;&lt;/span&gt; donee&lt;span class="op"&gt;.&lt;/span&gt;shape_size&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-88"&gt;&lt;a aria-hidden="true" href="#cb1-88" tabindex="-1"&gt;&lt;/a&gt;        donor_vector_index&lt;span class="op"&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-89"&gt;&lt;a aria-hidden="true" href="#cb1-89" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt; &lt;span class="cf"&gt;else&lt;/span&gt; &lt;span class="cf"&gt;if&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;donor&lt;span class="op"&gt;.&lt;/span&gt;shape_size &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; donee&lt;span class="op"&gt;.&lt;/span&gt;shape_size&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-90"&gt;&lt;a aria-hidden="true" href="#cb1-90" tabindex="-1"&gt;&lt;/a&gt;        donee_vector_index&lt;span class="op"&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-91"&gt;&lt;a aria-hidden="true" href="#cb1-91" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt; &lt;span class="cf"&gt;else&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-92"&gt;&lt;a aria-hidden="true" href="#cb1-92" tabindex="-1"&gt;&lt;/a&gt;        &lt;span class="co"&gt;// Match found: establish alias and remove donor from pool.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-93"&gt;&lt;a aria-hidden="true" href="#cb1-93" tabindex="-1"&gt;&lt;/a&gt;        TF_RETURN_IF_ERROR&lt;span class="op"&gt;(&lt;/span&gt;alias_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;SetUpAlias&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-94"&gt;&lt;a aria-hidden="true" href="#cb1-94" tabindex="-1"&gt;&lt;/a&gt;            donee&lt;span class="op"&gt;.&lt;/span&gt;index&lt;span class="op"&gt;,&lt;/span&gt; donor&lt;span class="op"&gt;.&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; donor&lt;span class="op"&gt;.&lt;/span&gt;index&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-95"&gt;&lt;a aria-hidden="true" href="#cb1-95" tabindex="-1"&gt;&lt;/a&gt;        TF_RETURN_IF_ERROR&lt;span class="op"&gt;(&lt;/span&gt;buffer_donor_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;RemoveBufferDonor&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-96"&gt;&lt;a aria-hidden="true" href="#cb1-96" tabindex="-1"&gt;&lt;/a&gt;            donor&lt;span class="op"&gt;.&lt;/span&gt;param_number&lt;span class="op"&gt;,&lt;/span&gt; donor&lt;span class="op"&gt;.&lt;/span&gt;index&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-97"&gt;&lt;a aria-hidden="true" href="#cb1-97" tabindex="-1"&gt;&lt;/a&gt;        donor_vector_index&lt;span class="op"&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-98"&gt;&lt;a aria-hidden="true" href="#cb1-98" tabindex="-1"&gt;&lt;/a&gt;        donee_vector_index&lt;span class="op"&gt;++;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-99"&gt;&lt;a aria-hidden="true" href="#cb1-99" tabindex="-1"&gt;&lt;/a&gt;        changed &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="kw"&gt;true&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-100"&gt;&lt;a aria-hidden="true" href="#cb1-100" tabindex="-1"&gt;&lt;/a&gt;      &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-101"&gt;&lt;a aria-hidden="true" href="#cb1-101" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-102"&gt;&lt;a aria-hidden="true" href="#cb1-102" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-103"&gt;&lt;a aria-hidden="true" href="#cb1-103" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-104"&gt;&lt;a aria-hidden="true" href="#cb1-104" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;return&lt;/span&gt; changed&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-105"&gt;&lt;a aria-hidden="true" href="#cb1-105" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-106"&gt;&lt;a aria-hidden="true" href="#cb1-106" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-107"&gt;&lt;a aria-hidden="true" href="#cb1-107" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="co"&gt;// Entry point for running the alias optimization on an HLO module.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-108"&gt;&lt;a aria-hidden="true" href="#cb1-108" tabindex="-1"&gt;&lt;/a&gt;absl&lt;span class="op"&gt;::&lt;/span&gt;StatusOr&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;&lt;span class="dt"&gt;bool&lt;/span&gt;&lt;span class="op"&gt;&amp;gt;&lt;/span&gt; OptimizeInputOutputBufferAlias&lt;span class="op"&gt;::&lt;/span&gt;Run&lt;span class="op"&gt;(&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-109"&gt;&lt;a aria-hidden="true" href="#cb1-109" tabindex="-1"&gt;&lt;/a&gt;    HloModule&lt;span class="op"&gt;*&lt;/span&gt; &lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-110"&gt;&lt;a aria-hidden="true" href="#cb1-110" tabindex="-1"&gt;&lt;/a&gt;    &lt;span class="at"&gt;const&lt;/span&gt; absl&lt;span class="op"&gt;::&lt;/span&gt;flat_hash_set&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;absl&lt;span class="op"&gt;::&lt;/span&gt;string_view&lt;span class="op"&gt;&amp;gt;&amp;amp;&lt;/span&gt; execution_threads&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-111"&gt;&lt;a aria-hidden="true" href="#cb1-111" tabindex="-1"&gt;&lt;/a&gt;  &lt;/span&gt;
&lt;span id="cb1-112"&gt;&lt;a aria-hidden="true" href="#cb1-112" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Extract input and output shapes from the module entry computation.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-113"&gt;&lt;a aria-hidden="true" href="#cb1-113" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="at"&gt;const&lt;/span&gt; &lt;span class="kw"&gt;auto&lt;/span&gt;&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; entry_computation_layout &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;entry_computation_layout&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-114"&gt;&lt;a aria-hidden="true" href="#cb1-114" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="bu"&gt;std::&lt;/span&gt;vector&lt;span class="op"&gt;&amp;lt;&lt;/span&gt;Shape&lt;span class="op"&gt;&amp;gt;&lt;/span&gt; input_shapes&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-115"&gt;&lt;a aria-hidden="true" href="#cb1-115" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;for&lt;/span&gt; &lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="dt"&gt;int64_t&lt;/span&gt; i &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="dv"&gt;0&lt;/span&gt;&lt;span class="op"&gt;;&lt;/span&gt; i &lt;span class="op"&gt;&amp;lt;&lt;/span&gt; &lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;entry_computation&lt;span class="op"&gt;()-&amp;gt;&lt;/span&gt;num_parameters&lt;span class="op"&gt;();&lt;/span&gt; &lt;span class="op"&gt;++&lt;/span&gt;i&lt;span class="op"&gt;)&lt;/span&gt; &lt;span class="op"&gt;{&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-116"&gt;&lt;a aria-hidden="true" href="#cb1-116" tabindex="-1"&gt;&lt;/a&gt;    input_shapes&lt;span class="op"&gt;.&lt;/span&gt;push_back&lt;span class="op"&gt;(&lt;/span&gt;entry_computation_layout&lt;span class="op"&gt;.&lt;/span&gt;parameter_shape&lt;span class="op"&gt;(&lt;/span&gt;i&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-117"&gt;&lt;a aria-hidden="true" href="#cb1-117" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-118"&gt;&lt;a aria-hidden="true" href="#cb1-118" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="at"&gt;const&lt;/span&gt; Shape&lt;span class="op"&gt;&amp;amp;&lt;/span&gt; output_shape &lt;span class="op"&gt;=&lt;/span&gt; entry_computation_layout&lt;span class="op"&gt;.&lt;/span&gt;result_shape&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-119"&gt;&lt;a aria-hidden="true" href="#cb1-119" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-120"&gt;&lt;a aria-hidden="true" href="#cb1-120" tabindex="-1"&gt;&lt;/a&gt;  HloInputOutputAliasConfig&lt;span class="op"&gt;*&lt;/span&gt; alias_config &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="op"&gt;&amp;amp;&lt;/span&gt;&lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;input_output_alias_config&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-121"&gt;&lt;a aria-hidden="true" href="#cb1-121" tabindex="-1"&gt;&lt;/a&gt;  HloBufferDonorConfig&lt;span class="op"&gt;*&lt;/span&gt; buffer_donor_config &lt;span class="op"&gt;=&lt;/span&gt; &lt;span class="op"&gt;&amp;amp;&lt;/span&gt;&lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;buffer_donor_config&lt;span class="op"&gt;();&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-122"&gt;&lt;a aria-hidden="true" href="#cb1-122" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-123"&gt;&lt;a aria-hidden="true" href="#cb1-123" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Attempt to build aliasing configuration.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-124"&gt;&lt;a aria-hidden="true" href="#cb1-124" tabindex="-1"&gt;&lt;/a&gt;  TF_ASSIGN_OR_RETURN&lt;span class="op"&gt;(&lt;/span&gt;&lt;span class="dt"&gt;bool&lt;/span&gt; changed&lt;span class="op"&gt;,&lt;/span&gt; Build&lt;span class="op"&gt;(&lt;/span&gt;input_shapes&lt;span class="op"&gt;,&lt;/span&gt; output_shape&lt;span class="op"&gt;,&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-125"&gt;&lt;a aria-hidden="true" href="#cb1-125" tabindex="-1"&gt;&lt;/a&gt;                                          alias_config&lt;span class="op"&gt;,&lt;/span&gt; buffer_donor_config&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-126"&gt;&lt;a aria-hidden="true" href="#cb1-126" tabindex="-1"&gt;&lt;/a&gt;  &lt;/span&gt;
&lt;span id="cb1-127"&gt;&lt;a aria-hidden="true" href="#cb1-127" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="co"&gt;// Verify that the resulting alias configuration is correct.&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-128"&gt;&lt;a aria-hidden="true" href="#cb1-128" tabindex="-1"&gt;&lt;/a&gt;  TF_RETURN_IF_ERROR&lt;span class="op"&gt;(&lt;/span&gt;alias_config&lt;span class="op"&gt;-&amp;gt;&lt;/span&gt;Verify&lt;span class="op"&gt;(*&lt;/span&gt;&lt;span class="kw"&gt;module&lt;/span&gt;&lt;span class="op"&gt;,&lt;/span&gt; &lt;span class="va"&gt;shape_size_fn_&lt;/span&gt;&lt;span class="op"&gt;));&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-129"&gt;&lt;a aria-hidden="true" href="#cb1-129" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-130"&gt;&lt;a aria-hidden="true" href="#cb1-130" tabindex="-1"&gt;&lt;/a&gt;  &lt;span class="cf"&gt;return&lt;/span&gt; changed&lt;span class="op"&gt;;&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-131"&gt;&lt;a aria-hidden="true" href="#cb1-131" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="op"&gt;}&lt;/span&gt;&lt;/span&gt;
&lt;span id="cb1-132"&gt;&lt;a aria-hidden="true" href="#cb1-132" tabindex="-1"&gt;&lt;/a&gt;&lt;/span&gt;
&lt;span id="cb1-133"&gt;&lt;a aria-hidden="true" href="#cb1-133" tabindex="-1"&gt;&lt;/a&gt;&lt;span class="op"&gt;}&lt;/span&gt; &lt;span class="co"&gt;// namespace xla&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This code performs input preparation, donor and donee collection,
size-based sorting, two-pointer matching of donors and donees, alias
setup, and verification of the final configuration.&lt;/p&gt;
&lt;h2 id="dynamic-shape-handling-on-aliasing"&gt;Dynamic Shape Handling on
Aliasing&lt;/h2&gt;
&lt;p&gt;Dynamic shapes introduce uncertainty in buffer sizes during runtime.
If an output tensor’s shape is dynamic, the memory required cannot be
reliably determined at compile time. As a result, XLA restricts aliasing
for dynamic shapes by disabling optimization whenever the output is
dynamic. Allowing aliasing for dynamic outputs could lead to incorrect
buffer allocations and data corruption if the output grows larger than
the input buffer. Verifying correctness would also require complex
runtime checks that can negate performance benefits. By disallowing
aliasing in such cases, XLA ensures a simple and predictable memory
model, though at the cost of missed optimization opportunities for
dynamic workloads.&lt;/p&gt;
&lt;p&gt;Recent research, such as work on dynamic bufferization in MLIR&lt;a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, proposes more sophisticated methods
to handle dynamic shapes while minimizing overhead.&lt;/p&gt;
&lt;h2 id="safe-aliasing-with-dynamic-shapes"&gt;Safe Aliasing with Dynamic
Shapes&lt;/h2&gt;
&lt;p&gt;One strategy to safely allow aliasing with dynamic shapes is to
introduce runtime shape guards. These guards would validate buffer
compatibility at execution time, only enabling aliasing if the actual
shapes are compatible. Another strategy involves over-allocating buffers
with conservative margins so that minor runtime variations can still be
safely accommodated. A third strategy is deferred memory binding, in
which buffer reuse decisions are delayed until runtime when precise
shape information is available. These approaches trade a small amount of
runtime complexity for potentially significant memory savings.&lt;/p&gt;
&lt;h2 id="aliasing-optimization-to-cross-computation-buffer-reuse"&gt;Aliasing
Optimization to Cross-Computation Buffer Reuse&lt;/h2&gt;
&lt;p&gt;To extend aliasing beyond a single computation, XLA would require
global buffer lifetime analysis across multiple HLO computations. By
analyzing liveness and memory usage across function calls and loops, it
would be possible to recycle buffers globally. Techniques like memory
pooling and cross-computation liveness tracking, adapted from
traditional compiler research &lt;a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;, could help enable
aliasing. Such global optimizations would offer further memory
reductions for large-scale training workloads.&lt;/p&gt;
&lt;h2 id="trade-offs-between-compile-time-and-runtime-aliasing"&gt;Trade-offs
Between Compile-Time and Runtime Aliasing&lt;/h2&gt;
&lt;p&gt;Compile-time aliasing offers a simpler and predictable execution
model with no runtime overhead, but it is inherently conservative. It
cannot exploit dynamic input properties or adapt to changing workloads.
Runtime aliasing decisions, in contrast, allow more aggressive
optimization by adapting to actual runtime shapes, but introduce
additional complexity and potential variability in performance. Hybrid
models that integrate compile-time planning with lightweight runtime
validation, such as those explored in MLIR &lt;a class="footnote-ref" href="#fn4" id="fnref4" role="doc-noteref"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;,
are promising future directions.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;section class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;“XLA: Optimizing Compiler for Machine Learning.” OpenXLA
Project, https://openxla.org/xla/tf2xla. Accessed 20 Dec. 2023.&lt;a class="footnote-back" href="#fnref1" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;Shape Inference - MLIR.
https://mlir.llvm.org/docs/ShapeInference/. Accessed 20 Dec. 2023.&lt;a class="footnote-back" href="#fnref2" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;Muchnick, Steven. Advanced compiler design
implementation. Morgan kaufmann, 1997.&lt;a class="footnote-back" href="#fnref3" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;Levental, Maksim. “Memory planning for deep neural
networks.” arXiv preprint arXiv:2203.00448 (2022).&lt;a class="footnote-back" href="#fnref4" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="ML Systems"/><category term="ml-code"/><category term="llm"/><category term="ml-acceleration"/><category term="hpc-concept"/></entry><entry><title>All Reduce Decomposition</title><link href="https://sengopal.me/posts/all-reduce-decomposition" rel="alternate"/><published>2023-12-20T00:00:00-08:00</published><updated>2023-12-20T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-20:/posts/all-reduce-decomposition</id><summary type="html">This post works through the metrics that are critical for ML inference</summary><content type="html">&lt;p&gt;As part of LLM serving, AllReduce is a common, but expensive
operation which also blocks compute utilization. A common improvement
would be to replace AllGather with a computationally equivalent
ReduceScatter + AllGather&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ReduceScatter: &lt;/strong&gt; In this step, each process performs
a reduction operation (e.g., sum, product, max, min) on its input data.
The resulting reduced values are then scattered across all processes,
such that &lt;strong&gt;each process receives a distinct portion of the
overall reduction result.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;AllGather:&lt;/strong&gt; After the ReduceScatter step, each
process holds a different portion of the overall reduction result. The
AllGather operation then &lt;strong&gt;collects all these partial results from
all processes&lt;/strong&gt; and distributes the complete result to every
process.&lt;/p&gt;
&lt;p&gt;By breaking down AllReduce into these two steps, it can potentially
improve performance and communication efficiency, particularly in
certain parallel computing architectures or communication patterns. Some
parallel computing libraries or frameworks may implement AllReduce as a
single operation or provide optimized implementations that combine the
two steps for better performance.&lt;/p&gt;
&lt;h2 id="diagram-explanation"&gt;Diagram Explanation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;     Initial State        Reduce-Scatter         AllGather          Final State
   ┌───┬───┬───┬───┐    ┌───┬───┬───┬───┐    ┌───┬───┬───┬───┐    ┌───┬───┬───┬───┐
D0 │ A │ B │ C │ D │ →  │ W │   │   │   │ →  │ W │ X │ Y │ Z │ →  │ W │ X │ Y │ Z │
   ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤
D1 │ E │ F │ G │ H │ →  │   │ X │   │   │ →  │ W │ X │ Y │ Z │ →  │ W │ X │ Y │ Z │
   ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤
D2 │ I │ J │ K │ L │ →  │   │   │ Y │   │ →  │ W │ X │ Y │ Z │ →  │ W │ X │ Y │ Z │
   ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤
D3 │ M │ N │ O │ P │ →  │   │   │   │ Z │ →  │ W │ X │ Y │ Z │ →  │ W │ X │ Y │ Z │
   └───┴───┴───┴───┘    └───┴───┴───┴───┘    └───┴───┴───┴───┘    └───┴───┴───┴───┘&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Explanation:&lt;/strong&gt;&lt;/p&gt;
&lt;ol type="1"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Initial State&lt;/strong&gt;: Each device (D0, D1, D2, D3) has
its own data (A-P).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reduce-Scatter&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each device performs a partial reduction on a specific portion of
the data.&lt;/li&gt;
&lt;li&gt;W = reduction of (A, E, I, M)&lt;/li&gt;
&lt;li&gt;X = reduction of (B, F, J, N)&lt;/li&gt;
&lt;li&gt;Y = reduction of (C, G, K, O)&lt;/li&gt;
&lt;li&gt;Z = reduction of (D, H, L, P)&lt;/li&gt;
&lt;li&gt;The results are scattered across devices.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;AllGather&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Each device gathers the partial results from all other devices.&lt;/li&gt;
&lt;li&gt;All devices now have the complete reduced result (W, X, Y, Z).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Final State&lt;/strong&gt;: All devices have the same, complete
reduced result.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This decomposition can be more efficient than a direct AllReduce in
certain network topologies and for large data sizes. It allows for
better utilization of network bandwidth and can reduce overall
communication time&lt;a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="citations"&gt;Citations&lt;/h2&gt;
&lt;section class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;https://marek.ai/allreduce-the-basis-of-multi-device-communication-for-neural-network-training.html&lt;a class="footnote-back" href="#fnref1" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html&lt;a class="footnote-back" href="#fnref2" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html&lt;a class="footnote-back" href="#fnref3" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="ML Systems"/><category term="ml-code"/><category term="llm"/><category term="ml-acceleration"/><category term="hpc-concept"/></entry><entry><title>Concept - Coalescing</title><link href="https://sengopal.me/posts/concept-coalescing" rel="alternate"/><published>2023-12-20T00:00:00-08:00</published><updated>2023-12-20T00:00:00-08:00</updated><author><name>Senthilkumar Gopal</name></author><id>tag:sengopal.me,2023-12-20:/posts/concept-coalescing</id><summary type="html">This post works through the metrics that are critical for ML inference</summary><content type="html">&lt;p&gt;Memory Coalescing is a fundamental performance consideration in HPC
programming&lt;a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; and provides another dimension for
memory efficiency and how &lt;strong&gt;aligned versus misaligned memory
accesses&lt;/strong&gt; can significantly impact throughput.&lt;/p&gt;
&lt;p&gt;The global memory of a CUDA device is implemented with DRAMs. Each
time a DRAM location is accessed, a range of consecutive locations that
includes the requested location is actually accessed. Many sensors are
provided in each DRAM chip and they work in parallel. Each senses the
content of a bit within these consecutive locations. Once detected by
the sensors, the data from all these consecutive locations can be
transferred at very high-speed to the processor. These consecutive
locations accessed and delivered are referred to as DRAM bursts.&lt;/p&gt;
&lt;p&gt;Recognizing the burst mechanism, current CUDA devices employ a
technique that allows the programmers to achieve high global memory
access efficiency by organizing memory access of threads into favorable
patterns. This technique takes advantage of the fact that threads in a
warp execute the same instruction at any given point in time (SIMT).
When all threads in a warp execute a load instruction, the hardware
detects whether they access consecutive global memory locations. If they
do, the hardware combines, or coalesces, all these accesses into a
consolidated access to consecutive DRAM locations. Such coalesced access
allows the DRAMs to deliver data as a burst.&lt;/p&gt;
&lt;h2 id="memory-layout-semantics-and-access-patterns"&gt;Memory Layout
Semantics and Access Patterns&lt;/h2&gt;
&lt;p&gt;Programming languages provide abstract representations of matrices
and arrays, but underneath these abstractions, memory is always linear.
In C/C++ and CUDA, multidimensional arrays are typically stored in
&lt;strong&gt;row-major order&lt;/strong&gt;, meaning consecutive elements of a row
occupy adjacent memory locations.&lt;/p&gt;
&lt;p&gt;This becomes critically relevant in CUDA, where &lt;strong&gt;thread
warps&lt;/strong&gt; execute memory accesses in parallel. When threads within
a warp access consecutive memory addresses, the hardware can
&lt;strong&gt;coalesce&lt;/strong&gt; those accesses into a single wide memory
transaction. Misaligned or scattered accesses, by contrast, require
independent transactions for each thread, resulting in substantial
performance degradation.&lt;/p&gt;
&lt;h3 id="coalesced-access-the-b-matrix"&gt;Coalesced Access – The &lt;em&gt;B&lt;/em&gt;
Matrix&lt;/h3&gt;
&lt;p&gt;Consider the classic case of dense matrix multiplication&lt;a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; When accessing the &lt;em&gt;B&lt;/em&gt;
matrix, each CUDA thread reads a distinct column. Since matrices are
stored in row-major order, reading columns means accessing
&lt;strong&gt;consecutive addresses&lt;/strong&gt; along a row — which are adjacent
in memory.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Image ref - CoffeeBeforeArch" src="/extras/images/coalescing/coalescing-1.png"/&gt;
&lt;figcaption aria-hidden="true"&gt;Image ref - CoffeeBeforeArch&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Image ref - CoffeeBeforeArch&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This access pattern enables &lt;strong&gt;coalescing&lt;/strong&gt;: instead of
multiple separate memory reads, the GPU issues a single transaction that
serves the entire warp. The result is reduced memory latency and
improved effective bandwidth.&lt;/p&gt;
&lt;h3 id="misaligned-access-the-a-matrix"&gt;Misaligned Access – The
&lt;em&gt;A&lt;/em&gt; Matrix&lt;/h3&gt;
&lt;p&gt;In contrast, accessing the &lt;em&gt;A&lt;/em&gt; matrix typically involves each
thread reading a different row and iterating across columns. Due to
row-major layout, rows are &lt;strong&gt;not adjacent in memory&lt;/strong&gt; —
they are separated by the width of the matrix.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Image ref - CoffeeBeforeArch" src="/extras/images/coalescing/coalescing-2.png"/&gt;
&lt;figcaption aria-hidden="true"&gt;Image ref - CoffeeBeforeArch&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;em&gt;Image ref - CoffeeBeforeArch&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;As a result, even if thread accesses are conceptually aligned in the
matrix, they are &lt;strong&gt;thousands of elements apart&lt;/strong&gt; in memory.
These accesses are non-coalesced, forcing the hardware to issue multiple
independent transactions per warp. This behavior incurs significantly
higher latency and reduces throughput.&lt;/p&gt;
&lt;h2 id="optimization-via-transposition"&gt;Optimization via
Transposition&lt;/h2&gt;
&lt;p&gt;To address the misaligned access pattern in the &lt;em&gt;A&lt;/em&gt; matrix, a
simple yet effective transformation can be applied:
&lt;strong&gt;pre-transposing&lt;/strong&gt; the &lt;em&gt;A&lt;/em&gt; matrix prior to the
kernel launch. Transposition transforms rows into columns. When both A
and B matrices are accessed along columns, &lt;strong&gt;all memory accesses
become coalesced&lt;/strong&gt;, optimizing memory throughput without
introducing shared memory or tiling techniques. Implementation-wise,
this requires updating index calculations inside the kernel. Rather than
iterating over fixed rows, threads iterate across columns (mirroring the
access pattern used for B)&lt;a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref"&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="tldr"&gt;TLDR;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Row-major layout&lt;/strong&gt; requires careful design of access
patterns to ensure spatial locality&lt;a class="footnote-ref" href="#fn4" id="fnref4" role="doc-noteref"&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Transposing input matrices can transform misaligned accesses into
coalesced accesses.&lt;/li&gt;
&lt;li&gt;Profiling should always validate performance assumptions; even minor
misalignments can incur major penalties.&lt;/li&gt;
&lt;li&gt;Optimization strategies must consider not only compute efficiency
but also &lt;strong&gt;global memory bandwidth and access
coalescing&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="references"&gt;References&lt;/h3&gt;
&lt;section class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn1"&gt;&lt;p&gt;&lt;a href="https://github.com/coffeeBeforeArch"&gt;CoffeeBeforeArch&lt;/a&gt; Github
Repository&lt;a class="footnote-back" href="#fnref1" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn2"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=_qSP455IekE&amp;amp;t=154s"&gt;CoffeeBeforeArch&lt;/a&gt;
Youtube tutorial&lt;a class="footnote-back" href="#fnref2" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn3"&gt;&lt;p&gt;https://homepages.math.uic.edu/~jan/mcs572f16/mcs572notes/lec35.html&lt;a class="footnote-back" href="#fnref3" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id="fn4"&gt;&lt;p&gt;https://medium.com/distributed-knowledge/cuda-memory-management-use-cases-f9d340f7c704&lt;a class="footnote-back" href="#fnref4" role="doc-backlink"&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;</content><category term="ML Systems"/><category term="ml-code"/><category term="llm"/><category term="ml-acceleration"/></entry></feed>