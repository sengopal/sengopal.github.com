<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Introduction to NLP and Word Vectors | Senthilkumar Gopal
</title>
  <link rel="canonical" href="https://sengopal.github.io/posts/introduction-to-nlp-and-word-vectors.html">

    <link rel="apple-touch-icon" href="https://sengopal.github.io/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="https://sengopal.github.io/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://sengopal.github.io/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="https://sengopal.github.io/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="https://sengopal.github.io/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://sengopal.github.io/theme/css/fontawesome.min.css">
  <link rel="stylesheet" href="https://sengopal.github.io/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://sengopal.github.io/theme/css/theme.css">
  <link rel="stylesheet" href="https://sengopal.github.io/extras/css/custom.css">
  <link rel="stylesheet" href="https://sengopal.github.io/extras/css/skylighting-solarized-theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://sengopal.github.io/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://sengopal.github.io/feeds/nlp.atom.xml">
<meta name="description" content="A post about Introduction to NLP and basics of Word Vectors">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="https://sengopal.github.io/">Senthilkumar Gopal</a></h1>
      <p class="text-muted">Musings of a machine learning engineer</p>
      <ul class="list-inline">
            <li class="list-inline-item"><a href="https://sengopal.github.io/pages/about.html">About me</a></li>
            <li class="list-inline-item"><a href="https://sengopal.github.io/pages/publications.html">Publications</a></li>
            <li class="list-inline-item"><a href="https://sengopal.github.io/pages/software.html">Software</a></li>
            <li class="list-inline-item"><a href="https://sengopal.github.io/pages/talks.html">Talks</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fas fa-rss" href="https://sengopal.github.io/feeds/all.atom.xml" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-github" href="https://github.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-speaker-deck" href="https://speakerdeck.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-medium" href="https://medium.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-twitter" href="https://twitter.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-linkedin" href="https://www.linkedin.com/in/senthilkumargopal" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>    Introduction to NLP and Word Vectors
</h1>
      <hr>
<article class="article">
    <header>
        <ul class="list-inline">
            <li class="list-inline-item text-muted" title="2022-03-16T00:00:00-07:00">
                <i class="fas fa-clock"></i>
                Wed 16 March 2022
            </li>
            <li class="list-inline-item">
                <i class="fas fa-folder-open"></i>
                <a href="https://sengopal.github.io/category/nlp.html">NLP</a>
            </li>
            <li class="list-inline-item">
                <i class="fas fa-tag"></i>
                <a href="https://sengopal.github.io/tag/cs224n.html">#cs224n</a>,                 <a href="https://sengopal.github.io/tag/learning.html">#learning</a>,                 <a href="https://sengopal.github.io/tag/nlp.html">#nlp</a>,                 <a href="https://sengopal.github.io/tag/machine-learning.html">#machine-learning</a>            </li>
        </ul>
    </header>
    <div class="hidden-xs hidden-sm">
        <div id="toc"><ul><li><a class="toc-href" href="#lecture-1-introduction-and-word-vectors" title="Lecture 1 &ndash;
Introduction and Word Vectors">Lecture 1 &ndash;
Introduction and Word Vectors</a><ul><li><a class="toc-href" href="#intent" title="Intent">Intent</a></li><li><a class="toc-href" href="#language-model" title="Language model">Language model</a></li><li><a class="toc-href" href="#how-do-word-vectors-work" title="How do word vectors work">How do word vectors work</a></li><li><a class="toc-href" href="#gpt-3" title="GPT-3">GPT-3</a></li><li><a class="toc-href" href="#what-is-language-and-its-meaning" title="What is language and its
meaning?`">What is language and its
meaning?`</a><ul><li><a class="toc-href" href="#problem-with-wordnet" title="Problem with WordNet">Problem with WordNet</a></li><li><a class="toc-href" href="#problem-with-traditional-nlp" title="Problem with traditional NLP">Problem with traditional NLP</a></li></ul></li><li><a class="toc-href" href="#one-hot-encoding-vector_1" title="One hot encoding vector">One hot encoding vector</a></li><li><a class="toc-href" href="#word-embeddings" title="Word Embeddings">Word Embeddings</a><ul><li><a class="toc-href" href="#example---banking" title="Example - &ldquo;banking&rdquo;">Example - &ldquo;banking&rdquo;</a></li></ul></li><li><a class="toc-href" href="#introduction-to-word2vec_1" title="Introduction to word2vec">Introduction to word2vec</a></li><li><a class="toc-href" href="#determining-the-probability-of-a-word-occurring-in-the-context-of-a-given-center-word" title="Determining
the probability of a word occurring in the context of a given center
word">Determining
the probability of a word occurring in the context of a given center
word</a></li><li><a class="toc-href" href="#likelihood-probability-calculation" title="Likelihood Probability
Calculation">Likelihood Probability
Calculation</a></li><li><a class="toc-href" href="#softmax-function" title="Softmax function">Softmax function</a></li><li><a class="toc-href" href="#construct-word-vectors" title="Construct word vectors">Construct word vectors</a></li><li><a class="toc-href" href="#multivariate-calculus" title="Multivariate Calculus">Multivariate Calculus</a></li><li><a class="toc-href" href="#gensim" title="Gensim">Gensim</a></li><li><a class="toc-href" href="#analogy-task" title="Analogy task">Analogy task</a></li><li><a class="toc-href" href="#why-two-different-vectors" title="Why two different vectors">Why two different vectors</a></li><li><a class="toc-href" href="#question-how-about-words-with-multiple-meanings-homonyms-and-common-words" title="Question:
How about words with multiple meanings (Homonyms) and common words">Question:
How about words with multiple meanings (Homonyms) and common words</a></li><li><a class="toc-href" href="#conclusion" title="Conclusion">Conclusion</a></li><li><a class="toc-href" href="#suggested-reading" title="Suggested reading">Suggested reading</a></li><li><a class="toc-href" href="#references" title="References">References</a></li></ul></li></ul></div>
    </div>
    <div class="content">
        <p>This blog post and the following series captures the path of
understanding NLP, usage of Deep Learning in NLP and the various
algorithms, by roughly following the lecture patterns for the Cs224n
course from Stanford.</p>
<h3 id="lecture-1-introduction-and-word-vectors">Lecture 1 &ndash;
Introduction and Word Vectors</h3>
<p>The following post is primarily about driving home the fact that a
word&rsquo;s meaning can be represented, not perfectly but really rather well
by a large vector of real numbers. This has been an amazing find which
has taken research away from the traditional approaches followed before
deep learning.</p>
<h4 id="intent">Intent</h4>
<ol type="1">
<li>foundation - good deep understanding of the effect of modern methods
for deep learning applied to NLP.</li>
<li>basics &amp; key methods that are used in NLP, recurrent networks,
attention transformers</li>
<li>Ability to build systems in PyTorch</li>
<li>Learning word meanings, dependency parsing, machine translation,
question answering.</li>
</ol>
<p><img alt="Source: xkcd: I Could Care Less" src="https://imgs.xkcd.com/comics/i_could_care_less.png" title="Source: xkcd: I Could Care Less" width="408"/></p>
<h4 id="language-model">Language model</h4>
<ol type="1">
<li><p>Building computational systems that try to get better at guessing
how their words will affect other people and what other people are
meaning by the words that they choose to say.</p></li>
<li><p>It is a system that was constructed by human beings relatively
recently in some sense.</p></li>
</ol>
<h4 id="how-do-word-vectors-work">How do word vectors work</h4>
<p>Language arose for human beings sort of somewhere in the range of
100,000 to a million years ago. But that powerful communication between
human beings quickly set off our ascendancy over other creatures. It was
much more recently again that humans developed writing, which allowed
knowledge to be communicated across distances of time and space. So a
key question for artificial intelligence and human-computer interaction
is how to get computers to be able to understand the information
conveyed in human languages.</p>
<p>We need knowledge to understand language and people well, but it&rsquo;s
also the case that a lot of that knowledge is contained in language
spread out across the books and web pages of the world.</p>
<p>So with recent advancements, machine translation works moderately
well. Learning other people&rsquo;s languages was a human task which required
a lot of effort and concentration. But now to get news from Kenya we can
use Google to translate Swahili from a Kenyan website.</p>
<p><img alt="slide-google-translate" src="/extras/images/introduction-to-nlp-and-word-vectors/2022-03-18-11-07-20-image.png" title="" width="748"/></p>
<h4 id="gpt-3">GPT-3</h4>
<p>One of the recent and biggest development in NLP, including the
popular media was GPT-3, which was a huge new model that was released by
OpenAI. Its exciting as it has started to look the first step on the
path to universal models, where we can train an extremely large model on
the world knowledge of human languages, of how to do tasks. So we are no
longer building a model to detect spam, to detect foreign language
content, rather just building all these separate supervised classifiers
for every different task, since we have a model that understands.</p>
<p>It is really good at predicting words. The two examples are explained
below.</p>
<ol type="1">
<li><p>Write about Elon Musk in the style of Doctor Seuss</p></li>
<li><p>Question prediction from a sentence using couple of examples. The
model started predicting the questions after just two examples.</p></li>
</ol>
<p>The way it generates more text is by just predicting one word at a
time, following words to complete its text.</p>
<p><strong>Another Example:</strong> Translating human language
sentences into SQL.</p>
<p><img alt="gpt-example" src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-11-13-56-image.png" title="gpt-example" width="762"/></p>
<h4 id="what-is-language-and-its-meaning">What is language and its
meaning?`</h4>
<p>How do we represent the meaning of a word? - Webster&rsquo;s dictionary
definition is really focused on the &ldquo;word idea&rdquo;, which is pretty close
to the most common way that linguists think about meaning. However,
<strong>denotational semantics</strong> captures word meaning as being a
pairing between a word which is a signifier or symbol, and the thing
that it signifies, the signified thing which is an idea or thing.</p>
<p>So the meaning of the word chair is the set of things that are
chairs. A term that&rsquo;s also used and similarly applied for the semantics
of programming languages. So traditionally the way that meaning has
normally been handled in natural language processing systems is to make
use of resources like dictionary, and thesaurus in particular. For
example, <strong>WordNet</strong>, which organized words and terms into
both synonyms sets of words that can mean the same thing, and hypernyms
which correspond to IS-A relationships.</p>
<h5 id="problem-with-wordnet">Problem with WordNet</h5>
<p>In WordNet, &ldquo;proficient&rdquo;&rdquo; is listed as a synonym for &ldquo;good&rdquo;, which is
accurate only in some contexts. it is limited as a human constructed
thesaurus. Its difficult to keep it up to date, including more current
terminology. For example, &ldquo;wicked&rdquo;&rdquo; is there for the wicked witch, but
not for more modern colloquial uses. &ldquo;Ninja&rdquo; is another example where
WordNet is not kept up to date. So it requires a lot of human labor, but
even then, it has a set of synonyms but does not really have a good
sense of words that means something similar. So this idea of meaning
similarity is something that would be really useful to make progress on,
and where deep learning models excel.</p>
<h5 id="problem-with-traditional-nlp">Problem with traditional NLP</h5>
<p>Problem with traditional NLP is that words are regarded as discrete
symbols. Symbols like hotel, conference, motel are words, which in deep
learning are referred as a localized representation. Because in a
statistical machine learning systems, these symbols need to be
represented in a statistical model to build a logistic regression model
with words as features, typically like an one-hot encoded vector.</p>
<h4 id="one-hot-encoding-vector_1">One hot encoding vector</h4>
<p>One hot encoding vector has a dimension for each different word. So
that means that we need huge vectors corresponding to the number of
words in our vocabulary. For a high school English dictionary it
probably have about 250,000 words in it and probably need a 500,000
dimensional vector to be able to cope with that. But the bigger with
discrete symbols is that there is no notion of word relationships and
similarity. So for example, if a user searches for Seattle motel, it
should match on documents containing Seattle &ldquo;hotel&rdquo; as well. So in a
mathematical sense, these two vectors are orthogonal, that there&rsquo;s no
natural notion of similarity between them.</p>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-11-27-00-image.png"/></p>
<h4 id="word-embeddings">Word Embeddings</h4>
<blockquote>
<p>you shall know a word by the company it keeps. - J. R Firth</p>
</blockquote>
<p>Modern deep learning method allows encoding similarity in real value
vector themselves. <strong>distributional semantics</strong> - where
word&rsquo;s meaning is going to be given by the words that frequently appear
close to it. This represent a sense for words, <strong>meaning as a
notion of what context that appears in</strong> has been a very
successful idea. It proves to be an extremely computational sense of
semantics, which has just led to it being used everywhere very
successfully in deep learning systems. So when a word appears in a text,
it has a context which are a set of words that appear along with it.</p>
<h5 id="example---banking">Example - &ldquo;banking&rdquo;</h5>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-11-38-34-image.png"/></p>
<p>The word &ldquo;banking&rdquo;&rdquo; occurs in text, and the nearby words (context
words) in some sense represent the meaning of the word banking. Based on
looking at the words that occur in context as vectors, we want to build
dense real valued vector for each word, that in some sense represents
the meaning of that word. The way it will represent the meaning of that
word, is when this vector would be useful for predicting other words
that occur in the context.</p>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-11-42-28-image.png"/></p>
<p>A simple 8-dimensional illustration (<em>in reality, usually 300
dimensional vectors are used</em>), of the neural word representations
or &ldquo;word embeddings&rdquo;, represents the distributed representation, not a
localized representation because the meaning of the word banking is
spread over all 300 dimensions of the vector. These are called word
embeddings because, in a group of words, these representations place
them in a high dimensional vector space, and so they&rsquo;re embedded into
that space.</p>
<h4 id="introduction-to-word2vec_1">Introduction to word2vec</h4>
<p>Word2Vec was introduced by <strong>Tomas Mikolov and
colleagues</strong> in 2013 as a framework for learning word vectors, It
uses a lot of text, commonly refer to as a corpus (originated from the
Latin word for body), meaning a body of text., with. a vocabulary size
of 400,000 and then create vectors for every word. To determine the best
vector for each word, we can learn these word vectors from just a big
pile of text by doing this distributional similarity task of being able
to predict, what words occur in the context of other words. So
specifically, going through the texts, and using a center word C, and
context words O, calculate the probability of a context word occurring,
given the center word according to our current model. Since the corpus
is available, it is known that certain words actually occur in the
context of that center word, we can keep adjusting the word vectors to
maximize the probability that&rsquo;s assigned to words that actually occur in
the context of the center word as we proceed through these texts.</p>
<p><img alt="word-vector-window" src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-11-54-10-image.png" title="" width="369"/>.
<img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-11-55-03-image.png"/></p>
<h4 id="determining-the-probability-of-a-word-occurring-in-the-context-of-a-given-center-word">Determining
the probability of a word occurring in the context of a given center
word</h4>
<p>For each position in the corpus, we want to predict context words
within a window of fixed size, given the center word W<sub>j</sub></p>
<p>Ideally we need to give high probability to words that actually occur
in the context. i.e., identify the likelihood of predicting words in the
context of other words correctly and this likelihood will be defined in
terms of the word vectors. These form the parameters of our model, and
it will the product of using each word as the center word, and each
other context word in the window to determine the probability of
predicting that context word in the center word. And to learn this
model, there would be an objective function, also called a cost or a
loss that we want to optimize. And essentially <strong>maximize the
likelihood of the context we see around center words.</strong></p>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-12-00-56-image.png"/></p>
<p>Following changes are made to the objective function:</p>
<ol type="1">
<li><p>Use log likelihood to convert all the products into
sums.</p></li>
<li><p>Also use average log likelihood, denoted by <em>1/T</em></p></li>
<li><p>Minimize our objective function, <span class="math inline"><em>J</em>(<em>&theta;</em>)</span> becomes maximizing our
predictive accuracy.</p></li>
</ol>
<blockquote>
<p>Note: Each word will have two word vectors - One word vector for when
it&rsquo;s used as the center word, and a different word vector when that&rsquo;s
used as a context word. This is done to simplify the math and the
optimization and makes building word vectors a lot easier,</p>
</blockquote>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-12-07-40-image.png"/></p>
<h4 id="likelihood-probability-calculation">Likelihood Probability
Calculation</h4>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-12-09-03-image.png"/></p>
<p>For a particular center word v<sub>c</sub> and a particular context
word u<sub>o</sub>, look up the vector representation of each word, and
take the dot product of those two vectors.</p>
<blockquote>
<p>Dot product is a natural measure for similarity between words because
it generates a component that adds to that dot product sum. If both are
negative, it&rsquo;ll add a lot to the dot product sum. If one&rsquo;s positive and
one&rsquo;s negative,it&rsquo;ll subtract from the similarity measure. Both of them
are zero, won&rsquo;t change the similarity.</p>
</blockquote>
<p><strong>if two words have a larger dot product, that means they&rsquo;re
more similar.</strong></p>
<h4 id="softmax-function">Softmax function</h4>
<p>The next step is to convert this how to turn this into a probability
distribution and to avoid negative probabilities exponentiate them and
normalize by dividing by the sum of the numerator quantity for each
different word in the vocabulary. This ensures that the distribution is
between 0 and 1. This formulates the softmax function which will take
any R in vector and turn it into values between 0 to 1.</p>
<ol type="1">
<li><p>&ldquo;max&rdquo; term - accentuates and emphasizes the big contents in the
different dimensions of calculating similarity, as it exponentiates the
probabilities.</p></li>
<li><p>&ldquo;soft&rdquo; term - gives a probability distribution of the next
possible words.</p></li>
</ol>
<blockquote>
<p>max function returns just one the biggest term, whereas softmax takes
a set of numbers, scales them, and returns a probability
distribution.</p>
</blockquote>
<h4 id="construct-word-vectors">Construct word vectors</h4>
<p>The plan is to optimize the word vectors to minimize the loss
function, i.e.&nbsp;maximize the probability of the words that were actually
in the context of the center word. <span class="math inline"><em>&theta;</em></span> represents all of the model
parameters in one very long vector. So for the model, word vectors are
the only parameters. So for each word there are two vectors, context
vector and center vector. And each of those is a D dimensional vector,
where D might be 300 and we have V many words in the vocabulary. So the
model is of size <span class="math inline">2&emsp14;*&emsp14;<em>D</em>&emsp14;*&emsp14;<em>V</em></span> . So for a
vocabulary of size 500k and with a 300 dimensionality vector, there
would be millions of millions of parameters, to train and maximize the
prediction of context words.</p>
<h4 id="multivariate-calculus">Multivariate Calculus</h4>
<p>Derivatives can be computed using multivariate calculus and the
gradients can be determined by walking downhill to minimize loss, using
stochastic gradient descent. We have <span class="math inline"><em>J</em>(<em>&theta;</em>)</span> that is needed to
minimize the average negative log likelihood. And then we iterate
through the words in each context, to compute <span class="math inline"><em>J</em>(<em>&theta;</em>)</span> between M words on
both sides except with itself. Then determine the log probability of the
context word at that position, given the word that&rsquo;s in the center
position <span class="math inline"><em>t</em></span>.</p>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-11-37-image.png"/></p>
<p>Probability <span class="math inline"><em>P</em>(<em>o</em>|<em>c</em>)</span> can be
determined as the softmax of the dot product of <span class="math inline"><em>u</em><sub>0</sub>&emsp14;*&emsp14;<em>V</em><sub><em>c</em></sub></span>
normalized by the sum of all probabilities of the word distribution. To
compute the gradient, the partial derivative of this expression with
respect to every parameter in the model is computed, and all the
parameters in the model are the components depending on the dimensions
of the word vectors of every word.</p>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-19-25-image.png"/></p>
<p>Walking through these in steps, the partial derivative with respect
to the center word vector(<em>a 300 dimensional word vector</em>) is
calculated. Considering the expression as A/B, using log turns it into
log A minus log B. Then the partial derivative of <span class="math inline"><em>V</em><sub><em>c</em></sub></span> is simply
<span class="math inline"><em>u</em><sub>0</sub></span></p>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-31-59-image.png"/></p>
<p>Now using the chain rule the denominator can be computed. This part
is essentially going from outside to inside in terms of derivatives. The
above image is more cleaner explanation.</p>
<p>Combining all the expressions together, rewriting the expression, by
moving the sum <code>w = 1 to v</code> inside the summation expression
we end up getting exactly the softmax formula probability that we saw
when we started. So the expression more conveniently becomes <span class="math inline"><em>U</em><sub>0</sub></span> minus the sum over
<code>X = 1 to V</code> of the probability of X given C times <span class="math inline"><em>U</em><sub><em>x</em></sub></span>.</p>
<p>And so what we have at that moment is this thing here is an
<strong>expectation.</strong></p>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-34-13-image.png"/></p>
<p>This is not an average over all the context vectors weighted by their
probability according to the model.it&rsquo;s always the case with these
softmax style models, we get the observed minus the expected for the
derivatives. So the model is good if on average it predicts exactly the
word vector that we actually see.</p>
<p>The next step is to try and adjust the parameters of our model to try
and make the probability estimates as high as we possibly can using
stochastic gradient.</p>
<h4 id="gensim">Gensim</h4>
<p>GENESIM is a package often used for word vectors, it&rsquo;s not really
used for deep learning and for testing glove word vectors were used by
loading a hundred dimensional word vectors.</p>
<p><img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-46-10-image.png"/></p>
<p>Checking the first 10 dimensions of the word vectors for
<em>bread</em> and <em>croissant</em>, these two words are a bit
similar, so both of them are negative in the first dimension, positive
in the second, negative in the third, positive in the fourth, negative
in the fifth and so on. So they might have a fair bit of dot product
which is kind of what we want because bread and croissant are kind of
similar. Few more examples,</p>
<ol type="1">
<li><p>Similar to banana</p></li>
<li><p>Similar to brioche</p></li>
<li><p>Similar to USA</p></li>
</ol>
<p><img alt="most-similar-banana" src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-47-48-image.png" title="most-similar-banana" width="282"/>.
<img src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-48-24-image.png"/>
<img alt="most-similar-usa" src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-49-08-image.png" title="" width="297"/></p>
<h4 id="analogy-task">Analogy task</h4>
<p>The idea of the analogy task defines that we start with a word like
<strong><em>king</em></strong>, and should be able to subtract out a
male component from that, add back in a woman component, and then we
should be able to ask for the appropriate word, which should be the word
<strong><em>queen</em></strong>.</p>
<p>Few other examples are illustrated below using Gensim</p>
<p><img alt="analogy-example" src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-50-13-image.png" title="" width="393"/>.
<img alt="analogy-example" src="/extras/images/lecture-01-introduction-word-vectors/2022-03-18-14-53-34-image.png" title="" width="400"/></p>
<p>Even linguistic analogies, such as the analogy of tall is to tallest
as long is to longest.</p>
<h4 id="why-two-different-vectors">Why two different vectors</h4>
<p>Recall the equation for <span class="math inline"><em>J</em>(<em>&theta;</em>)</span> taking a sum over
every word which is appearing as the center word, and then inside that
there&rsquo;s a second sum which is for each word in the context, where we
count each word as a context word, and then for one particular term of
that objective function you&rsquo;ve got a particular context word and a
particular center word that you&rsquo;re then sort of summing over different
context words for each center word, and then you&rsquo;re summing over all of
the decisions of different center words. In case the window contains the
same word as the center and context word, it messes with the
derivatives. while taking them as separate vectors ensures that this
issue does not occur. The two vectors would be very similar, but not
identical due to technical reasons such as occurring at the ends of
documents and other similar differences.</p>
<p>The usual method (followed for word2vec algorithm) is to average
those two vectors and consider the average vector as the representation
of the word.</p>
<h4 id="question-how-about-words-with-multiple-meanings-homonyms-and-common-words">Question:
How about words with multiple meanings (Homonyms) and common words</h4>
<ol type="1">
<li><p>For a word like <strong>star</strong>, that can be astronomical
object or it can be a movie star,. Taking all those uses of the word
star and collapsing them together into one word vector. actually turns
out to work rather well.</p></li>
<li><p>For very common words that are commonly referred to as
<strong>function words</strong> by linguists, which includes words like
<em>so</em> and <em>not</em>, prepositions, words such as <em>to</em>,
<em>on</em> etc., the suspicion is that the word vectors would not work
very well because they occur in all kinds of different contexts. However
large language models do a great job in those words as well</p></li>
</ol>
<h4 id="conclusion">Conclusion</h4>
<p>Another feature of the word2vec model is that it actually ignores the
position of words, ie., it will predict every word around the center
word before or after, one or two positions away in either direction
using the one probability function. But this sort of destroys the
ability at capturing the subtleties more common grammatical words which
occur or do not occur at the end of a sentence. But we can build
slightly different models that are more sensitive to the structure of
sentences, which can then perform better on these errors. So word2vec is
more of a framework for building word vectors, and there are several
variant precise algorithms within the framework. One such variant is the
prediction of either the context words (skip grand model) or the center
word.</p>
<p>So to learn word vectors we start off by having a vector for each
word type both for context and outside and those vectors we initialize
randomly, so that we just place small little numbers that are randomly
generated in each vector component. And that&rsquo;s just the starting point,
And from there on we are using an iterative algorithm where we are
progressively updating those word vectors, so they do a better job at
predicting which words appear in the context of other words. And the way
that we are going to do that is by using the gradients and once we have
a gradient, we can walk in the opposite direction of the gradient and we
are then walking downhill, i.e.&nbsp;we are minimizing your loss and repeat
until our word vectors get as good as possible.</p>
<h4 id="suggested-reading">Suggested reading</h4>
<ol type="1">
<li><a href="http://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of
Word Representations in Vector Space</a> (original word2vec paper)</li>
<li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed
Representations of Words and Phrases and their Compositionality</a>
(negative sampling paper)</li>
</ol>
<h4 id="references">References</h4>
<ol type="1">
<li><a href="https://www.youtube.com/watch?v=8rXD5-xhemo">Video</a></li>
<li><a href="http://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture01-wordvecs1.pdf">Slides</a></li>
<li><a href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf">Notes</a></li>
</ol>
    </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
    <ul class="col-sm-6 list-inline">
        <li class="list-inline-item"><a href="https://sengopal.github.io/#">Resume</a></li>
        <li class="list-inline-item"><a href="https://sengopal.github.io/archives">Archives</a></li>
        <li class="list-inline-item"><a href="https://sengopal.github.io/categories">Categories</a></li>
        <li class="list-inline-item"><a href="https://sengopal.github.io/tags">Tags</a></li>
    </ul>
    <p class="col-sm-6 text-sm-right text-muted">
        Opinions my own. Made with &#x2764; using <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a> / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>        
    </p>
</div>    </div>
  </footer>

</body>

</html>