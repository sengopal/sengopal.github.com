<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Paged Attention and Chunked Prefill for LLM Inference | Senthilkumar Gopal
</title>
  <link rel="canonical" href="https://sengopal.me/posts/paged-attention-and-chunked-prefill-for-llm-inference.html">

    <link rel="apple-touch-icon" href="https://sengopal.me/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="https://sengopal.me/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://sengopal.me/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="https://sengopal.me/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="https://sengopal.me/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/all.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/theme.css">
  <link rel="stylesheet" href="https://sengopal.me/extras/css/skylighting-solarized-theme.css">
  <link rel="stylesheet" href="https://sengopal.me/extras/css/custom.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://sengopal.me/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://sengopal.me/feeds/large-language-models.atom.xml">
<meta name="description" content="This post explains how Paged Attention and Chunked Prefill optimize memory and computation in vLLM by organizing key-value caches into dynamic blocks and processing input sequences in manageable chunks. It includes a simple walkthrough with tensor shapes and code to showcase their integration for LLM inference.">

<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o);
      a.async = 1;
      a.src = g;
      m = s.getElementsByTagName(o)[0];
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-67843911-1', 'auto');
    ga('send', 'pageview');
  </script>


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="https://sengopal.me/">Senthilkumar Gopal</a></h1>
      <p class="text-muted">Musings of a machine learning researcher, engineer and leader</p>
      <ul class="list-inline">
            <li class="list-inline-item"><a href="https://sengopal.me/pages/about.html">About me</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/publications.html">Publications</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/software.html">Software</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/talks.html">Talks</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fas fa-rss" href="https://sengopal.me/feeds/all.atom.xml" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-github" href="https://github.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-speaker-deck" href="https://speakerdeck.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-google-scholar" href="https://scholar.google.com/citations?user=bs8WraEAAAAJ&hl=en" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-medium" href="https://medium.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-twitter" href="https://twitter.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-linkedin" href="https://www.linkedin.com/in/senthilkumargopal" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
<article class="article">
    <header>
        <ul class="list-inline">
            <li class="list-inline-item text-muted" title="2024-12-28T00:00:00-08:00">
                <i class="fas fa-clock"></i>
                Sat 28 December 2024
            </li>
            <li class="list-inline-item">
                <i class="fas fa-folder-open"></i>
                <a href="https://sengopal.me/category/large-language-models">Large Language Models</a>
            </li>
            <li class="list-inline-item">
                <i class="fas fa-tag"></i>
                <a href="https://sengopal.me/tag/ml-code">#ml-code</a>,                 <a href="https://sengopal.me/tag/llm">#llm</a>,                 <a href="https://sengopal.me/tag/vllm">#vllm</a>,                 <a href="https://sengopal.me/tag/serving">#serving</a>            </li>
        </ul>
    </header>
    <h1>Paged Attention and Chunked Prefill for LLM Inference</h1>
    <div class="hidden-xs hidden-sm">
        <nav class="toc" role="doc-toc">
<ul>
<li><a href="#memory-bottlenecks-in-llm-inference" id="toc-memory-bottlenecks-in-llm-inference">Memory Bottlenecks in LLM
Inference</a></li>
<li><a href="#paged-attention" id="toc-paged-attention">Paged
Attention</a>
<ul>
<li><a href="#mechanism" id="toc-mechanism">Mechanism</a></li>
<li><a href="#example-illustration" id="toc-example-illustration">Example Illustration</a></li>
</ul></li>
<li><a href="#chunked-prefill" id="toc-chunked-prefill">Chunked
Prefill</a>
<ul>
<li><a href="#mechanism-1" id="toc-mechanism-1">Mechanism</a></li>
<li><a href="#example" id="toc-example">Example</a></li>
</ul></li>
<li><a href="#combined-example-and-execution-flow" id="toc-combined-example-and-execution-flow">Combined Example and
Execution Flow</a>
<ul>
<li><a href="#configuration" id="toc-configuration">Configuration</a></li>
<li><a href="#prefill-phase" id="toc-prefill-phase">Prefill
Phase</a></li>
<li><a href="#decoding-phase" id="toc-decoding-phase">Decoding
Phase</a></li>
<li><a href="#tensor-shapes-summary" id="toc-tensor-shapes-summary">Tensor Shapes Summary</a></li>
</ul></li>
<li><a href="#sample-code-structure-with-paged-attention-and-chunked-prefill-implementation" id="toc-sample-code-structure-with-paged-attention-and-chunked-prefill-implementation">Sample
code structure with Paged Attention and Chunked Prefill
Implementation</a>
<ul>
<li><a href="#operational-flow" id="toc-operational-flow">Operational
Flow</a></li>
</ul></li>
<li><a href="#conclusion" id="toc-conclusion">Conclusion</a>
<ul>
<li><a href="#references" id="toc-references">References</a></li>
</ul></li>
</ul>
</nav>
    </div>
    <div class="content">
        <p>Large Language Models (LLMs) impose significant memory and compute
demands during inference, particularly when handling long sequences or
large batch sizes. To address these challenges, the vLLM system
introduces two key optimization techniques - Paged Attention<a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a> and Chunked Prefill<a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>
that target memory efficiency at different stages of inference. This
post examines these techniques, clarifying their roles, mechanisms, and
interplay. We demonstrate that they can be jointly employed to optimize
both the prefill and decoding phases without conflict. Through tensor
shape analysis and execution flow, we illustrate how these strategies
improve throughput and memory utilization.</p>
<h2 id="memory-bottlenecks-in-llm-inference">Memory Bottlenecks in LLM
Inference</h2>
<p>Inference in autoregressive LLMs typically involves two phases:</p>
<ol type="1">
<li><strong>Prefill Phase:</strong> The model processes a prompt
sequence, computes hidden states, and builds key-value caches for
subsequent decoding.</li>
<li><strong>Decoding Phase:</strong> The model generates tokens
autoregressively, using previously computed key-value caches.</li>
</ol>
<p>Both phases can be memory-intensive. In the prefill phase, peak
memory usage can spike when processing long sequences. In the decoding
phase, the cumulative storage of key-value tensors can saturate memory,
particularly in multi-user, high-throughput settings.</p>
<h2 id="paged-attention">Paged Attention</h2>
<p><strong>Paged Attention</strong> is a memory management strategy that
organizes key-value tensors into fixed-size <strong>blocks</strong>.
Instead of pre-allocating memory proportional to the maximum possible
sequence length, Paged Attention dynamically allocates and pages these
blocks into memory as needed during attention score computation.</p>
<h3 id="mechanism">Mechanism</h3>
<p>Let the input sequence have length <span class="math inline"><em>L</em></span>, batch size <span class="math inline"><em>B</em></span>, hidden size <span class="math inline"><em>H</em></span>, and number of heads <span class="math inline"><em>n</em><sub>head</sub></span>. Without Paged
Attention, key-value tensors are stored as:</p>
<p><span class="math display">Keys,
Values ∈ ℝ<sup><em>B</em> × <em>L</em> × <em>n</em><sub>head</sub> × <em>d</em><sub>head</sub></sup></span></p>
<p>Paged Attention partitions these tensors into <strong>blocks</strong>
of size <code>block_size</code>. Each block stores:</p>
<p><span class="math display">Block
Tensor ∈ ℝ<sup><em>B</em> × block_size × <em>n</em><sub>head</sub> × <em>d</em><sub>head</sub></sup></span></p>
<p>During attention computation at position <span class="math inline"><em>t</em></span>, only the relevant blocks
containing key-value pairs from positions <span class="math inline">0</span> to <span class="math inline"><em>t</em> − 1</span> are loaded into memory.</p>
<h3 id="example-illustration">Example Illustration</h3>
<p>Here’s your entire content block cleaned and fully formatted for
Markdown with MathJAX, using <code>$</code> and <code>$$</code> properly
and keeping <code>_</code> intact:</p>
<hr/>
<p>Consider the following configuration:</p>
<ul>
<li>Sequence length <span class="math inline"><em>L</em> = 12</span></li>
<li>Batch size <span class="math inline"><em>B</em> = 2</span></li>
<li>Hidden size <span class="math inline"><em>H</em> = 8</span></li>
<li>Number of heads <span class="math inline"><em>n_head</em> = 2</span></li>
<li>Head dimension <span class="math inline"><em>d_head</em> = 4</span>
(since <span class="math inline"><em>H</em> = <em>n_head</em> × <em>d_head</em></span>)</li>
<li>Block size = 4</li>
</ul>
<p>The key-value tensor shapes without Paged Attention would be:</p>
<p><span class="math display">Keys,
Values ∈ ℝ<sup>2 × 12 × 2 × 4</sup></span></p>
<p>With Paged Attention, the sequence is divided into:</p>
<ul>
<li>Block 1: Positions 0–3 → Shape <span class="math inline">(2, 4, 2, 4)</span><br/>
</li>
<li>Block 2: Positions 4–7 → Shape <span class="math inline">(2, 4, 2, 4)</span><br/>
</li>
<li>Block 3: Positions 8–11 → Shape <span class="math inline">(2, 4, 2, 4)</span></li>
</ul>
<p>When computing attention for position <span class="math inline"><em>t</em> = 9</span>, the model dynamically
loads:</p>
<ul>
<li>Block 1: Positions 0–3</li>
<li>Block 2: Positions 4–7</li>
<li>First two positions of Block 3: Positions 8–9</li>
</ul>
<h2 id="chunked-prefill">Chunked Prefill</h2>
<p><strong>Chunked Prefill</strong> is an optimization technique applied
during the prefill phase to control peak memory usage. It divides the
input sequence into fixed-size <strong>chunks</strong> and processes
them sequentially, limiting memory allocation for hidden state and
key-value computation.</p>
<h3 id="mechanism-1">Mechanism</h3>
<p>Given a sequence of length ( L ), the sequence is partitioned into
chunks of size <code>chunk_size</code>. For each chunk:</p>
<ol type="1">
<li>The model computes the hidden states for tokens within the
chunk.</li>
<li>It computes and caches the key-value tensors corresponding to these
tokens.</li>
</ol>
<p>Critically, <strong>Chunked Prefill performs full attention
computation within each chunk</strong>, meaning each token’s query
attends to all preceding tokens, including those in prior chunks.</p>
<p>However, during prefill, the model <strong>does not compute attention
outputs for future decoding tokens</strong>; it only prepares the
key-value cache required for the decoding phase.</p>
<h3 id="example">Example</h3>
<p>Using the same configuration:</p>
<ul>
<li>Sequence length <span class="math inline"><em>L</em> = 12</span></li>
<li>Batch size <span class="math inline"><em>B</em> = 2</span></li>
<li>Hidden size <span class="math inline"><em>H</em> = 8</span></li>
<li>Number of heads <span class="math inline"><em>n_head</em> = 2</span></li>
<li>Head dimension <span class="math inline"><em>d_head</em> = 4</span></li>
<li>Chunk size = 6</li>
</ul>
<p>The sequence is divided into:</p>
<ul>
<li><strong>Chunk 1:</strong> Positions 0–5</li>
<li><strong>Chunk 2:</strong> Positions 6–11</li>
</ul>
<p>For each chunk:</p>
<ul>
<li>Hidden state tensor shape: <span class="math inline">(2, 6, 8)</span><br/>
</li>
<li>Key-value tensor shape: <span class="math inline">(2, 6, 2, 4)</span></li>
</ul>
<p><strong>Attention Computation during Prefill:</strong></p>
<p>For token at position <span class="math inline"><em>t</em> = 4</span>
in Chunk 1, attention is computed over positions 0–3 within Chunk
1.<br/>
For token at position <span class="math inline"><em>t</em> = 7</span> in
Chunk 2, attention is computed over:</p>
<ul>
<li>Cached key-values from Chunk 1 (positions 0–5)</li>
<li>Tokens 6–6 (preceding tokens in Chunk 2)</li>
</ul>
<h2 id="combined-example-and-execution-flow">Combined Example and
Execution Flow</h2>
<h3 id="configuration">Configuration</h3>
<p>Let:</p>
<ul>
<li>Sequence length <span class="math inline"><em>L</em> = 12</span><br/>
</li>
<li>Batch size <span class="math inline"><em>B</em> = 2</span><br/>
</li>
<li>Hidden size <span class="math inline"><em>H</em> = 8</span><br/>
</li>
<li>Number of heads <span class="math inline"><em>n_head</em> = 2</span><br/>
</li>
<li>Head dimension <span class="math inline"><em>d_head</em> = 4</span><br/>
</li>
<li>Chunk size = 6<br/>
</li>
<li>Block size = 4</li>
</ul>
<h3 id="prefill-phase">Prefill Phase</h3>
<p>The model processes:</p>
<ul>
<li><strong>Chunk 1 (positions 0–5):</strong>
<ul>
<li>Hidden states: <span class="math inline">(2, 6, 8)</span><br/>
</li>
<li>Key-value tensors cached: <span class="math inline">(2, 6, 2, 4)</span><br/>
</li>
</ul></li>
<li><strong>Chunk 2 (positions 6–11):</strong>
<ul>
<li>Hidden states: <span class="math inline">(2, 6, 8)</span><br/>
</li>
<li>Key-value tensors cached: <span class="math inline">(2, 6, 2, 4)</span></li>
</ul></li>
</ul>
<p>Each token in Chunk 2 attends to all cached key-value pairs from
Chunk 1 and preceding tokens in Chunk 2.</p>
<h3 id="decoding-phase">Decoding Phase</h3>
<p>During decoding, suppose the model is generating token at position
<span class="math inline"><em>t</em> = 12</span>:</p>
<ul>
<li>The model computes attention for the query at <span class="math inline"><em>t</em> = 12</span> over all cached key-value
tensors from positions 0–11.</li>
<li>Key-value tensors are organized in <strong>blocks</strong>:</li>
</ul>
<p>Paged Attention dynamically loads the necessary blocks during
attention computation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Block</th>
<th style="text-align: center;">Positions</th>
<th style="text-align: center;">Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0–3</td>
<td style="text-align: center;">(2, 4, 2, 4)</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4–7</td>
<td style="text-align: center;">(2, 4, 2, 4)</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8–11</td>
<td style="text-align: center;">(2, 4, 2, 4)</td>
</tr>
</tbody>
</table>
<h3 id="tensor-shapes-summary">Tensor Shapes Summary</h3>
<table>
<colgroup>
<col style="width: 46%"/>
<col style="width: 53%"/>
</colgroup>
<thead>
<tr>
<th>Component</th>
<th>Tensor Shape</th>
</tr>
</thead>
<tbody>
<tr>
<td>Key/Value tensor (no paging)</td>
<td>(B, L, n_head, d_head)</td>
</tr>
<tr>
<td>Key/Value tensor in Paged Attention</td>
<td>(B, block_size, n_head, d_head) per block</td>
</tr>
<tr>
<td>Key/Value tensor in Chunked Prefill</td>
<td>(B, chunk_size, n_head, d_head) per chunk</td>
</tr>
</tbody>
</table>
<hr/>
<p>A simple structure of how <code>block_size</code> and
<code>chunk_size</code> are utilized in attention mechanisms is provided
below.</p>
<p><strong>Paged Attention and <code>block_size</code>:</strong></p>
<p>Paged Attention manages the Key-Value (KV) cache by dividing it into
fixed-size blocks, allowing dynamic memory allocation and efficient
handling of varying sequence lengths. The <code>block_size</code>
parameter determines the number of tokens each block can store. In vLLM,
this is evident in the attention backend implementations.</p>
<p><em>Example from <code>flash_attn.py</code>:</em></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="co"># Each block can contain up to block_size tokens.</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a>block_tables: Optional[torch.Tensor]</span></code></pre></div>
<p>In this snippet, <code>block_tables</code> is a tensor that maps
sequences to their respective blocks in the KV cache, with each block
capable of holding up to <code>block_size</code> tokens. This structure
enables efficient memory usage by allocating only the necessary amount
of memory based on the sequence length.</p>
<p><strong>Chunked Prefill and <code>chunk_size</code>:</strong></p>
<p>Chunked Prefill processes long input sequences by dividing them into
smaller chunks, reducing peak memory usage during the prefill phase. The
<code>chunk_size</code> parameter defines the size of these chunks. In
vLLM, the attention backends handle chunking to ensure that memory
constraints are respected during computation.</p>
<p><em>Example from <code>pallas.py</code>:</em></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a aria-hidden="true" href="#cb2-1" tabindex="-1"></a><span class="co"># Make sure the chunk size is a multiple of 2.</span></span>
<span id="cb2-2"><a aria-hidden="true" href="#cb2-2" tabindex="-1"></a>chunk_size <span class="op">=</span> chunk_size <span class="op">//</span> <span class="dv">2</span> <span class="op">*</span> <span class="dv">2</span></span>
<span id="cb2-3"><a aria-hidden="true" href="#cb2-3" tabindex="-1"></a>num_chunks <span class="op">=</span> (batch_size <span class="op">+</span> chunk_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> chunk_size</span></code></pre></div>
<p>Here, the code ensures that <code>chunk_size</code> is an even number
and calculates <code>num_chunks</code>, the total number of chunks
needed to process the batch. This approach allows the model to handle
large sequences by processing manageable chunks sequentially, thereby
optimizing memory usage.</p>
<p><strong>Integration in Attention Computation:</strong></p>
<p>The interplay between chunk size and block size exposes a tunable
trade-off between <strong>latency</strong> and <strong>memory
efficiency</strong>. A smaller chunk size reduces peak memory during
prefill but may increase latency due to more sequential chunk
processing. A smaller block size reduces persistent memory overhead but
may increase the number of memory page operations, potentially affecting
throughput.</p>
<p>Adaptive strategies that adjust chunk and block sizes based on
sequence length, batch size, and memory pressure may further improve
resource utilization. During attention computation, especially in
scenarios involving long sequences or large batches, vLLM’s attention
backends utilize both <code>block_size</code> and
<code>chunk_size</code> to manage memory effectively. The KV cache is
organized into blocks of size <code>block_size</code>, and input
sequences are processed in chunks of size <code>chunk_size</code>. This
dual approach ensures that the model can handle extensive inputs without
exceeding memory limitations.</p>
<p><em>Example from <code>flashinfer.py</code>:</em></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="co"># Get the number of valid blocks based on sequence length.</span></span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2" tabindex="-1"></a>block_table_bound <span class="op">=</span> seq_len <span class="op">//</span> <span class="va">self</span>.block_size <span class="op">+</span> <span class="dv">1</span> <span class="cf">if</span> seq_len <span class="op">%</span> <span class="va">self</span>.block_size <span class="op">!=</span> <span class="dv">0</span> <span class="cf">else</span> seq_len <span class="op">//</span> <span class="va">self</span>.block_size</span></code></pre></div>
<p>This snippet calculates the number of valid blocks required for a
given sequence length, ensuring that the KV cache is allocated
appropriately. By managing the KV cache in blocks and processing input
in chunks, vLLM effectively balances memory usage and computational
efficiency during attention operations.</p>
<h2 id="sample-code-structure-with-paged-attention-and-chunked-prefill-implementation">Sample
code structure with Paged Attention and Chunked Prefill
Implementation</h2>
<p>To provide further clarity on the operational dynamics of
<strong>Paged Attention</strong> and <strong>Chunked Prefill</strong>,
the below includes a code walkthrough that models the mechanisms used in
the vLLM system. The following Python code captures the essential logic
behind how the key-value (KV) cache is organized into blocks and how
input sequences are processed in chunks.</p>
<p>This example is based on the configuration discussed earlier:</p>
<ul>
<li>Batch size <span class="math inline"><em>B</em> = 2</span><br/>
</li>
<li>Sequence length <span class="math inline"><em>L</em> = 12</span><br/>
</li>
<li>Hidden size <span class="math inline"><em>H</em> = 8</span><br/>
</li>
<li>Number of attention heads <span class="math inline"><em>n_head</em> = 2</span><br/>
</li>
<li>Head dimension <span class="math inline"><em>d_head</em> = 4</span><br/>
</li>
<li>Block size <span class="math inline">4</span><br/>
</li>
<li>Chunk size <span class="math inline">6</span></li>
</ul>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a aria-hidden="true" href="#cb4-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-2"><a aria-hidden="true" href="#cb4-2" tabindex="-1"></a></span>
<span id="cb4-3"><a aria-hidden="true" href="#cb4-3" tabindex="-1"></a><span class="co"># Configuration</span></span>
<span id="cb4-4"><a aria-hidden="true" href="#cb4-4" tabindex="-1"></a>B <span class="op">=</span> <span class="dv">2</span>                     <span class="co"># Batch size</span></span>
<span id="cb4-5"><a aria-hidden="true" href="#cb4-5" tabindex="-1"></a>L <span class="op">=</span> <span class="dv">12</span>                    <span class="co"># Sequence length</span></span>
<span id="cb4-6"><a aria-hidden="true" href="#cb4-6" tabindex="-1"></a>H <span class="op">=</span> <span class="dv">8</span>                     <span class="co"># Hidden size</span></span>
<span id="cb4-7"><a aria-hidden="true" href="#cb4-7" tabindex="-1"></a>n_head <span class="op">=</span> <span class="dv">2</span>                <span class="co"># Number of heads</span></span>
<span id="cb4-8"><a aria-hidden="true" href="#cb4-8" tabindex="-1"></a>d_head <span class="op">=</span> H <span class="op">//</span> n_head      <span class="co"># Head dimension</span></span>
<span id="cb4-9"><a aria-hidden="true" href="#cb4-9" tabindex="-1"></a>block_size <span class="op">=</span> <span class="dv">4</span>            <span class="co"># Paged Attention block size</span></span>
<span id="cb4-10"><a aria-hidden="true" href="#cb4-10" tabindex="-1"></a>chunk_size <span class="op">=</span> <span class="dv">6</span>            <span class="co"># Chunked Prefill size</span></span>
<span id="cb4-11"><a aria-hidden="true" href="#cb4-11" tabindex="-1"></a></span>
<span id="cb4-12"><a aria-hidden="true" href="#cb4-12" tabindex="-1"></a><span class="co"># Initialize key-value cache using block table</span></span>
<span id="cb4-13"><a aria-hidden="true" href="#cb4-13" tabindex="-1"></a>num_blocks <span class="op">=</span> (L <span class="op">+</span> block_size <span class="op">-</span> <span class="dv">1</span>) <span class="op">//</span> block_size</span>
<span id="cb4-14"><a aria-hidden="true" href="#cb4-14" tabindex="-1"></a></span>
<span id="cb4-15"><a aria-hidden="true" href="#cb4-15" tabindex="-1"></a><span class="co"># Each sequence in batch has its own block table mapping token idx to physical block</span></span>
<span id="cb4-16"><a aria-hidden="true" href="#cb4-16" tabindex="-1"></a>block_table <span class="op">=</span> torch.arange(num_blocks).repeat(B, <span class="dv">1</span>)  <span class="co"># [B, num_blocks]</span></span>
<span id="cb4-17"><a aria-hidden="true" href="#cb4-17" tabindex="-1"></a>kv_cache <span class="op">=</span> torch.randn(B, num_blocks, block_size, n_head, d_head)  <span class="co"># KV store</span></span>
<span id="cb4-18"><a aria-hidden="true" href="#cb4-18" tabindex="-1"></a></span>
<span id="cb4-19"><a aria-hidden="true" href="#cb4-19" tabindex="-1"></a><span class="co"># Dummy queries for current chunk</span></span>
<span id="cb4-20"><a aria-hidden="true" href="#cb4-20" tabindex="-1"></a>Q <span class="op">=</span> torch.randn(B, chunk_size, n_head, d_head)</span>
<span id="cb4-21"><a aria-hidden="true" href="#cb4-21" tabindex="-1"></a></span>
<span id="cb4-22"><a aria-hidden="true" href="#cb4-22" tabindex="-1"></a><span class="co"># Prefill processing loop (Chunked Prefill)</span></span>
<span id="cb4-23"><a aria-hidden="true" href="#cb4-23" tabindex="-1"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb4-24"><a aria-hidden="true" href="#cb4-24" tabindex="-1"></a>    <span class="cf">for</span> chunk_start <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, L, chunk_size):</span>
<span id="cb4-25"><a aria-hidden="true" href="#cb4-25" tabindex="-1"></a>        chunk_end <span class="op">=</span> <span class="bu">min</span>(chunk_start <span class="op">+</span> chunk_size, L)</span>
<span id="cb4-26"><a aria-hidden="true" href="#cb4-26" tabindex="-1"></a>        curr_chunk_len <span class="op">=</span> chunk_end <span class="op">-</span> chunk_start</span>
<span id="cb4-27"><a aria-hidden="true" href="#cb4-27" tabindex="-1"></a>        q <span class="op">=</span> Q[b, :curr_chunk_len]  <span class="co"># [chunk_len, n_head, d_head]</span></span>
<span id="cb4-28"><a aria-hidden="true" href="#cb4-28" tabindex="-1"></a></span>
<span id="cb4-29"><a aria-hidden="true" href="#cb4-29" tabindex="-1"></a>        <span class="co"># Accumulate attention outputs</span></span>
<span id="cb4-30"><a aria-hidden="true" href="#cb4-30" tabindex="-1"></a>        attn_out <span class="op">=</span> []</span>
<span id="cb4-31"><a aria-hidden="true" href="#cb4-31" tabindex="-1"></a></span>
<span id="cb4-32"><a aria-hidden="true" href="#cb4-32" tabindex="-1"></a>        <span class="co"># Compute attention over previous tokens using Paged Attention blocks</span></span>
<span id="cb4-33"><a aria-hidden="true" href="#cb4-33" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(chunk_start, chunk_end):</span>
<span id="cb4-34"><a aria-hidden="true" href="#cb4-34" tabindex="-1"></a>            <span class="co"># For query at position t, get all keys from positions [0, t]</span></span>
<span id="cb4-35"><a aria-hidden="true" href="#cb4-35" tabindex="-1"></a>            t_block_idx <span class="op">=</span> t <span class="op">//</span> block_size</span>
<span id="cb4-36"><a aria-hidden="true" href="#cb4-36" tabindex="-1"></a></span>
<span id="cb4-37"><a aria-hidden="true" href="#cb4-37" tabindex="-1"></a>            context_keys <span class="op">=</span> []</span>
<span id="cb4-38"><a aria-hidden="true" href="#cb4-38" tabindex="-1"></a>            context_values <span class="op">=</span> []</span>
<span id="cb4-39"><a aria-hidden="true" href="#cb4-39" tabindex="-1"></a></span>
<span id="cb4-40"><a aria-hidden="true" href="#cb4-40" tabindex="-1"></a>            <span class="co"># Page in required blocks</span></span>
<span id="cb4-41"><a aria-hidden="true" href="#cb4-41" tabindex="-1"></a>            <span class="cf">for</span> blk_id <span class="kw">in</span> <span class="bu">range</span>(t_block_idx <span class="op">+</span> <span class="dv">1</span>):  <span class="co"># include current block</span></span>
<span id="cb4-42"><a aria-hidden="true" href="#cb4-42" tabindex="-1"></a>                physical_block <span class="op">=</span> block_table[b, blk_id]</span>
<span id="cb4-43"><a aria-hidden="true" href="#cb4-43" tabindex="-1"></a>                kv_block <span class="op">=</span> kv_cache[b, physical_block]   <span class="co"># [block_size, n_head, d_head]</span></span>
<span id="cb4-44"><a aria-hidden="true" href="#cb4-44" tabindex="-1"></a>                context_keys.append(kv_block)</span>
<span id="cb4-45"><a aria-hidden="true" href="#cb4-45" tabindex="-1"></a>                context_values.append(kv_block)  <span class="co"># simplified: keys = values</span></span>
<span id="cb4-46"><a aria-hidden="true" href="#cb4-46" tabindex="-1"></a></span>
<span id="cb4-47"><a aria-hidden="true" href="#cb4-47" tabindex="-1"></a>            <span class="co"># Flatten context into a single tensor [T', n_head, d_head]</span></span>
<span id="cb4-48"><a aria-hidden="true" href="#cb4-48" tabindex="-1"></a>            K <span class="op">=</span> torch.cat(context_keys, dim<span class="op">=</span><span class="dv">0</span>)[:t<span class="op">+</span><span class="dv">1</span>]  <span class="co"># causal</span></span>
<span id="cb4-49"><a aria-hidden="true" href="#cb4-49" tabindex="-1"></a>            V <span class="op">=</span> torch.cat(context_values, dim<span class="op">=</span><span class="dv">0</span>)[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb4-50"><a aria-hidden="true" href="#cb4-50" tabindex="-1"></a></span>
<span id="cb4-51"><a aria-hidden="true" href="#cb4-51" tabindex="-1"></a>            q_t <span class="op">=</span> q[t <span class="op">-</span> chunk_start]  <span class="co"># query vector at position t</span></span>
<span id="cb4-52"><a aria-hidden="true" href="#cb4-52" tabindex="-1"></a></span>
<span id="cb4-53"><a aria-hidden="true" href="#cb4-53" tabindex="-1"></a>            <span class="co"># Scaled dot-product attention</span></span>
<span id="cb4-54"><a aria-hidden="true" href="#cb4-54" tabindex="-1"></a>            scores <span class="op">=</span> torch.einsum(<span class="st">"hd,thd-&gt;t"</span>, q_t, K) <span class="op">/</span> (d_head <span class="op">**</span> <span class="fl">0.5</span>)</span>
<span id="cb4-55"><a aria-hidden="true" href="#cb4-55" tabindex="-1"></a>            probs <span class="op">=</span> torch.softmax(scores, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-56"><a aria-hidden="true" href="#cb4-56" tabindex="-1"></a>            ctx <span class="op">=</span> torch.einsum(<span class="st">"t,thd-&gt;hd"</span>, probs, V)</span>
<span id="cb4-57"><a aria-hidden="true" href="#cb4-57" tabindex="-1"></a></span>
<span id="cb4-58"><a aria-hidden="true" href="#cb4-58" tabindex="-1"></a>            attn_out.append(ctx)</span>
<span id="cb4-59"><a aria-hidden="true" href="#cb4-59" tabindex="-1"></a></span>
<span id="cb4-60"><a aria-hidden="true" href="#cb4-60" tabindex="-1"></a>        <span class="co"># Stack outputs: [chunk_len, n_head, d_head]</span></span>
<span id="cb4-61"><a aria-hidden="true" href="#cb4-61" tabindex="-1"></a>        attn_out <span class="op">=</span> torch.stack(attn_out, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-62"><a aria-hidden="true" href="#cb4-62" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Chunk [</span><span class="sc">{</span>chunk_start<span class="sc">}</span><span class="ss">:</span><span class="sc">{</span>chunk_end<span class="sc">}</span><span class="ss">] attention output shape:"</span>, attn_out.shape)</span></code></pre></div>
<h3 id="operational-flow">Operational Flow</h3>
<ul>
<li><p>The block_table holds logical-to-physical mappings of KV blocks
for each sequence in the batch. Each block can store
<code>block_size</code> tokens.</p></li>
<li><p>The KV cache (kv_cache) is allocated as <span class="math inline">[<em>B</em>, num_blocks, block_size, <em>n_head</em>, <em>d_head</em>]</span>.</p></li>
<li><p>For each chunk (driven by <code>chunk_size</code>), queries are
processed in a causal way: each query <span class="math inline"><em>t</em></span> attends to all previous tokens
(positions 0 to <span class="math inline"><em>t</em></span>), using the
paged blocks.</p></li>
<li><p>Only the relevant blocks (from block 0 up to block <span class="math inline">⌊<em>t</em>/block_size⌋</span>) are paged
in.</p></li>
<li><p>Attention is computed with the standard scaled dot-product
mechanism using concatenated keys and values.</p></li>
<li><p><strong>Paged Attention</strong> ensures that the key-value cache
is organized into fixed-size <strong>blocks</strong>. Each token’s query
accesses the minimum required set of blocks based on its position <span class="math inline"><em>t</em></span>. This reduces persistent memory
footprint and supports dynamic allocation.</p></li>
<li><p><strong>Chunked Prefill</strong> ensures that the input sequences
are processed in <strong>chunks</strong> of size <span class="math inline">6</span>. For each chunk, attention scores are
computed incrementally over all previously processed tokens, including
those in earlier chunks. This limits peak memory usage during the
prefill phase.</p></li>
<li><p><strong>Dynamic Paging</strong> is setup such that for each token
<span class="math inline"><em>t</em></span> in a chunk, only the blocks
containing keys/values from positions <span class="math inline">0</span>
to <span class="math inline"><em>t</em></span> are loaded, ensuring
efficient memory utilization.</p></li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p><strong>Paged Attention</strong> and <strong>Chunked Prefill</strong>
are orthogonal strategies for optimizing LLM inference in
memory-constrained environments. Paged Attention reduces persistent
memory consumption during attention score computation by dynamically
paging key-value blocks. Chunked Prefill mitigates peak memory usage
during the prefill phase by processing input sequences in chunks. There
are also exploration around kernel-level optimizations, fused attention
score computation across blocks, and dynamic eviction policies for the
block table to further improve inference throughput under multi-user,
long-sequence workloads.</p>
<h3 id="references">References</h3>
<section class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p>vLLM Paged Attention — vLLM.
https://docs.vllm.ai/en/latest/design/kernel/paged_attention.html#vllm-paged-attention.
Accessed 27 Mar. 2025.<a class="footnote-back" href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Optimization and Tuning — vLLM.
https://docs.vllm.ai/en/latest/performance/optimization.html#chunked-prefill.
Accessed 27 Mar. 2025.<a class="footnote-back" href="#fnref2" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
    </div>
    <hr/>
    <p>If you found this useful, please cite this post using</p>
    <blockquote class="blockquote-citation">
        <p>Senthilkumar Gopal. (Dec 2024). Paged Attention and Chunked Prefill for LLM Inference. sengopal.me. https://sengopal.me/posts/paged-attention-and-chunked-prefill-for-llm-inference</p>
    </blockquote>
    <p>or</p>
    <div class="citation">
        <pre class="citation">@article{gopal2024pagedattentionandchunkedprefillforllminference,
  title   = {Paged Attention and Chunked Prefill for LLM Inference},
  author  = {Senthilkumar Gopal},
  journal = {sengopal.me},
  year    = {2024},
  month   = {Dec},
  url     = {https://sengopal.me/posts/paged-attention-and-chunked-prefill-for-llm-inference}
}</pre>
    </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
    <ul class="col-sm-6 list-inline">
        <li class="list-inline-item"><a href="https://sengopal.me/archives">Archives</a></li>
        <li class="list-inline-item"><a href="https://sengopal.me/categories">Categories</a></li>
        <li class="list-inline-item"><a href="https://sengopal.me/tags">Tags</a></li>
    </ul>
    <p class="col-sm-6 text-sm-right text-muted">
        Opinions my own. Made with &#x2764; using <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a> / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>        
    </p>
</div>    </div>
  </footer>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" crossorigin="anonymous"></script>
</body>

</html>