<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Neuron Glossary | Senthilkumar Gopal
</title>
  <link rel="canonical" href="https://sengopal.me/posts/neuron-glossary.html">

    <link rel="apple-touch-icon" href="https://sengopal.me/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="https://sengopal.me/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://sengopal.me/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="https://sengopal.me/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="https://sengopal.me/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/all.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/theme.css">
  <link rel="stylesheet" href="https://sengopal.me/extras/css/skylighting-solarized-theme.css">
  <link rel="stylesheet" href="https://sengopal.me/extras/css/custom.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://sengopal.me/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://sengopal.me/feeds/neuron.atom.xml">
<meta name="description" content="This post acts as a running glossary for Neuron and HPC related terms and technologies.">

<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o);
      a.async = 1;
      a.src = g;
      m = s.getElementsByTagName(o)[0];
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-67843911-1', 'auto');
    ga('send', 'pageview');
  </script>


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="https://sengopal.me/">Senthilkumar Gopal</a></h1>
      <p class="text-muted">Musings of a machine learning researcher, engineer and leader</p>
      <ul class="list-inline">
            <li class="list-inline-item"><a href="https://sengopal.me/pages/about.html">About me</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/publications.html">Publications</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/software.html">Software</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/talks.html">Talks</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fas fa-rss" href="https://sengopal.me/feeds/all.atom.xml" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-github" href="https://github.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-speaker-deck" href="https://speakerdeck.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-google-scholar" href="https://scholar.google.com/citations?user=bs8WraEAAAAJ&hl=en" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-medium" href="https://medium.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-twitter" href="https://twitter.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-linkedin" href="https://www.linkedin.com/in/senthilkumargopal" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
<article class="article">
    <header>
        <ul class="list-inline">
            <li class="list-inline-item text-muted" title="2023-12-13T00:00:00-08:00">
                <i class="fas fa-clock"></i>
                Wed 13 December 2023
            </li>
            <li class="list-inline-item">
                <i class="fas fa-folder-open"></i>
                <a href="https://sengopal.me/category/neuron">Neuron</a>
            </li>
            <li class="list-inline-item">
                <i class="fas fa-tag"></i>
                <a href="https://sengopal.me/tag/ml-code">#ml-code</a>,                 <a href="https://sengopal.me/tag/llm">#llm</a>            </li>
        </ul>
    </header>
    <h1>Neuron Glossary</h1>
    <div class="hidden-xs hidden-sm">
        None
    </div>
    <div class="content">
        <p>Last Updated on <em>Mar 12, 2024</em></p>
<p>The following are a running list of new terms that are encountered
while working on the Neuron stack. This acts as a jump off point into
deeper dives into these terms and context behind them</p>
<p><strong>FPGA (Field-Programmable Gate Array)</strong> Unlike GPUs
with a fixed design, FPGAs are essentially blank slates. They contain a
fabric of programmable logic blocks that can be configured to perform
specific tasks. This flexibility allows FPGAs to be customized for a
wide range of applications, including cryptography, financial modeling,
and high-frequency trading.</p>
<p><strong>ASIC</strong> An ASIC is a chip designed for a specific
purpose. It offers high performance and efficiency for that particular
task because the hardware is optimized for it. This aligns exactly with
how Intel describes the Gaudi as a deep learning accelerator.</p>
<p><strong>Arithmetic Intensity</strong> Metric that quantifies the
ratio of computational operations (measured in floating-point
operations, or FLOPs) to data movement (measured in bytes) during a
computation. It helps determine whether a particular operation is
compute-bound or memory-bound. For example, applying the ReLU activation
function to a tensor involves reading 2 bytes, performing 1 comparison
operation, and writing 2 bytes per element, resulting in an arithmetic
intensity of 1 FLOP per 4 bytes accessed. This low ratio indicates that
such operations are typically memory-bound, meaning the time spent on
memory accesses exceeds the time spent on computations. <a href="https://astralord.github.io/posts/transformer-inference-optimization-toolset/">Ref</a></p>
<p><strong>Gradient checkpointing /activation checkpointing</strong> A
technique to reduce memory usage by clearing activations of certain
layers and recomputing them during a backward pass. Effectively, this
trades extra computation time for reduced memory usage. If a module is
checkpointed, at the end of a forward pass, the inputs to and outputs
from the module stay in memory.<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-activation-checkpointing.html">ref</a></p>
<p><strong>Strength Reduction</strong><br/>
This optimization replaces computationally expensive operations with
equivalent but less costly ones. For example, replacing multiplication
by a constant with a shift operation.</p>
<p><strong>Constant Folding</strong><br/>
Evaluates constant expressions at compile time, replacing them with
their computed values to reduce runtime computation.</p>
<p><strong>Common Subexpression Elimination</strong><br/>
Identifies and eliminates duplicate calculations by reusing previously
computed values, enhancing efficiency.</p>
<p><strong>Dead-Code Elimination</strong><br/>
Removes code that does not affect the programâ€™s outcome, such as
computations whose results are never used, thereby streamlining the
codebase.</p>
<p><strong>Scalar Replacement</strong><br/>
Replaces array references with scalar variables when possible, reducing
memory access overhead and improving performance.</p>
<p><strong>If-Conversion</strong><br/>
Transforms conditional branches into conditional instructions,
minimizing branch penalties and enhancing instruction-level
parallelism.</p>
<p><strong>Function Inlining</strong><br/>
Substitutes the body of a called function directly into the calling
code, eliminating call overhead and enabling further optimizations.</p>
<p><strong>Call Specialization</strong><br/>
Tailors function calls based on known arguments, creating specialized
versions of functions to improve performance.</p>
<p><strong>Peephole Optimizations</strong><br/>
Examines a small window of consecutive instructions to identify and
replace inefficient sequences with more efficient ones, enhancing code
quality.</p>
<p><strong>Canonicalizing Loops</strong><br/>
Transforms loops to start at index 0 and stride by 1, which simplifies
the loop structure and may enable more optimizations.</p>
<p><strong>Generating Constants</strong><br/>
Replaces index calculations with pre-computed constants, reducing the
need for computation during program execution.</p>
<p><strong>Eliminating Load/Store Pairs</strong><br/>
Removes redundant load and store operations by combining them or
eliminating unnecessary memory accesses.</p>
<p><strong>XLA (Accelerated Linear Algebra)</strong><br/>
XLA is a domain-specific compiler for linear algebra that optimizes
computations for machine learning models. It is particularly focused on
optimizing TensorFlow computations and improving performance on hardware
accelerators like GPUs and TPUs.</p>
<p><strong>TVM</strong><br/>
TVM is an open-source deep learning compiler stack designed to optimize
the performance of deep learning models across different hardware
platforms. It provides a flexible and efficient way to deploy models on
a wide range of devices, including CPUs, GPUs, and specialized
accelerators.</p>
<p><strong>MLIR (Multi-Level Intermediate Representation)</strong><br/>
MLIR is an intermediate representation used to define and optimize
programs at multiple levels of abstraction. It is designed to facilitate
cross-compiler optimizations and improve the portability and efficiency
of code across different hardware architectures.</p>
<p><strong>LLVM / GCC</strong> LLVM is a modular and flexible compiler
infrastructure consisting of reusable components that provide
fine-grained control over code generation and optimization. Its
intermediate representation (IR) is portable and efficient, allowing it
to target various hardware architectures. In contrast, GCC is a
monolithic compiler system that integrates the compiler, assembler,
linker, and debugger, following a more traditional approach to code
generation and optimizations with a long history of use in production
environments. While LLVMâ€™s modularity allows for greater extensibility
and customization, GCC tends to be less modular, requiring deeper
integration for adding features. LLVM excels in modern hardware
optimizations, especially for GPUs and specialized accelerators, whereas
GCC is more focused on traditional CPU-based platforms and embedded
systems.</p>
<p><strong>PCIe</strong> high-speed interface standard used to connect
peripheral devices, such as graphics cards, network cards, and storage
devices, to the CPU and memory. It provides fast, low-latency data
transfer with a scalable architecture, offering multiple lanes for
simultaneous data communication. PCIe operates with a point-to-point
architecture, where each device communicates directly with the CPU
through a dedicated link, ensuring high performance and low overhead.
Its speed and flexibility make it ideal for modern hardware
accelerators, like GPUs and NVMe storage devices.</p>
<p><strong>Memory-Mapped I/O (MMIO)</strong> technique where I/O devices
are mapped to specific memory addresses, allowing the CPU to communicate
with them as if they were part of the systemâ€™s memory. This method
provides a simple and efficient way to read from and write to I/O
devices using standard memory access instructions, without the need for
special I/O instructions. MMIO allows devices such as graphics cards,
network interfaces, and other peripherals to interact directly with the
processor and memory, enabling faster and more efficient data transfers,
especially when working with high-performance devices like GPUs or
custom accelerators.</p>
<p><strong>Hypervisors</strong> Hypervisors are software, firmware, or
hardware components that enable virtualization by managing the creation
and execution of virtual machines. Type 1 (bare-metal) run directly on
the hardware, providing high performance and better isolation for VMs.
Examples include VMware ESXi, Microsoft Hyper-V, and Xen. Type 2
(hosted) run on top of a host operating system, with the hypervisor
providing virtualized hardware resources to guest operating systems.
Examples include VMware Workstation and Oracle VirtualBox.</p>
<p><strong>SR-IOV (Single Root I/O Virtualization)</strong> allows a
single physical network interface card (NIC) or other I/O devices to
appear as multiple separate virtual devices to virtual machines. It
improves performance by allowing VMs to access I/O resources directly
without the need for extensive virtualization overhead. SR-IOV enables
better scalability and efficiency by allowing multiple VMs to share a
single physical device while maintaining near-native performance. In
SR-IOV-enabled systems, the hypervisor configures the physical device
(e.g., NIC, GPU) to expose virtual interfaces, which are then directly
assigned to virtual machines.</p>
    </div>
    <hr/>
    <p>If you found this useful, please cite this post using</p>
    <blockquote class="blockquote-citation">
        <p>Senthilkumar Gopal. (Dec 2023). Neuron Glossary. sengopal.me. https://sengopal.me/posts/neuron-glossary</p>
    </blockquote>
    <p>or</p>
    <div class="citation">
        <pre class="citation">@article{gopal2023neuronglossary,
  title   = {Neuron Glossary},
  author  = {Senthilkumar Gopal},
  journal = {sengopal.me},
  year    = {2023},
  month   = {Dec},
  url     = {https://sengopal.me/posts/neuron-glossary}
}</pre>
    </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
    <ul class="col-sm-6 list-inline">
        <li class="list-inline-item"><a href="https://sengopal.me/archives">Archives</a></li>
        <li class="list-inline-item"><a href="https://sengopal.me/categories">Categories</a></li>
        <li class="list-inline-item"><a href="https://sengopal.me/tags">Tags</a></li>
    </ul>
    <p class="col-sm-6 text-sm-right text-muted">
        Opinions my own. Made with &#x2764; using <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a> / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>        
    </p>
</div>    </div>
  </footer>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" crossorigin="anonymous"></script>
</body>

</html>