<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Aliasing on XLA | Senthilkumar Gopal
</title>
  <link rel="canonical" href="https://sengopal.me/posts/aliasing-on-xla.html">

    <link rel="apple-touch-icon" href="https://sengopal.me/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="https://sengopal.me/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://sengopal.me/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="https://sengopal.me/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="https://sengopal.me/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/all.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="https://sengopal.me/theme/css/theme.css">
  <link rel="stylesheet" href="https://sengopal.me/extras/css/skylighting-solarized-theme.css">
  <link rel="stylesheet" href="https://sengopal.me/extras/css/custom.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="https://sengopal.me/feeds/all.atom.xml">
  <link rel="alternate" type="application/atom+xml" title="Categories Atom Feed"
        href="https://sengopal.me/feeds/ml-systems.atom.xml">
<meta name="description" content="This post explores the concept of aliasing in XLA, its significance, the mechanisms through which it is implemented, and future directions for extending aliasing optimizations.">

<script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

  <script>
    (function(i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function() {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o);
      a.async = 1;
      a.src = g;
      m = s.getElementsByTagName(o)[0];
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
    ga('create', 'UA-67843911-1', 'auto');
    ga('send', 'pageview');
  </script>


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="https://sengopal.me/">Senthilkumar Gopal</a></h1>
      <p class="text-muted">Musings of a machine learning researcher, engineer and leader</p>
      <ul class="list-inline">
            <li class="list-inline-item"><a href="https://sengopal.me/pages/about.html">About me</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/publications.html">Publications</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/software.html">Software</a></li>
            <li class="list-inline-item"><a href="https://sengopal.me/pages/talks.html">Talks</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fas fa-rss" href="https://sengopal.me/feeds/all.atom.xml" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-github" href="https://github.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-speaker-deck" href="https://speakerdeck.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-google-scholar" href="https://scholar.google.com/citations?user=bs8WraEAAAAJ&hl=en" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-medium" href="https://medium.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-twitter" href="https://twitter.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-linkedin" href="https://www.linkedin.com/in/senthilkumargopal" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
<article class="article">
    <header>
        <ul class="list-inline">
            <li class="list-inline-item text-muted" title="2023-12-20T00:00:00-08:00">
                <i class="fas fa-clock"></i>
                Wed 20 December 2023
            </li>
            <li class="list-inline-item">
                <i class="fas fa-folder-open"></i>
                <a href="https://sengopal.me/category/ml-systems">ML Systems</a>
            </li>
            <li class="list-inline-item">
                <i class="fas fa-tag"></i>
                <a href="https://sengopal.me/tag/ml-code">#ml-code</a>,                 <a href="https://sengopal.me/tag/llm">#llm</a>,                 <a href="https://sengopal.me/tag/ml-acceleration">#ml-acceleration</a>,                 <a href="https://sengopal.me/tag/hpc-concept">#hpc-concept</a>            </li>
        </ul>
    </header>
    <h1>Aliasing on XLA</h1>
    <div class="hidden-xs hidden-sm">
        <nav class="toc" role="doc-toc">
<ul>
<li><a href="#what-is-aliasing" id="toc-what-is-aliasing">What is
Aliasing</a></li>
<li><a href="#how-xla-supports-aliasing" id="toc-how-xla-supports-aliasing">How XLA Supports Aliasing</a></li>
<li><a href="#dynamic-shape-handling-on-aliasing" id="toc-dynamic-shape-handling-on-aliasing">Dynamic Shape Handling on
Aliasing</a></li>
<li><a href="#safe-aliasing-with-dynamic-shapes" id="toc-safe-aliasing-with-dynamic-shapes">Safe Aliasing with Dynamic
Shapes</a></li>
<li><a href="#aliasing-optimization-to-cross-computation-buffer-reuse" id="toc-aliasing-optimization-to-cross-computation-buffer-reuse">Aliasing
Optimization to Cross-Computation Buffer Reuse</a></li>
<li><a href="#trade-offs-between-compile-time-and-runtime-aliasing" id="toc-trade-offs-between-compile-time-and-runtime-aliasing">Trade-offs
Between Compile-Time and Runtime Aliasing</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</nav>
    </div>
    <div class="content">
        <p>Modern machine learning models demand substantial memory, especially
during training and inference. To address this challenge, compilers like
XLA (Accelerated Linear Algebra) implement memory optimizations such as
aliasing. Aliasing allows different parts of a computation to share the
same underlying memory buffer, thereby reducing memory footprint and
improving execution performance.</p>
<h2 id="what-is-aliasing">What is Aliasing</h2>
<p>Aliasing in XLA refers to mapping multiple logical buffers onto the
same physical memory region. In practice, this means that input and
output tensors in a computation can share the same memory if certain
conditions are met. By setting up such controlled aliases, XLA can avoid
allocating separate memory for intermediate results, resulting in
reduced memory usage and faster data access.</p>
<p>However, establishing aliases must be managed carefully to maintain
computational correctness. If two aliases modify the same memory
simultaneously without proper sequencing, it can lead to corrupted
results. Consequently, XLA enforces strict constraints based on shape
compatibility, computation dependencies, and memory layouts, as outlined
in the design of the XLA compiler<a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<h2 id="how-xla-supports-aliasing">How XLA Supports Aliasing</h2>
<p>XLA supports aliasing primarily through the
<code>OptimizeInputOutputBufferAlias</code> class. This optimization
pass attempts to reuse input buffers for output buffers wherever
possible while respecting correctness guarantees. The following
annotated code illustrates the core flow:</p>
<p>_Ref: <a href="https://github.com/openxla/xla/blob/main/xla/hlo/transforms/simplifiers/optimize_input_output_buffer_alias.cc">optimize_input_output_buffer_alias.cc</a>
with annotations</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="kw">namespace</span> xla <span class="op">{</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a aria-hidden="true" href="#cb1-3" tabindex="-1"></a><span class="co">// Attempts to establish aliases between input and output buffers.</span></span>
<span id="cb1-4"><a aria-hidden="true" href="#cb1-4" tabindex="-1"></a>absl<span class="op">::</span>StatusOr<span class="op">&lt;</span><span class="dt">bool</span><span class="op">&gt;</span> OptimizeInputOutputBufferAlias<span class="op">::</span>Build<span class="op">(</span></span>
<span id="cb1-5"><a aria-hidden="true" href="#cb1-5" tabindex="-1"></a>    absl<span class="op">::</span>Span<span class="op">&lt;</span><span class="at">const</span> Shape<span class="op">&gt;</span> input_shapes<span class="op">,</span> </span>
<span id="cb1-6"><a aria-hidden="true" href="#cb1-6" tabindex="-1"></a>    <span class="at">const</span> Shape<span class="op">&amp;</span> output_shape<span class="op">,</span></span>
<span id="cb1-7"><a aria-hidden="true" href="#cb1-7" tabindex="-1"></a>    HloInputOutputAliasConfig<span class="op">*</span> alias_config<span class="op">,</span></span>
<span id="cb1-8"><a aria-hidden="true" href="#cb1-8" tabindex="-1"></a>    HloBufferDonorConfig<span class="op">*</span> buffer_donor_config<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-9"><a aria-hidden="true" href="#cb1-9" tabindex="-1"></a>  </span>
<span id="cb1-10"><a aria-hidden="true" href="#cb1-10" tabindex="-1"></a>  <span class="dt">bool</span> changed <span class="op">=</span> <span class="kw">false</span><span class="op">;</span></span>
<span id="cb1-11"><a aria-hidden="true" href="#cb1-11" tabindex="-1"></a>  </span>
<span id="cb1-12"><a aria-hidden="true" href="#cb1-12" tabindex="-1"></a>  <span class="co">// Disable aliasing if the output has a dynamic shape.</span></span>
<span id="cb1-13"><a aria-hidden="true" href="#cb1-13" tabindex="-1"></a>  <span class="cf">if</span> <span class="op">(</span>output_shape<span class="op">.</span>is_dynamic<span class="op">())</span> <span class="op">{</span></span>
<span id="cb1-14"><a aria-hidden="true" href="#cb1-14" tabindex="-1"></a>    <span class="cf">return</span> <span class="kw">false</span><span class="op">;</span></span>
<span id="cb1-15"><a aria-hidden="true" href="#cb1-15" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb1-16"><a aria-hidden="true" href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a aria-hidden="true" href="#cb1-17" tabindex="-1"></a>  <span class="co">// Structures to collect potential donor input buffers.</span></span>
<span id="cb1-18"><a aria-hidden="true" href="#cb1-18" tabindex="-1"></a>  <span class="kw">struct</span> DonorEntry <span class="op">{</span></span>
<span id="cb1-19"><a aria-hidden="true" href="#cb1-19" tabindex="-1"></a>    <span class="dt">int64_t</span> param_number<span class="op">;</span></span>
<span id="cb1-20"><a aria-hidden="true" href="#cb1-20" tabindex="-1"></a>    ShapeIndex index<span class="op">;</span></span>
<span id="cb1-21"><a aria-hidden="true" href="#cb1-21" tabindex="-1"></a>    <span class="dt">int64_t</span> shape_size<span class="op">;</span></span>
<span id="cb1-22"><a aria-hidden="true" href="#cb1-22" tabindex="-1"></a>  <span class="op">};</span></span>
<span id="cb1-23"><a aria-hidden="true" href="#cb1-23" tabindex="-1"></a>  absl<span class="op">::</span>flat_hash_map<span class="op">&lt;</span><span class="dt">int64_t</span><span class="op">,</span> <span class="bu">std::</span>vector<span class="op">&lt;</span>DonorEntry<span class="op">&gt;&gt;</span> donors<span class="op">;</span></span>
<span id="cb1-24"><a aria-hidden="true" href="#cb1-24" tabindex="-1"></a></span>
<span id="cb1-25"><a aria-hidden="true" href="#cb1-25" tabindex="-1"></a>  <span class="co">// Populate donors: traverse each input's subshapes and collect eligible buffers.</span></span>
<span id="cb1-26"><a aria-hidden="true" href="#cb1-26" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int64_t</span> param_number <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> param_number <span class="op">&lt;</span> input_shapes<span class="op">.</span>size<span class="op">();</span> <span class="op">++</span>param_number<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-27"><a aria-hidden="true" href="#cb1-27" tabindex="-1"></a>    <span class="at">const</span> Shape<span class="op">&amp;</span> input_shape <span class="op">=</span> input_shapes<span class="op">[</span>param_number<span class="op">];</span></span>
<span id="cb1-28"><a aria-hidden="true" href="#cb1-28" tabindex="-1"></a>    TF_RET_CHECK<span class="op">(</span>LayoutUtil<span class="op">::</span>HasLayout<span class="op">(</span>input_shape<span class="op">));</span></span>
<span id="cb1-29"><a aria-hidden="true" href="#cb1-29" tabindex="-1"></a>    ShapeUtil<span class="op">::</span>ForEachSubshape<span class="op">(</span>input_shape<span class="op">,</span> <span class="op">[&amp;](</span><span class="at">const</span> Shape<span class="op">&amp;</span> subshape<span class="op">,</span> <span class="at">const</span> ShapeIndex<span class="op">&amp;</span> index<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-30"><a aria-hidden="true" href="#cb1-30" tabindex="-1"></a>      <span class="cf">if</span> <span class="op">(!</span>LayoutUtil<span class="op">::</span>IsDenseArray<span class="op">(</span>subshape<span class="op">)</span> <span class="op">||</span> subshape<span class="op">.</span>is_dynamic<span class="op">())</span> <span class="op">{</span></span>
<span id="cb1-31"><a aria-hidden="true" href="#cb1-31" tabindex="-1"></a>        <span class="cf">return</span><span class="op">;</span></span>
<span id="cb1-32"><a aria-hidden="true" href="#cb1-32" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb1-33"><a aria-hidden="true" href="#cb1-33" tabindex="-1"></a>      <span class="cf">if</span> <span class="op">(</span>alias_config<span class="op">-&gt;</span>ParameterHasAlias<span class="op">(</span>param_number<span class="op">,</span> index<span class="op">))</span> <span class="op">{</span></span>
<span id="cb1-34"><a aria-hidden="true" href="#cb1-34" tabindex="-1"></a>        <span class="cf">return</span><span class="op">;</span></span>
<span id="cb1-35"><a aria-hidden="true" href="#cb1-35" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb1-36"><a aria-hidden="true" href="#cb1-36" tabindex="-1"></a>      <span class="cf">if</span> <span class="op">(</span><span class="va">registered_buffer_donor_only_</span> <span class="op">&amp;&amp;</span></span>
<span id="cb1-37"><a aria-hidden="true" href="#cb1-37" tabindex="-1"></a>          <span class="op">!</span>buffer_donor_config<span class="op">-&gt;</span>ParameterIsBufferDonor<span class="op">(</span>param_number<span class="op">,</span> index<span class="op">))</span> <span class="op">{</span></span>
<span id="cb1-38"><a aria-hidden="true" href="#cb1-38" tabindex="-1"></a>        <span class="cf">return</span><span class="op">;</span></span>
<span id="cb1-39"><a aria-hidden="true" href="#cb1-39" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb1-40"><a aria-hidden="true" href="#cb1-40" tabindex="-1"></a>      <span class="dt">int64_t</span> memory_space <span class="op">=</span> subshape<span class="op">.</span>layout<span class="op">().</span>memory_space<span class="op">();</span></span>
<span id="cb1-41"><a aria-hidden="true" href="#cb1-41" tabindex="-1"></a>      donors<span class="op">[</span>memory_space<span class="op">].</span>emplace_back<span class="op">(</span></span>
<span id="cb1-42"><a aria-hidden="true" href="#cb1-42" tabindex="-1"></a>          DonorEntry<span class="op">{</span>param_number<span class="op">,</span> index<span class="op">,</span> <span class="va">shape_size_fn_</span><span class="op">(</span>subshape<span class="op">)});</span></span>
<span id="cb1-43"><a aria-hidden="true" href="#cb1-43" tabindex="-1"></a>    <span class="op">});</span></span>
<span id="cb1-44"><a aria-hidden="true" href="#cb1-44" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb1-45"><a aria-hidden="true" href="#cb1-45" tabindex="-1"></a></span>
<span id="cb1-46"><a aria-hidden="true" href="#cb1-46" tabindex="-1"></a>  <span class="co">// Structures to collect potential donee output buffers.</span></span>
<span id="cb1-47"><a aria-hidden="true" href="#cb1-47" tabindex="-1"></a>  <span class="kw">struct</span> DoneeEntry <span class="op">{</span></span>
<span id="cb1-48"><a aria-hidden="true" href="#cb1-48" tabindex="-1"></a>    ShapeIndex index<span class="op">;</span></span>
<span id="cb1-49"><a aria-hidden="true" href="#cb1-49" tabindex="-1"></a>    <span class="dt">int64_t</span> shape_size<span class="op">;</span></span>
<span id="cb1-50"><a aria-hidden="true" href="#cb1-50" tabindex="-1"></a>  <span class="op">};</span></span>
<span id="cb1-51"><a aria-hidden="true" href="#cb1-51" tabindex="-1"></a>  absl<span class="op">::</span>flat_hash_map<span class="op">&lt;</span><span class="dt">int64_t</span><span class="op">,</span> <span class="bu">std::</span>vector<span class="op">&lt;</span>DoneeEntry<span class="op">&gt;&gt;</span> donees<span class="op">;</span></span>
<span id="cb1-52"><a aria-hidden="true" href="#cb1-52" tabindex="-1"></a></span>
<span id="cb1-53"><a aria-hidden="true" href="#cb1-53" tabindex="-1"></a>  TF_RET_CHECK<span class="op">(</span>LayoutUtil<span class="op">::</span>HasLayout<span class="op">(</span>output_shape<span class="op">));</span></span>
<span id="cb1-54"><a aria-hidden="true" href="#cb1-54" tabindex="-1"></a>  ShapeUtil<span class="op">::</span>ForEachSubshape<span class="op">(</span>output_shape<span class="op">,</span> <span class="op">[&amp;](</span><span class="at">const</span> Shape<span class="op">&amp;</span> subshape<span class="op">,</span> <span class="at">const</span> ShapeIndex<span class="op">&amp;</span> index<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-55"><a aria-hidden="true" href="#cb1-55" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(!</span>LayoutUtil<span class="op">::</span>IsDenseArray<span class="op">(</span>subshape<span class="op">))</span> <span class="op">{</span></span>
<span id="cb1-56"><a aria-hidden="true" href="#cb1-56" tabindex="-1"></a>      <span class="cf">return</span><span class="op">;</span></span>
<span id="cb1-57"><a aria-hidden="true" href="#cb1-57" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb1-58"><a aria-hidden="true" href="#cb1-58" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>alias_config<span class="op">-&gt;</span>OutputHasAlias<span class="op">(</span>index<span class="op">))</span> <span class="op">{</span></span>
<span id="cb1-59"><a aria-hidden="true" href="#cb1-59" tabindex="-1"></a>      <span class="cf">return</span><span class="op">;</span></span>
<span id="cb1-60"><a aria-hidden="true" href="#cb1-60" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb1-61"><a aria-hidden="true" href="#cb1-61" tabindex="-1"></a>    <span class="dt">int64_t</span> memory_space <span class="op">=</span> subshape<span class="op">.</span>layout<span class="op">().</span>memory_space<span class="op">();</span></span>
<span id="cb1-62"><a aria-hidden="true" href="#cb1-62" tabindex="-1"></a>    donees<span class="op">[</span>memory_space<span class="op">].</span>emplace_back<span class="op">(</span></span>
<span id="cb1-63"><a aria-hidden="true" href="#cb1-63" tabindex="-1"></a>        DoneeEntry<span class="op">{</span>index<span class="op">,</span> <span class="va">shape_size_fn_</span><span class="op">(</span>subshape<span class="op">)});</span></span>
<span id="cb1-64"><a aria-hidden="true" href="#cb1-64" tabindex="-1"></a>  <span class="op">});</span></span>
<span id="cb1-65"><a aria-hidden="true" href="#cb1-65" tabindex="-1"></a></span>
<span id="cb1-66"><a aria-hidden="true" href="#cb1-66" tabindex="-1"></a>  <span class="co">// For each memory space, match donors and donees based on shape size.</span></span>
<span id="cb1-67"><a aria-hidden="true" href="#cb1-67" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="kw">auto</span><span class="op">&amp;</span> <span class="op">[</span>memory_space<span class="op">,</span> donor_vector<span class="op">]</span> <span class="op">:</span> donors<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-68"><a aria-hidden="true" href="#cb1-68" tabindex="-1"></a>    <span class="kw">auto</span> donee_it <span class="op">=</span> donees<span class="op">.</span>find<span class="op">(</span>memory_space<span class="op">);</span></span>
<span id="cb1-69"><a aria-hidden="true" href="#cb1-69" tabindex="-1"></a>    <span class="cf">if</span> <span class="op">(</span>donee_it <span class="op">==</span> donees<span class="op">.</span>end<span class="op">())</span> <span class="op">{</span></span>
<span id="cb1-70"><a aria-hidden="true" href="#cb1-70" tabindex="-1"></a>      <span class="cf">continue</span><span class="op">;</span></span>
<span id="cb1-71"><a aria-hidden="true" href="#cb1-71" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb1-72"><a aria-hidden="true" href="#cb1-72" tabindex="-1"></a>    <span class="kw">auto</span><span class="op">&amp;</span> donee_vector <span class="op">=</span> donee_it<span class="op">-&gt;</span>second<span class="op">;</span></span>
<span id="cb1-73"><a aria-hidden="true" href="#cb1-73" tabindex="-1"></a></span>
<span id="cb1-74"><a aria-hidden="true" href="#cb1-74" tabindex="-1"></a>    <span class="co">// Sort both donors and donees by decreasing size to maximize reuse of large buffers.</span></span>
<span id="cb1-75"><a aria-hidden="true" href="#cb1-75" tabindex="-1"></a>    absl<span class="op">::</span>c_stable_sort<span class="op">(</span>donor_vector<span class="op">,</span></span>
<span id="cb1-76"><a aria-hidden="true" href="#cb1-76" tabindex="-1"></a>        <span class="op">[](</span><span class="at">const</span> DonorEntry<span class="op">&amp;</span> a<span class="op">,</span> <span class="at">const</span> DonorEntry<span class="op">&amp;</span> b<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> a<span class="op">.</span>shape_size <span class="op">&gt;</span> b<span class="op">.</span>shape_size<span class="op">;</span> <span class="op">});</span></span>
<span id="cb1-77"><a aria-hidden="true" href="#cb1-77" tabindex="-1"></a>    absl<span class="op">::</span>c_stable_sort<span class="op">(</span>donee_vector<span class="op">,</span></span>
<span id="cb1-78"><a aria-hidden="true" href="#cb1-78" tabindex="-1"></a>        <span class="op">[](</span><span class="at">const</span> DoneeEntry<span class="op">&amp;</span> a<span class="op">,</span> <span class="at">const</span> DoneeEntry<span class="op">&amp;</span> b<span class="op">)</span> <span class="op">{</span> <span class="cf">return</span> a<span class="op">.</span>shape_size <span class="op">&gt;</span> b<span class="op">.</span>shape_size<span class="op">;</span> <span class="op">});</span></span>
<span id="cb1-79"><a aria-hidden="true" href="#cb1-79" tabindex="-1"></a></span>
<span id="cb1-80"><a aria-hidden="true" href="#cb1-80" tabindex="-1"></a>    <span class="co">// Two-pointer matching: match donor and donee of the same size.</span></span>
<span id="cb1-81"><a aria-hidden="true" href="#cb1-81" tabindex="-1"></a>    <span class="dt">int64_t</span> donor_vector_index <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-82"><a aria-hidden="true" href="#cb1-82" tabindex="-1"></a>    <span class="dt">int64_t</span> donee_vector_index <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-83"><a aria-hidden="true" href="#cb1-83" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(</span>donor_vector_index <span class="op">&lt;</span> donor_vector<span class="op">.</span>size<span class="op">()</span> <span class="op">&amp;&amp;</span></span>
<span id="cb1-84"><a aria-hidden="true" href="#cb1-84" tabindex="-1"></a>           donee_vector_index <span class="op">&lt;</span> donee_vector<span class="op">.</span>size<span class="op">())</span> <span class="op">{</span></span>
<span id="cb1-85"><a aria-hidden="true" href="#cb1-85" tabindex="-1"></a>      <span class="at">const</span> <span class="kw">auto</span><span class="op">&amp;</span> donor <span class="op">=</span> donor_vector<span class="op">[</span>donor_vector_index<span class="op">];</span></span>
<span id="cb1-86"><a aria-hidden="true" href="#cb1-86" tabindex="-1"></a>      <span class="at">const</span> <span class="kw">auto</span><span class="op">&amp;</span> donee <span class="op">=</span> donee_vector<span class="op">[</span>donee_vector_index<span class="op">];</span></span>
<span id="cb1-87"><a aria-hidden="true" href="#cb1-87" tabindex="-1"></a>      <span class="cf">if</span> <span class="op">(</span>donor<span class="op">.</span>shape_size <span class="op">&gt;</span> donee<span class="op">.</span>shape_size<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-88"><a aria-hidden="true" href="#cb1-88" tabindex="-1"></a>        donor_vector_index<span class="op">++;</span></span>
<span id="cb1-89"><a aria-hidden="true" href="#cb1-89" tabindex="-1"></a>      <span class="op">}</span> <span class="cf">else</span> <span class="cf">if</span> <span class="op">(</span>donor<span class="op">.</span>shape_size <span class="op">&lt;</span> donee<span class="op">.</span>shape_size<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-90"><a aria-hidden="true" href="#cb1-90" tabindex="-1"></a>        donee_vector_index<span class="op">++;</span></span>
<span id="cb1-91"><a aria-hidden="true" href="#cb1-91" tabindex="-1"></a>      <span class="op">}</span> <span class="cf">else</span> <span class="op">{</span></span>
<span id="cb1-92"><a aria-hidden="true" href="#cb1-92" tabindex="-1"></a>        <span class="co">// Match found: establish alias and remove donor from pool.</span></span>
<span id="cb1-93"><a aria-hidden="true" href="#cb1-93" tabindex="-1"></a>        TF_RETURN_IF_ERROR<span class="op">(</span>alias_config<span class="op">-&gt;</span>SetUpAlias<span class="op">(</span></span>
<span id="cb1-94"><a aria-hidden="true" href="#cb1-94" tabindex="-1"></a>            donee<span class="op">.</span>index<span class="op">,</span> donor<span class="op">.</span>param_number<span class="op">,</span> donor<span class="op">.</span>index<span class="op">));</span></span>
<span id="cb1-95"><a aria-hidden="true" href="#cb1-95" tabindex="-1"></a>        TF_RETURN_IF_ERROR<span class="op">(</span>buffer_donor_config<span class="op">-&gt;</span>RemoveBufferDonor<span class="op">(</span></span>
<span id="cb1-96"><a aria-hidden="true" href="#cb1-96" tabindex="-1"></a>            donor<span class="op">.</span>param_number<span class="op">,</span> donor<span class="op">.</span>index<span class="op">));</span></span>
<span id="cb1-97"><a aria-hidden="true" href="#cb1-97" tabindex="-1"></a>        donor_vector_index<span class="op">++;</span></span>
<span id="cb1-98"><a aria-hidden="true" href="#cb1-98" tabindex="-1"></a>        donee_vector_index<span class="op">++;</span></span>
<span id="cb1-99"><a aria-hidden="true" href="#cb1-99" tabindex="-1"></a>        changed <span class="op">=</span> <span class="kw">true</span><span class="op">;</span></span>
<span id="cb1-100"><a aria-hidden="true" href="#cb1-100" tabindex="-1"></a>      <span class="op">}</span></span>
<span id="cb1-101"><a aria-hidden="true" href="#cb1-101" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb1-102"><a aria-hidden="true" href="#cb1-102" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb1-103"><a aria-hidden="true" href="#cb1-103" tabindex="-1"></a></span>
<span id="cb1-104"><a aria-hidden="true" href="#cb1-104" tabindex="-1"></a>  <span class="cf">return</span> changed<span class="op">;</span></span>
<span id="cb1-105"><a aria-hidden="true" href="#cb1-105" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb1-106"><a aria-hidden="true" href="#cb1-106" tabindex="-1"></a></span>
<span id="cb1-107"><a aria-hidden="true" href="#cb1-107" tabindex="-1"></a><span class="co">// Entry point for running the alias optimization on an HLO module.</span></span>
<span id="cb1-108"><a aria-hidden="true" href="#cb1-108" tabindex="-1"></a>absl<span class="op">::</span>StatusOr<span class="op">&lt;</span><span class="dt">bool</span><span class="op">&gt;</span> OptimizeInputOutputBufferAlias<span class="op">::</span>Run<span class="op">(</span></span>
<span id="cb1-109"><a aria-hidden="true" href="#cb1-109" tabindex="-1"></a>    HloModule<span class="op">*</span> <span class="kw">module</span><span class="op">,</span></span>
<span id="cb1-110"><a aria-hidden="true" href="#cb1-110" tabindex="-1"></a>    <span class="at">const</span> absl<span class="op">::</span>flat_hash_set<span class="op">&lt;</span>absl<span class="op">::</span>string_view<span class="op">&gt;&amp;</span> execution_threads<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-111"><a aria-hidden="true" href="#cb1-111" tabindex="-1"></a>  </span>
<span id="cb1-112"><a aria-hidden="true" href="#cb1-112" tabindex="-1"></a>  <span class="co">// Extract input and output shapes from the module entry computation.</span></span>
<span id="cb1-113"><a aria-hidden="true" href="#cb1-113" tabindex="-1"></a>  <span class="at">const</span> <span class="kw">auto</span><span class="op">&amp;</span> entry_computation_layout <span class="op">=</span> <span class="kw">module</span><span class="op">-&gt;</span>entry_computation_layout<span class="op">();</span></span>
<span id="cb1-114"><a aria-hidden="true" href="#cb1-114" tabindex="-1"></a>  <span class="bu">std::</span>vector<span class="op">&lt;</span>Shape<span class="op">&gt;</span> input_shapes<span class="op">;</span></span>
<span id="cb1-115"><a aria-hidden="true" href="#cb1-115" tabindex="-1"></a>  <span class="cf">for</span> <span class="op">(</span><span class="dt">int64_t</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> <span class="kw">module</span><span class="op">-&gt;</span>entry_computation<span class="op">()-&gt;</span>num_parameters<span class="op">();</span> <span class="op">++</span>i<span class="op">)</span> <span class="op">{</span></span>
<span id="cb1-116"><a aria-hidden="true" href="#cb1-116" tabindex="-1"></a>    input_shapes<span class="op">.</span>push_back<span class="op">(</span>entry_computation_layout<span class="op">.</span>parameter_shape<span class="op">(</span>i<span class="op">));</span></span>
<span id="cb1-117"><a aria-hidden="true" href="#cb1-117" tabindex="-1"></a>  <span class="op">}</span></span>
<span id="cb1-118"><a aria-hidden="true" href="#cb1-118" tabindex="-1"></a>  <span class="at">const</span> Shape<span class="op">&amp;</span> output_shape <span class="op">=</span> entry_computation_layout<span class="op">.</span>result_shape<span class="op">();</span></span>
<span id="cb1-119"><a aria-hidden="true" href="#cb1-119" tabindex="-1"></a></span>
<span id="cb1-120"><a aria-hidden="true" href="#cb1-120" tabindex="-1"></a>  HloInputOutputAliasConfig<span class="op">*</span> alias_config <span class="op">=</span> <span class="op">&amp;</span><span class="kw">module</span><span class="op">-&gt;</span>input_output_alias_config<span class="op">();</span></span>
<span id="cb1-121"><a aria-hidden="true" href="#cb1-121" tabindex="-1"></a>  HloBufferDonorConfig<span class="op">*</span> buffer_donor_config <span class="op">=</span> <span class="op">&amp;</span><span class="kw">module</span><span class="op">-&gt;</span>buffer_donor_config<span class="op">();</span></span>
<span id="cb1-122"><a aria-hidden="true" href="#cb1-122" tabindex="-1"></a></span>
<span id="cb1-123"><a aria-hidden="true" href="#cb1-123" tabindex="-1"></a>  <span class="co">// Attempt to build aliasing configuration.</span></span>
<span id="cb1-124"><a aria-hidden="true" href="#cb1-124" tabindex="-1"></a>  TF_ASSIGN_OR_RETURN<span class="op">(</span><span class="dt">bool</span> changed<span class="op">,</span> Build<span class="op">(</span>input_shapes<span class="op">,</span> output_shape<span class="op">,</span></span>
<span id="cb1-125"><a aria-hidden="true" href="#cb1-125" tabindex="-1"></a>                                          alias_config<span class="op">,</span> buffer_donor_config<span class="op">));</span></span>
<span id="cb1-126"><a aria-hidden="true" href="#cb1-126" tabindex="-1"></a>  </span>
<span id="cb1-127"><a aria-hidden="true" href="#cb1-127" tabindex="-1"></a>  <span class="co">// Verify that the resulting alias configuration is correct.</span></span>
<span id="cb1-128"><a aria-hidden="true" href="#cb1-128" tabindex="-1"></a>  TF_RETURN_IF_ERROR<span class="op">(</span>alias_config<span class="op">-&gt;</span>Verify<span class="op">(*</span><span class="kw">module</span><span class="op">,</span> <span class="va">shape_size_fn_</span><span class="op">));</span></span>
<span id="cb1-129"><a aria-hidden="true" href="#cb1-129" tabindex="-1"></a></span>
<span id="cb1-130"><a aria-hidden="true" href="#cb1-130" tabindex="-1"></a>  <span class="cf">return</span> changed<span class="op">;</span></span>
<span id="cb1-131"><a aria-hidden="true" href="#cb1-131" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb1-132"><a aria-hidden="true" href="#cb1-132" tabindex="-1"></a></span>
<span id="cb1-133"><a aria-hidden="true" href="#cb1-133" tabindex="-1"></a><span class="op">}</span> <span class="co">// namespace xla</span></span></code></pre></div>
<p>This code performs input preparation, donor and donee collection,
size-based sorting, two-pointer matching of donors and donees, alias
setup, and verification of the final configuration.</p>
<h2 id="dynamic-shape-handling-on-aliasing">Dynamic Shape Handling on
Aliasing</h2>
<p>Dynamic shapes introduce uncertainty in buffer sizes during runtime.
If an output tensor’s shape is dynamic, the memory required cannot be
reliably determined at compile time. As a result, XLA restricts aliasing
for dynamic shapes by disabling optimization whenever the output is
dynamic. Allowing aliasing for dynamic outputs could lead to incorrect
buffer allocations and data corruption if the output grows larger than
the input buffer. Verifying correctness would also require complex
runtime checks that can negate performance benefits. By disallowing
aliasing in such cases, XLA ensures a simple and predictable memory
model, though at the cost of missed optimization opportunities for
dynamic workloads.</p>
<p>Recent research, such as work on dynamic bufferization in MLIR<a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref"><sup>2</sup></a>, proposes more sophisticated methods
to handle dynamic shapes while minimizing overhead.</p>
<h2 id="safe-aliasing-with-dynamic-shapes">Safe Aliasing with Dynamic
Shapes</h2>
<p>One strategy to safely allow aliasing with dynamic shapes is to
introduce runtime shape guards. These guards would validate buffer
compatibility at execution time, only enabling aliasing if the actual
shapes are compatible. Another strategy involves over-allocating buffers
with conservative margins so that minor runtime variations can still be
safely accommodated. A third strategy is deferred memory binding, in
which buffer reuse decisions are delayed until runtime when precise
shape information is available. These approaches trade a small amount of
runtime complexity for potentially significant memory savings.</p>
<h2 id="aliasing-optimization-to-cross-computation-buffer-reuse">Aliasing
Optimization to Cross-Computation Buffer Reuse</h2>
<p>To extend aliasing beyond a single computation, XLA would require
global buffer lifetime analysis across multiple HLO computations. By
analyzing liveness and memory usage across function calls and loops, it
would be possible to recycle buffers globally. Techniques like memory
pooling and cross-computation liveness tracking, adapted from
traditional compiler research <a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref"><sup>3</sup></a>, could help enable
aliasing. Such global optimizations would offer further memory
reductions for large-scale training workloads.</p>
<h2 id="trade-offs-between-compile-time-and-runtime-aliasing">Trade-offs
Between Compile-Time and Runtime Aliasing</h2>
<p>Compile-time aliasing offers a simpler and predictable execution
model with no runtime overhead, but it is inherently conservative. It
cannot exploit dynamic input properties or adapt to changing workloads.
Runtime aliasing decisions, in contrast, allow more aggressive
optimization by adapting to actual runtime shapes, but introduce
additional complexity and potential variability in performance. Hybrid
models that integrate compile-time planning with lightweight runtime
validation, such as those explored in MLIR <a class="footnote-ref" href="#fn4" id="fnref4" role="doc-noteref"><sup>4</sup></a>,
are promising future directions.</p>
<h2 id="references">References</h2>
<section class="footnotes footnotes-end-of-document" id="footnotes" role="doc-endnotes">
<hr/>
<ol>
<li id="fn1"><p>“XLA: Optimizing Compiler for Machine Learning.” OpenXLA
Project, https://openxla.org/xla/tf2xla. Accessed 20 Dec. 2023.<a class="footnote-back" href="#fnref1" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Shape Inference - MLIR.
https://mlir.llvm.org/docs/ShapeInference/. Accessed 20 Dec. 2023.<a class="footnote-back" href="#fnref2" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Muchnick, Steven. Advanced compiler design
implementation. Morgan kaufmann, 1997.<a class="footnote-back" href="#fnref3" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Levental, Maksim. “Memory planning for deep neural
networks.” arXiv preprint arXiv:2203.00448 (2022).<a class="footnote-back" href="#fnref4" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
    </div>
    <hr/>
    <p>If you found this useful, please cite this post using</p>
    <blockquote class="blockquote-citation">
        <p>Senthilkumar Gopal. (Dec 2023). Aliasing on XLA. sengopal.me. https://sengopal.me/posts/aliasing-on-xla</p>
    </blockquote>
    <p>or</p>
    <div class="citation">
        <pre class="citation">@article{gopal2023aliasingonxla,
  title   = {Aliasing on XLA},
  author  = {Senthilkumar Gopal},
  journal = {sengopal.me},
  year    = {2023},
  month   = {Dec},
  url     = {https://sengopal.me/posts/aliasing-on-xla}
}</pre>
    </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
    <ul class="col-sm-6 list-inline">
        <li class="list-inline-item"><a href="https://sengopal.me/archives">Archives</a></li>
        <li class="list-inline-item"><a href="https://sengopal.me/categories">Categories</a></li>
        <li class="list-inline-item"><a href="https://sengopal.me/tags">Tags</a></li>
    </ul>
    <p class="col-sm-6 text-sm-right text-muted">
        Opinions my own. Made with &#x2764; using <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a> / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>        
    </p>
</div>    </div>
  </footer>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" crossorigin="anonymous"></script>
</body>

</html>