<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>LLM Inference Systems | Senthilkumar Gopal
</title>
  <link rel="canonical" href="/posts/llm-inference-systems.html">

    <link rel="apple-touch-icon" href="/apple-touch-icon.png" sizes="180x180">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="manifest" href="/manifest.json">
    <meta name="theme-color" content="#333333">

  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/all.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">
  <link rel="stylesheet" href="/extras/css/custom.css">
  <link rel="stylesheet" href="/extras/css/skylighting-solarized-theme.css">

  <link rel="alternate" type="application/atom+xml" title="Full Atom Feed"
        href="/feeds/all.atom.xml">

<meta name="description" content="A quick clarification between the terms - Triton, TensorRT, and TensorRT-LLM">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
  <div class="col-sm-12">
    <h1 class="title"><a href="/">Senthilkumar Gopal</a></h1>
      <p class="text-muted">Musings of a machine learning researcher, engineer and leader</p>
      <ul class="list-inline">
            <li class="list-inline-item"><a href="/pages/about.html">About me</a></li>
            <li class="list-inline-item"><a href="/pages/publications.html">Publications</a></li>
            <li class="list-inline-item"><a href="/pages/software.html">Software</a></li>
            <li class="list-inline-item"><a href="/pages/talks.html">Talks</a></li>
            <li class=" list-inline-item text-muted">|</li>
          <li class="list-inline-item"><a class="fab fa-github" href="https://github.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-speaker-deck" href="https://speakerdeck.com/sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-google-scholar" href="https://scholar.google.com/citations?user=bs8WraEAAAAJ&hl=en" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-medium" href="https://medium.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-twitter" href="https://twitter.com/@sengopal" target="_blank"></a></li>
          <li class="list-inline-item"><a class="fab fa-linkedin" href="https://www.linkedin.com/in/senthilkumargopal" target="_blank"></a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
<article class="article">
    <header>
        <ul class="list-inline">
            <li class="list-inline-item text-muted" title="2024-05-18T00:00:00-07:00">
                <i class="fas fa-clock"></i>
                Sat 18 May 2024
            </li>
            <li class="list-inline-item">
                <i class="fas fa-folder-open"></i>
                <a href="/category/large-language-models.html">Large Language Models</a>
            </li>
            <li class="list-inline-item">
                <i class="fas fa-tag"></i>
                <a href="/tag/ml-code.html">#ml-code</a>,                 <a href="/tag/llm.html">#llm</a>            </li>
        </ul>
    </header>
    <h1>LLM Inference Systems</h1>
    <div class="hidden-xs hidden-sm">
        <nav class="toc" role="doc-toc">
<ul>
<li><a href="#triton" id="toc-triton">Triton</a></li>
<li><a href="#tensorrt" id="toc-tensorrt">TensorRT</a></li>
<li><a href="#tensorrt-llm" id="toc-tensorrt-llm">TensorRT-LLM</a></li>
</ul>
</nav>
    </div>
    <div class="content">
        <p>In my exploration of various inference systems, there is a general
confusion about Triton, TensorRT, and TensorRT-LLM. These are all
related to optimizing and deploying machine learning models, but with
different purposes and use cases, specifically what inputs and outputs
that they support.</p>
<h3 id="triton">Triton</h3>
<p>Triton is an open-source inference <strong>server</strong> (NVIDIA)
designed to optimize and deploy machine learning models for inference
workloads and supports multiple deep learning frameworks, including
TensorFlow, PyTorch, and TensorRT. Triton supportsfeatures like
batching, concurrent model execution, and dynamic batching to improve
performance and efficiency.</p>
<ul>
<li><strong>Inputs</strong> Triton can accept inputs in various formats,
such as images, text, or numerical data, depending on the machine
learning model being deployed.</li>
<li><strong>Outputs</strong> The outputs depend on the specific machine
learning model and its task, such as classification results, object
detection bounding boxes, or text generation.</li>
</ul>
<h3 id="tensorrt">TensorRT</h3>
<p>TensorRT is a high-performance deep learning inference optimizer and
runtime <strong>library</strong> developed by NVIDIA designed to
optimize and accelerate the inference performance of deep learning
models on NVIDIA GPUs. TensorRT can optimize models from various deep
learning frameworks, including TensorFlow, PyTorch, and ONNX.</p>
<ul>
<li><strong>Inputs</strong> TensorRT accepts deep learning models in
various formats, such as TensorFlow frozen graphs, PyTorch traced
models, or ONNX models.</li>
<li><strong>Outputs</strong> TensorRT optimizes the models for faster
inference on NVIDIA GPUs, but it does not change the modelâ€™s output
format. The outputs remain the same as the original model, such as
classification probabilities, object detection bounding boxes, or
segmentation masks.</li>
</ul>
<h3 id="tensorrt-llm">TensorRT-LLM</h3>
<p>TensorRT-LLM is a branch of TensorRT specifically designed for
optimizing and deploying large language models (LLMs) on NVIDIA GPUs. It
provides optimizations and techniques tailored for the efficient
inference of LLMs, which can be computationally expensive due to their
large size and complex architectures.</p>
<ul>
<li><strong>Inputs</strong> TensorRT-LLM accepts pre-trained LLM models
in various formats, such as TensorFlow, PyTorch, or ONNX.</li>
<li><strong>Outputs</strong> The outputs of TensorRT-LLM remain the same
as the original LLM model, which typically involves generating text
based on the input prompt or context.</li>
</ul>
<p>TL;DR - Triton is an inference server that can deploy and optimize
models from different frameworks, including TensorRT-optimized models.
TensorRT is a library for optimizing and accelerating deep learning
models for inference on NVIDIA GPUs. TensorRT-LLM is a specialized
branch of TensorRT focused on optimizing and deploying large language
models efficiently on NVIDIA GPUs.</p>
    </div>
</article>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
    <ul class="col-sm-6 list-inline">
        <li class="list-inline-item"><a href="/archives">Archives</a></li>
        <li class="list-inline-item"><a href="/categories">Categories</a></li>
        <li class="list-inline-item"><a href="/tags">Tags</a></li>
    </ul>
    <p class="col-sm-6 text-sm-right text-muted">
        Opinions my own. Made with &#x2764; using <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a> / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>        
    </p>
</div>    </div>
  </footer>

  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" crossorigin="anonymous"></script>
</body>

</html>